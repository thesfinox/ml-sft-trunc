{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Level Truncation in Bosonic Open String Field Theory\n",
    "\n",
    "We consider the position of minima of the tachyon potential in bosonic open string field theory (OSFT) at mass level truncation of finite order. We then extrapolate predictions for level-$\\infty$ truncation using machine learning (ML) techniques. In particular we will consider:\n",
    "\n",
    "- [linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) (LR) to provide a solid baseline for the computation,\n",
    "- [elastic net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) (EN) to introduce $l_1$ and $l_2$ regularisation to weight the contributions (we could consider [lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) or [ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) regression if we notice $l_1$ or $l_2$ are particularly favoured),\n",
    "- [linear SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html) (l-SVR) to introduce support vectors,\n",
    "- [SVR (rbf kernel)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) (SVR) to use the _kernel trick_ inside the support vectors,\n",
    "- [random forests of decision trees](https://xgboost.readthedocs.io/en/latest/tutorials/rf.html) (RFs) to bag several decision trees and decrease the variance of the tree-based model,\n",
    "- [boosted decision trees](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training) (BDTs) to increase the predictive ability to tackle the problem of reducing bias and variance (we also consider the case of boosted random forests of decision trees (BRF),\n",
    "- [artificial neural networks](https://www.tensorflow.org) (ANNs) to introduce more complex regression functions.\n",
    "\n",
    "In general we use [Scikit](https://scikit-learn.org) for most manipulations and [XGBoost](https://xgboost.ai/) for the BDTs. We then use [Tensorflow](https://www.tensorflow.org/) as a deep learning framework for the ANNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preanalysis\n",
    "\n",
    "First of all we consider the dataset containing the positions of minimum points of the tachyon potential for several finite levels of mass truncation. Given a set of initial conditions, these are computed and stored in 'systems' (in the dataset there are 46 of them). Then the extrapolation for level-$\\infty$ is given for each.\n",
    "\n",
    "The idea is therefore to manipulate the system by studying the distribution of the systems: we flatten the database in order to have a feature matrix containing data for training and a vector of labels for the supervised learning. We also exclude duplicates (in this case, data identical over all columns) and identify the outliers of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The following analysis is performed on a machine with the following specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"CPU: $(lscpu| awk '/^Model name/ {$1=\"\"; $2=\"\"; print}'| sed 's/^[[:space:]]*//g')\"\n",
    "!echo \"GPU: $(lspci| awk '/3D controller/ {$1=\"\"; $2=\"\"; $3=\"\"; print}'| sed 's/^[[:space:]]*//g')\"\n",
    "!echo \"RAM: $(free --giga| awk '/^Mem/ {print $2}')GB (avail. now: $(free --giga| awk '/^Mem/ {print $7}')GB)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computations will be executed using a restricted amount of CPU threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_thread = 8\n",
    "\n",
    "# sanitise the input\n",
    "if multi_thread > 8:\n",
    "    multi_thread = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use several module in this Python notebook. We import them early to take a look at their version number and to keep track of changes in the intallation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib        as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn           as skl\n",
    "import skopt             as skp\n",
    "import tensorflow        as tf\n",
    "import xgboost           as xgb\n",
    "\n",
    "# Jupyter magics\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=12) #------- set size of the labels in Matplotlib\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# check for restrictions and print version number\n",
    "try:\n",
    "    assert np.__version__ >= '1.18.0', 'Numpy version should be at least 1.18.0 to avoid conflict with Pandas and PyTables'\n",
    "    print('Numpy_version: {}'.format(np.__version__))\n",
    "    \n",
    "    assert pd.__version__ >= '1.0.0', 'Pandas version should be at least 1.0.0 to use PyTables correctly'\n",
    "    print('Pandas version: {}'.format(pd.__version__))\n",
    "    \n",
    "    assert mpl.__version__ > '3.1.0', 'Matplotlib version should be at least 3.1.0'\n",
    "    print('Matplotlib version: {}'.format(mpl.__version__))\n",
    "\n",
    "    assert skl.__version__ >= '0.22.0', 'Scikit-learn version should be at least 0.22.0 to use newest implementations.'\n",
    "    print('Scikit-learn version: {}'.format(skl.__version__))\n",
    "\n",
    "    assert skp.__version__ >= '0.7.4', 'Scikit-optimize version should be at least 0.7.4 to use newest implementations.'\n",
    "    print('Scikit-optimize version: {}'.format(skl.__version__))\n",
    "    \n",
    "    assert tf.__version__ >= '2.0.0', 'Tensorflow version should be at least 2.0.0 to use newest implementations.'\n",
    "    print('Tensorflow version: {}'.format(tf.__version__))\n",
    "    \n",
    "    assert xgb.__version__ >= '0.88.0', 'XGBoost version should be at least 0.88.0 to use newest implementations.'\n",
    "    print('XGBoost version: {}'.format(tf.__version__))\n",
    "    \n",
    "except AssertionError as msg:\n",
    "    print(msg)\n",
    "    \n",
    "# fix the random seed\n",
    "RAND = 42\n",
    "np.random.seed(RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately set the memory growth for the gpu in order not to overflow it and avoid leaks when using Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gpus devices\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# set memory growth\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the directory structure and path names to work within the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "\n",
    "# define directory names\n",
    "ROOT_DIR = '.' #-------------------------------------------------- root directory\n",
    "IMG_DIR  = 'img' #------------------------------------------------ images\n",
    "MOD_DIR  = 'models' #--------------------------------------------- saved models\n",
    "LOG_DIR  = 'log' #------------------------------------------------ logs\n",
    "OUT_DIR  = 'output' #--------------------------------------------- saved predictions, relevant output, etc.\n",
    "\n",
    "DB_NAME = 'data_sft_dict' #--------------------------------------- name of the dataset\n",
    "DB_FILE = DB_NAME + '.json' #------------------------------------- full name with extension\n",
    "DB_PATH = path.join(ROOT_DIR, DB_FILE) #-------------------------- full path of the dataset\n",
    "\n",
    "# define full paths\n",
    "IMG_PATH = path.join(ROOT_DIR, IMG_DIR)\n",
    "MOD_PATH = path.join(ROOT_DIR, MOD_DIR)\n",
    "LOG_PATH = path.join(ROOT_DIR, LOG_DIR)\n",
    "OUT_PATH = path.join(ROOT_DIR, OUT_DIR)\n",
    "\n",
    "# create directories if non existent\n",
    "if not path.isdir(IMG_PATH):\n",
    "    makedirs(IMG_PATH, exist_ok=True)\n",
    "if not path.isdir(MOD_PATH):\n",
    "    makedirs(MOD_PATH, exist_ok=True)\n",
    "if not path.isdir(LOG_PATH):\n",
    "    makedirs(LOG_PATH, exist_ok=True)\n",
    "if not path.isdir(OUT_PATH):\n",
    "    makedirs(OUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally create a logging session to store debug info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from mltools.liblog import create_logfile\n",
    "\n",
    "LOG_NAME = 'sft-trunc-ml'\n",
    "path_to_log = path.join(LOG_PATH, LOG_NAME + '.log') #------------------------- path to the log\n",
    "log = create_logfile(path_to_log, name=LOG_NAME, level=logging.DEBUG) #---------- create log file and session\n",
    "\n",
    "log.info('\\n\\n'\n",
    "         '--------------------------------------------\\n'\n",
    "         '  MACHINE LEARNING FOR LEVEL TRUNCATION IN\\n'\n",
    "         '  BOSONIC OPEN STRING FIELD THEORY\\n\\n'\n",
    "         '--------------------------------------------\\n'\n",
    "         '  Authors: Harold Erbin, Riccardo Finotello\\n'\n",
    "         '--------------------------------------------\\n'\n",
    "         '  Abstract:\\n\\n'\n",
    "         '  We consider the position of the lumps of\\n'\n",
    "         '  the tachyon potential in bosonic open\\n'\n",
    "         '  string field theory at a finite mass level\\n'\n",
    "         '  truncation. We then extrapolate the\\n'\n",
    "         '  predictions for level-$\\infty$ using\\n'\n",
    "         '  machine learning techniques.\\n\\n'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Database\n",
    "\n",
    "We import the database containing the positions of the lumps of the tachyon potential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if path.isfile(DB_PATH):\n",
    "    df = pd.read_json(DB_PATH)\n",
    "    \n",
    "    if not df.empty:\n",
    "        log.debug('Successfully imported {}'.format(DB_PATH))\n",
    "    else:\n",
    "        sys.stderr.write('Database is empty!')\n",
    "        log.error('Database is empty!')\n",
    "else:\n",
    "    sys.stderr.write('Cannot find database!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then start to analyse the _dtypes_ of the columns to understand what the dataset is made of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we take a look at the first few entries to understand the composition of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total we are dealing with a dataset of shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset we have therefore different predictions for the position of the lumps of the bosonic potential: they correspond to different choices of the initial point and other properties and data for 18 levels of mass truncation are provided together with the extrapolation for the level-$\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Extraction and Manipulation\n",
    "\n",
    "Before moving to the analysis we need to extract the modify the features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we exclude the first entry which looks too static and artificially \"perfect\" to help in the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[1:].reset_index(drop=True) #------ drop first entry and reset the index counter (do not include former index as column)\n",
    "print('New shape of the dataset: {}'.format(df.shape))\n",
    "\n",
    "log.info('Dropped first entry of the dataset. New shape: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check that row-wise the size of the elements is unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sizes = df.applymap(np.shape)\\\n",
    "             .apply(np.unique, axis=1) #-------------------------------- compute the shape of each element of the dataframe and\n",
    "#----------------------------------------------------------------------- take the unique values in each row\n",
    "\n",
    "df_sizes_if_unique = df_sizes.apply(lambda x: np.shape(x) == (1,))\\\n",
    "                             .sum(axis=0) #----------------------------- check if each row contains only one element and\n",
    "#----------------------------------------------------------------------- sum True values which should equal the size of the dataframe\n",
    "\n",
    "try:\n",
    "    assert df_sizes_if_unique == df.shape[0], 'Sizes along rows are not unique!'\n",
    "    log.info('Sizes along rows are unique.')\n",
    "except AssertionError as msg:\n",
    "    log.error(msg)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add one column which represents the position of each datum inside its own system of solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_index(index, shape):\n",
    "    '''\n",
    "    Pad the index number with a certain shape.\n",
    "    \n",
    "    Required arguments:\n",
    "        index: the index to pad\n",
    "        shape: the shape of the padding\n",
    "        \n",
    "    Returns:\n",
    "        the padded index\n",
    "    '''\n",
    "    \n",
    "    return [index] * np.prod(shape)\n",
    "\n",
    "def pad_index_list(index_list, shape_list):\n",
    "    '''\n",
    "    Pad the entire list of indices.\n",
    "    \n",
    "    Required arguments:\n",
    "        index_list: list of indices to pad\n",
    "        shape_list: list of shapes of the paddings\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        assert df.shape[0] == df_sizes.shape[0]\n",
    "    except AssertionError as msg:\n",
    "        print(msg)\n",
    "        \n",
    "    full_list = []\n",
    "    for n in range(df.shape[0]):\n",
    "        full_list.append(pad_index(index_list[n], shape_list[n]))\n",
    "        \n",
    "    return full_list\n",
    "\n",
    "# add the column with the list of indices\n",
    "df['system'] = pad_index_list(df.index, df_sizes.apply(lambda x: x[0]))\n",
    "log.info('Add \"system\" feature as reference.')\n",
    "\n",
    "# reorder the dataframe\n",
    "df = df[['system', 'init', 'weight', 'type', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', 'exp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then \"flatten\" the systems: for each system we build a new dataframe and then concatenate each of the new dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = pd.concat([pd.DataFrame({f: df[f].iloc[n] for f in df}) for n in range(df.shape[0])], axis=0, ignore_index=True)\n",
    "log.debug('New flattened dataframe has been built.')\n",
    "\n",
    "# describe the new dataset\n",
    "df_flat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates Search and Outliers Detection\n",
    "\n",
    "We then analyse the new dataset in search of duplicates and outliers. We will drop the firsts, while we will keep the seconds even though we want to identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodup = df_flat.drop_duplicates(ignore_index=True)\n",
    "df_nodup.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then look for outliers. However we have no idea how many of them are in the dataset, thus [`sklearn.EllipticEnvelope`](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html) is not a good choice. We use the _interquartile range_ (IQR) to detect outliers and decide (if needed) some boundaries for the outliers. We also define an _interdecile range_ (IDR) to select extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_detection(feature):\n",
    "    '''\n",
    "    Compute the interquartile range of a given feature and return the indices of the outliers.\n",
    "    \n",
    "    Required arguments:\n",
    "        feature: the list of data to analyse.\n",
    "        \n",
    "    Returns:\n",
    "        the list of indices of outliers.\n",
    "    '''\n",
    "    \n",
    "    q1, q3 = np.percentile(feature, [25, 75]) #-------------------------------- get values of 1st and 3rd quartile\n",
    "    iqr    = q3 - q1 #--------------------------------------------------------- compute the interquartile range (IQR)\n",
    "    \n",
    "    lower  = q1 - (iqr * 1.5) #------------------------------------------------ lower bound\n",
    "    upper  = q3 + (iqr * 1.5) #------------------------------------------------ higher bound\n",
    "    \n",
    "    return np.where((feature > upper) | (feature < lower))[0].tolist() #------- return indices out of bounds\n",
    "\n",
    "def idr_detection(feature):\n",
    "    '''\n",
    "    Compute the interdecile range and return indices of points outside the limit.\n",
    "    \n",
    "    Required arguments:\n",
    "        feature: the list of data to analyse.\n",
    "        \n",
    "    Returns:\n",
    "        the list of indices of outliers.\n",
    "    '''\n",
    "    \n",
    "    d1, d9 = np.percentile(feature, [10, 90]) #-------------------------------- get values of 1st and 9th decile\n",
    "    idr    = d9 - d1 #--------------------------------------------------------- compute the interdecile range (IDR)\n",
    "\n",
    "    lower  = d1 - (idr * 1.5) #------------------------------------------------ lower bound\n",
    "    upper  = d9 + (idr * 1.5) #------------------------------------------------ higher bound\n",
    "    \n",
    "    return np.where((feature > upper) | (feature < lower))[0].tolist() #------- return indice out of bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give a few exemples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in df_nodup.columns:\n",
    "    print('Number of outlying samples in \"{}\":         {:d}'.format(feature, np.shape(iqr_detection(df_nodup[feature]))[0]))\n",
    "    print('Number of extreme outlying samples in \"{}\": {:d}'.format(feature, np.shape(idr_detection(df_nodup[feature]))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Dataset\n",
    "\n",
    "We finally save the dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodup.to_json(path.join(ROOT_DIR, 'data_sft_analysis.json'))\n",
    "df = df_nodup #------------------------------------------------- rename the variable for simpler referencing\n",
    "\n",
    "log.debug('Dataset of the preanalysis saved to file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of elements for each system should be rather contained and can be visualised in a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltools.libplot import Plot\n",
    "\n",
    "hist_data = df.groupby(by=['system']).count().max(axis=1) #-------------- group by system, then count the number of elements and take a representative\n",
    "\n",
    "plot = Plot(rows=1, columns=2)\n",
    "\n",
    "plot.hist2D(df['system'],\n",
    "            axis=0,\n",
    "            title='Distribution of Samples in the Systems',\n",
    "            xlabel='systems',\n",
    "            ylabel='no. of samples',\n",
    "            binstep=4\n",
    "           )\n",
    "\n",
    "plot.hist2D(hist_data,\n",
    "            axis=1,\n",
    "            title='Distribution of Systems as Function of Their Size',\n",
    "            xlabel='size',\n",
    "            ylabel='no. of systems',\n",
    "            bins=range(hist_data.min(), hist_data.max() + 1, 1)\n",
    "           )\n",
    "\n",
    "plot.save_and_close(path.join(IMG_PATH, 'system_samples_distribution'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "We then move to the ML analysis using the previously described algorithms.\n",
    "\n",
    "We first split the dataset into test and training set and then we select a portion of the training set to be used as development: this is done to preserve the ordering of the systems inside each set, in order to have samples coming from the same distributions. Cross validation (CV) would be a better choice, but the restricted number of samples in the training may spoil the results. Moreover data seems to have a good variability and any choice of the development set seem to reproduce it quite well.\n",
    "\n",
    "We show the results of the predictions for each algorithm separately. Furthermore we first show the study operated through Bayes optimisation using [scikit-optimize](https://scikit-optimize.github.io) on the development set to avoid using the test set. We finally show the predictions for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Selection\n",
    "\n",
    "We split the training set into a test and training sets keeping samples of the same system in the same set. As our dataset is quite small, we will try to keep around 20% of data in the test set to keep a good predictive ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first divide features from labels (in the features, we also drop the `init` column as we are not interested in using the initial values for the learning algorithms and the `system` column which will only be used to split training and test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=RAND) #-------------------------------------------------- first shuffle the dataset\n",
    "\n",
    "log.debug('Database has been shuffled.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the dataset using the `system` label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_system, test_system = train_test_split(df['system'].unique(), test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to training and test set with the following shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.loc[df['system'].isin(train_system)]\n",
    "df_test  = df.loc[df['system'].isin(test_system)]\n",
    "\n",
    "# print the shape\n",
    "print('Training set size: {:d} ({:.1f}% of the total dataset)'.format(df_train.shape[0], 100 * df_train.shape[0] / df.shape[0]))\n",
    "print('Test set size:     {:d} ({:.1f}% of the total dataset)'.format(df_test.shape[0], 100 * df_test.shape[0] / df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Strategy\n",
    "\n",
    "For the same reason we used the `system` column to choose the training and test sets, we also need to use the same strategy for the validation set. Given the restricted size of the training dataset, we use a single _development set_ whose size must be decided. We use a linear regression to plot the error on the validation set as a function of the size of the development set. We clearly use the same approach as before: we take the `system` label, split the training set accordingly, fit the training data and evaluate on the development set.\n",
    "\n",
    "In foresight of what we are going to do, we also apply a standardization to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model  import LinearRegression\n",
    "from sklearn.metrics       import mean_squared_error\n",
    "\n",
    "# define what development set sizes to explore\n",
    "dev_sizes = np.arange(0.15, 0.95, 0.05)\n",
    "\n",
    "# save results in an array (made of [dev set size, mse] for each sample: shape = (dev_sizes.shape[0], 2))\n",
    "results = np.zeros((dev_sizes.shape[0], 2))\n",
    "\n",
    "# explore each dev size\n",
    "for n in range(dev_sizes.shape[0]):\n",
    "    \n",
    "    # separate train and dev systems values\n",
    "    train_system, dev_system = train_test_split(df_train['system'].unique(), test_size=dev_sizes[n], shuffle=False)\n",
    "\n",
    "    # separate train and dev sets\n",
    "    df_train_tmp = df_train.loc[df_train['system'].isin(train_system)].drop(columns=['system', 'init'])\n",
    "    df_dev_tmp   = df_train.loc[df_train['system'].isin(dev_system)].drop(columns=['system', 'init'])\n",
    "\n",
    "    # compute the size of the dev set w.r.t. the training set (it can differ from the size selected with the 'system' label)\n",
    "    dev_set_size = df_dev_tmp.shape[0] / df_train.shape[0]\n",
    "\n",
    "    # separate features from labels\n",
    "    X_train, y_train = df_train_tmp.drop(columns='exp'), df_train_tmp['exp']\n",
    "    X_dev,   y_dev   = df_dev_tmp.drop(columns='exp'),   df_dev_tmp['exp']\n",
    "\n",
    "    # standardize the features\n",
    "    std = StandardScaler()\n",
    "\n",
    "    X_train = std.fit_transform(X_train)\n",
    "    X_dev   = std.transform(X_dev)\n",
    "\n",
    "    # compute the linear regression\n",
    "    est = LinearRegression(fit_intercept=True, n_jobs=multi_thread)\n",
    "    est.fit(X_train, y_train)\n",
    "\n",
    "    # compute the predictions on the development set\n",
    "    y_dev_pred = est.predict(X_dev)\n",
    "\n",
    "    # compute the MSE on the predictions\n",
    "    mse = mean_squared_error(y_dev, y_dev_pred)\n",
    "\n",
    "    # save the results in the list\n",
    "    results[n,:] = (dev_set_size, mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot the results of the study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot().series2D(results[:,1],\n",
    "                title='Validation Error (MSE) as Function of the Development Set Size',\n",
    "                xlabel='size of the development set',\n",
    "                ylabel='MSE',\n",
    "                ylog=True,\n",
    "                labels=np.around(results[:,0], decimals=2)\n",
    "               ).save_and_close(path.join(IMG_PATH, 'val_strat_error'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a good compromise between the number of samples and the validation error might be around 25% of the training set. We therefore select that value as a reference to split definitely the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of the 'results' vector which is closer the desired quantity and take the corresponding 'dev_sizes' position\n",
    "chosen_dev_size = 0.25\n",
    "[dev_size]      = dev_sizes[np.where(np.isclose(results[:,0], chosen_dev_size, atol=0.01))]\n",
    "\n",
    "# split the datasets\n",
    "train_system, dev_system = train_test_split(df_train['system'].unique(), test_size=dev_size, shuffle=False)\n",
    "train_split = df_train.loc[df_train['system'].isin(train_system)].drop(columns=['system', 'init'])\n",
    "dev_split   = df_train.loc[df_train['system'].isin(dev_system)].drop(columns=['system', 'init'])\n",
    "test_split  = df_test.drop(columns=['system', 'init'])\n",
    "\n",
    "# compute train, dev and test sizes\n",
    "print('Training set size:    {:.1f}%'.format(100 * train_split.shape[0] / df.shape[0]))\n",
    "print('Development set size: {:.1f}%'.format(100 * dev_split.shape[0] / df.shape[0]))\n",
    "print('Test set size:        {:.1f}%'.format(100 * test_split.shape[0] / df.shape[0]))\n",
    "\n",
    "log.info('Partition of train/dev/test: {:.1f}/{:.1f}/{:.1f}'.format(100 * train_split.shape[0] / df.shape[0],\n",
    "                                                                    100 * dev_split.shape[0] / df.shape[0],\n",
    "                                                                    100 * test_split.shape[0] / df.shape[0]\n",
    "                                                                   )\n",
    "        )\n",
    "\n",
    "# divide features and labels\n",
    "X_train, y_train = train_split.drop(columns='exp'), train_split['exp'].values.reshape(-1,1)\n",
    "X_dev,   y_dev   = dev_split.drop(columns='exp'),   dev_split['exp'].values.reshape(-1,1)\n",
    "X_test,  y_test  = test_split.drop(columns='exp'),  test_split['exp'].values.reshape(-1,1)\n",
    "\n",
    "# apply the standardization\n",
    "std     = StandardScaler()\n",
    "X_train = std.fit_transform(X_train)\n",
    "X_dev   = std.transform(X_dev)\n",
    "X_test  = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Analysis\n",
    "\n",
    "We can now start to implement the regression algorithms we are trying to study. In general we use Bayes optimisation to tune the hyperparameters of the algorithm (exception made for linear regression: we will use a grid search) trying to minimise the mean squared error loss function.\n",
    "\n",
    "For each algorithm we will try to output several metrics, even though we will only use the MSE to score the algorithm:\n",
    "\n",
    "- _mean squared error_:\n",
    "\\begin{equation}\n",
    "\\mathrm{MSE}( y_{true}, y_{pred} ) = \\frac{1}{N} \\sum\\limits_{i = 1}^N \\left( y^{(i)}_{true} - y^{(i)}_{pred} \\right) ^2,\n",
    "\\end{equation}\n",
    "\n",
    "- _$R^2$ score_:\n",
    "\\begin{equation}\n",
    "\\mathrm{R}^2( y_{true}, y_{pred} ) = 1 - \\frac{\\sum\\limits_{i = 1}^N \\left( y^{(i)}_{true} - y^{(i)}_{pred} \\right)^2}{\\sum\\limits_{i = 1}^N \\left( y^{(i)}_{true} - \\bar{y}_{pred} \\right)^2}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "\\bar{y}_{pred} = \\frac{1}{N} \\sum\\limits_{i = 1}^N y^{(i)}_{true},\n",
    "\\end{equation}\n",
    "\n",
    "- _explained variance score_:\n",
    "\\begin{equation}\n",
    "\\mathrm{EVar}( y_{true}, y_{pred} ) = 1 - \\frac{\\mathrm{Var}\\{ y_{true} - y_{pred} \\}}{\\mathrm{Var}\\{ y_{true} \\}}.\n",
    "\\end{equation}\n",
    "\n",
    "We will also compute the confidence interval of the variance: this represents the the range of values for which we have confidence that the variance estimated from the data is a good representative of the true error. We therefore fix here the confidence we are trying to achieve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.95 #--- usually we set 95% of the variance (lower values are stricter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "We now consider a simple linear regression whose only hyperparameter is `fit_intercept` $\\in \\lbrace 0, 1 \\rbrace$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics      import mean_squared_error, explained_variance_score, r2_score\n",
    "\n",
    "# define the grid search\n",
    "fit_intercept = np.array([False, True]).reshape(-1,1)\n",
    "\n",
    "# define a vector of results with a shape (fit_intercept.shape[0], 3), where 3 is the number of metrics used\n",
    "results = np.zeros((fit_intercept.shape[0], 3))\n",
    "\n",
    "# define the estimator (set normalize to False since we already added standardization)\n",
    "lin_reg = LinearRegression(normalize=False, n_jobs=multi_thread)\n",
    "log.info('Fitting LR.')\n",
    "\n",
    "# loop over the hyperparameter optimisation space and save the results in a dictionary\n",
    "opt_results = {'fit_intercept': [],\n",
    "               'mse':           [],\n",
    "               'r2':            [],\n",
    "               'evar':          []\n",
    "              }\n",
    "for n in range(fit_intercept.shape[0]):\n",
    "    \n",
    "    # insert hyperparameter in the estimator\n",
    "    lin_reg.set_params(**{'fit_intercept': fit_intercept[n,:]})\n",
    "\n",
    "    # fit the estimator\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "\n",
    "    # generate predictions on the validation set\n",
    "    y_dev_pred = lin_reg.predict(X_dev)\n",
    "\n",
    "    # compute metrics\n",
    "    mse  = mean_squared_error(y_dev, y_dev_pred) #---------- compute MSE\n",
    "    r2   = r2_score(y_dev, y_dev_pred) #-------------------- compute R2\n",
    "    evar = explained_variance_score(y_dev, y_dev_pred) #---- compute explained variance\n",
    "    results[n,:] = (mse, r2, evar) #------------------------ add to the results\n",
    "    \n",
    "    # save to dictionary\n",
    "    opt_results['fit_intercept'].append(fit_intercept[n,:].squeeze().tolist())\n",
    "    opt_results['mse'].append(mse)\n",
    "    opt_results['r2'].append(r2)\n",
    "    opt_results['evar'].append(evar)\n",
    "    \n",
    "# look for the best results\n",
    "best_arg     = np.argmin(results[:,0])\n",
    "best_results = results[best_arg,:].reshape(-1,1)\n",
    "best_params  = fit_intercept[best_arg,:].reshape(-1,1)\n",
    "\n",
    "# retrain over the training set with the best parameters\n",
    "lin_reg.set_params(**{'fit_intercept': fit_intercept[best_arg,:]})\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# generate predictions on the dev set\n",
    "y_dev_pred = lin_reg.predict(X_dev).reshape(-1,1)\n",
    "\n",
    "# print the best results\n",
    "print('Results for LR:\\n\\n'\n",
    "      '  Best parameters:\\n\\n'\n",
    "      '    fit_intercept: {}\\n\\n'\n",
    "      '  MSE:  {:.3f}\\n'\n",
    "      '  R2:   {:.3f}\\n'\n",
    "      '  EVAR: {:.3f}'\n",
    "      .format(best_params[0,:].squeeze(),\n",
    "              best_results[0,:].squeeze(),\n",
    "              best_results[1,:].squeeze(),\n",
    "              best_results[2,:].squeeze()\n",
    "             )\n",
    "     )\n",
    "\n",
    "# save results to file\n",
    "joblib.dump(opt_results, path.join(OUT_PATH, 'lin_reg_dev_opt_res.pkl.xz'), compress=('xz', 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence interval can be estimated using a Student's $t$-distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def confidence_interval(y_true, y_pred, confidence=0.95):\n",
    "    '''\n",
    "    Compute the confidence interval of the variance.\n",
    "    \n",
    "    Required arguments:\n",
    "        y_true: true values,\n",
    "        y_pred: predictions.\n",
    "        \n",
    "    Returns:\n",
    "        the array of lower and upper bounds of the confidence interval.\n",
    "    '''\n",
    "    \n",
    "    # compute the deviation of the data and the squared errors\n",
    "    deviation = y_pred - y_true #-------------------------------------- > 0 if overestimating, < 0 if underestimating\n",
    "    sq_errors = deviation ** 2 #--------------------------------------- squared errors\n",
    "\n",
    "    conf_interval = stats.t.interval(confidence,\n",
    "                                     sq_errors.shape[0] - 1,\n",
    "                                     loc=sq_errors.mean(),\n",
    "                                     scale=stats.sem(sq_errors)\n",
    "                                    ) #-------------------------------- compute the confidence interval\n",
    "    \n",
    "    return conf_interval\n",
    "\n",
    "conf_interval = confidence_interval(y_dev, y_dev_pred, confidence=confidence)\n",
    "print('Confidence interval: [ {:.3f}, {:.3f} ]'.format(conf_interval[0].squeeze(), conf_interval[1].squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the model and its predictions for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def save_res(estimator, short_name):\n",
    "    '''\n",
    "    Save results to file.\n",
    "    \n",
    "    Required arguments:\n",
    "        estimator:  the estimator to save,\n",
    "        short_name: a short identifier of the estimator considered.\n",
    "    '''\n",
    "    # save the model to file\n",
    "    joblib.dump(estimator, path.join(MOD_PATH, short_name + '.pkl.xz'), compress=('xz', 9))\n",
    "\n",
    "    # save predictions to file\n",
    "    dev_predictions = {'y_dev_true': y_dev.squeeze().tolist(),\n",
    "                       'y_dev_pred': y_dev_pred.squeeze().tolist(),\n",
    "                       'y_dev_err':  (y_dev_pred - y_dev).squeeze().tolist()\n",
    "                      }\n",
    "    joblib.dump(dev_predictions, path.join(OUT_PATH, short_name + '_dev_preds.pkl.xz'), compress=('xz', 9))\n",
    "\n",
    "save_res(lin_reg, 'lin_reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the absolute deviation of the predictions in various ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "def error_visualisation(y_true, y_pred):\n",
    "    '''\n",
    "    Visualise plots of the variance of the errors.\n",
    "    \n",
    "    Required arguments:\n",
    "        y_true: true values,\n",
    "        y_pred: predictions.\n",
    "        \n",
    "    Returns:\n",
    "        the Matplotlib axis with the plots.\n",
    "    '''\n",
    "    # compute useful quantities\n",
    "    deviation    = y_pred - y_true #----------------------------------------------- > 0 if overestimating, < 0 if underestimating\n",
    "    binarization = Binarizer(threshold=0.0).transform(deviation) #----------------- +1 if overestimating, 0 if underestimating\n",
    "    digit_bins   = range(int(np.ceil(deviation.min())),\n",
    "                         int(np.ceil(deviation.max())) + 1\n",
    "                        ) #-------------------------------------------------------- define the bins to discretize the deviation\n",
    "    digitization = np.digitize(deviation, \n",
    "                               bins=digit_bins\n",
    "                              ) #-------------------------------------------------- discretize deviation into unit bins\n",
    "\n",
    "    # plot the results\n",
    "    plot = Plot(rows=1, columns=3)\n",
    "\n",
    "    plot.series2D(deviation,\n",
    "                  axis=0,\n",
    "                  title='Deviation of the Predictions',\n",
    "                  xlabel='id of the predictions',\n",
    "                  ylabel='$y_{pred} - y_{true}$',\n",
    "                  binstep=14\n",
    "                 )\n",
    "\n",
    "    plot.hist2D(binarization,\n",
    "                axis=1,\n",
    "                title='Underestimating and Overestimating Predictions',\n",
    "                ylabel='#',\n",
    "                bins=[0,1,2],\n",
    "                labels=['underest.', 'overest.']\n",
    "               )\n",
    "\n",
    "    plot.hist2D(digitization,\n",
    "                axis=2,\n",
    "                title='Distribution of Errors',\n",
    "                ylabel='#',\n",
    "                bins=range(len(digit_bins)+1),\n",
    "                labels=['{} < {}'.format(n-1, n) for n in digit_bins]\n",
    "               )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "plot = error_visualisation(y_dev, y_dev_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'lin_reg_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LR we therefore see that:\n",
    "\n",
    "- even though the MSE seems good, the variance has a very large confidence interval,\n",
    "- in general linear regression seems to underestimate the true values,\n",
    "- the distribution of the errors is peaked but in any case spread across a larger number of values,\n",
    "- the $R^2$ and explained variance are similar but far from good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net\n",
    "\n",
    "We now consider the linear regression in the presence of $l_1$ and $l_2$ regularisation. We will need to study the hyperparameter space given by the choice of `fit_intercept` as in the linear regression, but also `alpha` and `l1-ratio` which control the regularisation: `alpha` $\\times$ `l1_ratio` is the magnitude of the $l_1$ regularisation, while `alpha` $\\times (1 -$ `l1_ratio` $) / 2$ controls the $l_2$ regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics      import mean_squared_error, explained_variance_score, r2_score\n",
    "from skopt                import gp_minimize\n",
    "from skopt.space          import Real, Integer\n",
    "from skopt.utils          import use_named_args\n",
    "\n",
    "# define the hyperparameter search space\n",
    "hyp_names  = ['fit_intercept', 'alpha', 'l1_ratio']\n",
    "hyp_search = [Integer(0, 1, name=hyp_names[0]),\n",
    "              Real(1.0e-7, 1.0e-4, prior='log-uniform', name=hyp_names[1]),\n",
    "              Real(0.0, 1.0, name=hyp_names[2])\n",
    "             ]\n",
    "\n",
    "# define the number of iterations of the optimisation\n",
    "n_calls    = 10\n",
    "\n",
    "# define the estimator\n",
    "el_net = ElasticNet(normalize=False, #------------------------ keep normalize = 0 since we already standardize the features\n",
    "                    max_iter=1e8,\n",
    "                    tol=1.0e-3,\n",
    "                    random_state=RAND\n",
    "                   )\n",
    "log.info('Fitting EN.')\n",
    "\n",
    "# define the objective function\n",
    "@use_named_args(hyp_search)\n",
    "def objective(**params):\n",
    "    '''\n",
    "    Compute the mean squared error on the validation set.\n",
    "    \n",
    "    Required arguments:\n",
    "        **params:  the hyperparameters search space.\n",
    "        \n",
    "    Returns:\n",
    "        the mean squared error of the dev set.\n",
    "    '''\n",
    "    \n",
    "    # fit the estimator to the train set\n",
    "    el_net.set_params(**params)\n",
    "    el_net.fit(X_train, y_train)\n",
    "    \n",
    "    # compute predictions on the dev set\n",
    "    y_dev_pred = el_net.predict(X_dev).reshape(-1,1)\n",
    "    \n",
    "    # compute the mean squared error\n",
    "    return mean_squared_error(y_dev, y_dev_pred)\n",
    "\n",
    "# save the results of the optimisation\n",
    "el_net_res = gp_minimize(objective, hyp_search, n_calls=n_calls, random_state=RAND, n_jobs=multi_thread)\n",
    "skp.dump(el_net_res, path.join(OUT_PATH, 'el_net_dev_opt_res.pkl.xz'), compress=('xz', 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the print the results and compute the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve best results, fit to the training set and print\n",
    "el_net.set_params(**dict(zip(hyp_names, el_net_res.x))) #----- set parameters to best possible choice\n",
    "el_net.fit(X_train, y_train) #-------------------------------- fit again on the training set\n",
    "y_dev_pred = el_net.predict(X_dev).reshape(-1,1) #------------ compute predictions\n",
    "\n",
    "# print the metrics\n",
    "def print_hyperparameters(names, results):\n",
    "    '''\n",
    "    Print best hyperparameters.\n",
    "    \n",
    "    Required arguments:\n",
    "        names:   the names of the hyperparameters,\n",
    "        results: results of the evaluation containing the hyperparameters.\n",
    "    '''\n",
    "    print('  Best hyperparameters:\\n')\n",
    "    for name, value in zip(names, results.x):\n",
    "        print('    {}: {}'.format(name, value))\n",
    "    \n",
    "def print_metrics(y_true, y_pred):\n",
    "    '''\n",
    "    Print regression metrics.\n",
    "    \n",
    "    Required arguments:\n",
    "        y_true: true values,\n",
    "        y_pred: predictions.\n",
    "    '''\n",
    "    print('\\n  MSE:  {:.3f}'.format(mean_squared_error(y_true, y_pred)))\n",
    "    print('  R2:   {:.3f}'.format(r2_score(y_true, y_pred)))\n",
    "    print('  EVAR: {:.3f}'.format(explained_variance_score(y_true, y_pred)))\n",
    "    \n",
    "print('Results for EN:\\n')\n",
    "print_hyperparameters(hyp_names, el_net_res)  \n",
    "print_metrics(y_dev, y_dev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results can then be completed by computing the confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interval = confidence_interval(y_dev, y_dev_pred, confidence=confidence)\n",
    "print('Confidence interval: [ {:.3f}, {:.3f} ]'.format(conf_interval[0].squeeze(), conf_interval[1].squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the results to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_res(el_net, 'el_net')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we visualise the distribution of the variance of the errors in plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = error_visualisation(y_dev, y_dev_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'el_net_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the introduction of regularisation we find:\n",
    "\n",
    "- in general the MSE is worse, and the confidence interval seems to be larger,\n",
    "- errors are in general underestimating, but they are confined in a smaller range,\n",
    "- overestimating errors tend to have larger absolute deviations,\n",
    "- $R^2$ and explained variance are again similar but worse than the simple linear regression,\n",
    "- the algorithm seems to prefer a very small $l_1$ regularization (any an almost vanishing $l_2$ regularisation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "Given the results of the elastic net, we implement the lasso regression which a pure $l_1$ regularisation. We will therefore control only one hyperparameter `alpha` which represents the magnitude of the regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics      import mean_squared_error, explained_variance_score, r2_score\n",
    "from skopt                import gp_minimize\n",
    "from skopt.space          import Real, Integer\n",
    "from skopt.utils          import use_named_args\n",
    "\n",
    "# define the hyperparameter search space\n",
    "hyp_names  = ['fit_intercept', 'alpha']\n",
    "hyp_search = [Integer(0, 1, name=hyp_names[0]),\n",
    "              Real(1.0e-5, 1.0e-2, prior='log-uniform', name=hyp_names[1])\n",
    "             ]\n",
    "\n",
    "# define the number of iterations of the optimisation\n",
    "n_calls    = 10\n",
    "\n",
    "# define the estimator\n",
    "lasso = Lasso(normalize=False, #------------------------ keep normalize = 0 since we already standardize the features\n",
    "              max_iter=1e8,\n",
    "              tol=1.0e-3,\n",
    "              random_state=RAND\n",
    "             )\n",
    "log.info('Fitting Lasso.')\n",
    "\n",
    "# define the objective function\n",
    "@use_named_args(hyp_search)\n",
    "def objective(**params):\n",
    "    '''\n",
    "    Compute the mean squared error on the validation set.\n",
    "    \n",
    "    Required arguments:\n",
    "        **params:  the hyperparameters search space.\n",
    "        \n",
    "    Returns:\n",
    "        the mean squared error of the dev set.\n",
    "    '''\n",
    "    \n",
    "    # fit the estimator to the train set\n",
    "    lasso.set_params(**params)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    \n",
    "    # compute predictions on the dev set\n",
    "    y_dev_pred = lasso.predict(X_dev).reshape(-1,1)\n",
    "    \n",
    "    # compute the mean squared error\n",
    "    return mean_squared_error(y_dev, y_dev_pred)\n",
    "\n",
    "# save the results of the optimisation\n",
    "lasso_res = gp_minimize(objective, hyp_search, n_calls=n_calls, random_state=RAND, n_jobs=multi_thread)\n",
    "skp.dump(lasso_res, path.join(OUT_PATH, 'lasso_dev_opt_res.pkl.xz'), compress=('xz', 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the print the results and compute the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve best results, fit to the training set and print\n",
    "lasso.set_params(**dict(zip(hyp_names, lasso_res.x))) #----- set parameters to best possible choice\n",
    "lasso.fit(X_train, y_train) #-------------------------------- fit again on the training set\n",
    "y_dev_pred = lasso.predict(X_dev).reshape(-1,1) #------------ compute predictions\n",
    "\n",
    "print('Results for LASSO:\\n')\n",
    "print_hyperparameters(hyp_names, lasso_res)\n",
    "print_metrics(y_dev, y_dev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results can then be completed by computing the confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interval = confidence_interval(y_dev, y_dev_pred, confidence=confidence)\n",
    "print('Confidence interval: [ {:.3f}, {:.3f} ]'.format(conf_interval[0].squeeze(), conf_interval[1].squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the results to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_res(lasso, 'lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we visualise the distribution of the variance of the errors in plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = error_visualisation(y_dev, y_dev_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'lasso_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the elastic net regression, we introduced just the $l_1$ regularised version of linear regression:\n",
    "\n",
    "- we find the same scores as for the elastic net ($l_2$ regularisation does not influence the results),\n",
    "- the confidence interval is the same as in the elastic net,\n",
    "- underestimating errors are restricted to a small interval, while overestimating errors are in general spread across several values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, it seems that adding the regularisation is not helping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVR\n",
    "\n",
    "We now study the support vector machine without using the kernel trick. We will therefore control a wider range of possible hyperparameters mainly related to the _soft margin_ implementation of SVR for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm     import LinearSVR\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, r2_score\n",
    "from skopt           import gp_minimize\n",
    "from skopt.space     import Real, Integer, Categorical\n",
    "from skopt.utils     import use_named_args\n",
    "\n",
    "# define the hyperparameter search space\n",
    "hyp_names  = ['fit_intercept', #------------ intercept fitting (True/False)\n",
    "              'epsilon', #------------------ penalty free boundary\n",
    "              'C', #------------------------ penalty magnitude\n",
    "              'loss', #--------------------- loss function (l1 or l2)\n",
    "              'intercept_scaling' #--------- scale the intercept (if fitting)\n",
    "             ]\n",
    "hyp_search = [Integer(0, 1, name=hyp_names[0]),\n",
    "              Real(1.0e-9, 1.0e-2, prior='log-uniform', name=hyp_names[1]),\n",
    "              Real(1.0e-4, 1.0e1, prior='log-uniform', name=hyp_names[2]),\n",
    "              Categorical(['epsilon_insensitive', 'squared_epsilon_insensitive'], name=hyp_names[3]),\n",
    "              Real(1.0e-3, 1.0e1, prior='log-uniform', name=hyp_names[4])\n",
    "             ]\n",
    "\n",
    "# define the number of iterations of the optimisation\n",
    "n_calls    = 20\n",
    "\n",
    "# define the estimator\n",
    "lin_svr = LinearSVR(dual=True,\n",
    "                    max_iter=1e7,\n",
    "                    random_state=RAND\n",
    "                   )\n",
    "log.info('Fitting l-SVR.')\n",
    "\n",
    "# define the objective function\n",
    "@use_named_args(hyp_search)\n",
    "def objective(**params):\n",
    "    '''\n",
    "    Compute the mean squared error on the validation set.\n",
    "    \n",
    "    Required arguments:\n",
    "        **params:  the hyperparameters search space.\n",
    "        \n",
    "    Returns:\n",
    "        the mean squared error of the dev set.\n",
    "    '''\n",
    "    \n",
    "    # fit the estimator to the train set\n",
    "    lin_svr.set_params(**params)\n",
    "    lin_svr.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # compute predictions on the dev set\n",
    "    y_dev_pred = lin_svr.predict(X_dev).reshape(-1,1)\n",
    "    \n",
    "    # compute the mean squared error\n",
    "    return mean_squared_error(y_dev, y_dev_pred)\n",
    "\n",
    "# save the results of the optimisation\n",
    "lin_svr_res = gp_minimize(objective, hyp_search, n_calls=n_calls, random_state=RAND, n_jobs=multi_thread)\n",
    "skp.dump(lin_svr_res, path.join(OUT_PATH, 'lin_svr_dev_opt_res.pkl.xz'), compress=('xz', 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the print the results and compute the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve best results, fit to the training set and print\n",
    "lin_svr.set_params(**dict(zip(hyp_names, lin_svr_res.x))) #---- set parameters to best possible choice\n",
    "lin_svr.fit(X_train, y_train.ravel()) #------------------------ fit again on the training set\n",
    "y_dev_pred = lin_svr.predict(X_dev).reshape(-1,1) #------------ compute predictions\n",
    "\n",
    "print('Results for l-SVR:\\n')\n",
    "print_hyperparameters(hyp_names, lin_svr_res)\n",
    "print_metrics(y_dev, y_dev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results can then be completed by computing the confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interval = confidence_interval(y_dev, y_dev_pred, confidence=confidence)\n",
    "print('Confidence interval: [ {:.3f}, {:.3f} ]'.format(conf_interval[0].squeeze(), conf_interval[1].squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the results to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_res(lin_svr, 'lin_svr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we visualise the distribution of the variance of the errors in plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = error_visualisation(y_dev, y_dev_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'lin_svr_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the support vectors without the kernel trick do not improve the situation seen with the linear regression:\n",
    "\n",
    "- we find the same scores as for the elastic net,\n",
    "- the confidence interval is more restricted, signalling in any case a better convergence of the values,\n",
    "- underestimating errors are restricted to a small interval, while overestimating errors are in general spread across several values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR (Gaussian kernel)\n",
    "\n",
    "We then focus on SVR with the kernel trick: we consider a Gaussian kernel (_rbf_) and implement the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm     import SVR\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, r2_score\n",
    "from skopt           import gp_minimize\n",
    "from skopt.space     import Real, Integer, Categorical\n",
    "from skopt.utils     import use_named_args\n",
    "\n",
    "# define the hyperparameter search space\n",
    "hyp_names  = ['gamma', #------------------- variance^{-1} of the kernel distribution\n",
    "              'epsilon', #----------------- penalty free boundary\n",
    "              'C' #------------------------ magnitude of the penalty\n",
    "             ]\n",
    "hyp_search = [Real(1.0e-3, 1.0e-1, prior='log-uniform', name=hyp_names[0]),\n",
    "              Real(1.0e-9, 1.0e-2, prior='log-uniform', name=hyp_names[1]),\n",
    "              Real(1.0e-4, 1.0e1, prior='log-uniform', name=hyp_names[2])\n",
    "             ]\n",
    "\n",
    "# define the number of iterations of the optimisation\n",
    "n_calls    = 30\n",
    "\n",
    "# define the estimator\n",
    "svr_rbf = SVR(kernel='rbf')\n",
    "log.info('Fitting SVR.')\n",
    "\n",
    "# define the objective function\n",
    "@use_named_args(hyp_search)\n",
    "def objective(**params):\n",
    "    '''\n",
    "    Compute the mean squared error on the validation set.\n",
    "    \n",
    "    Required arguments:\n",
    "        **params:  the hyperparameters search space.\n",
    "        \n",
    "    Returns:\n",
    "        the mean squared error of the dev set.\n",
    "    '''\n",
    "    \n",
    "    # fit the estimator to the train set\n",
    "    svr_rbf.set_params(**params)\n",
    "    svr_rbf.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # compute predictions on the dev set\n",
    "    y_dev_pred = svr_rbf.predict(X_dev).reshape(-1,1)\n",
    "    \n",
    "    # compute the mean squared error\n",
    "    return mean_squared_error(y_dev, y_dev_pred)\n",
    "\n",
    "# save the results of the optimisation\n",
    "svr_rbf_res = gp_minimize(objective, hyp_search, n_calls=n_calls, random_state=RAND, n_jobs=multi_thread)\n",
    "skp.dump(svr_rbf_res, path.join(OUT_PATH, 'svr_rbf_dev_opt_res.pkl.xz'), compress=('xz', 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the print the results and compute the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve best results, fit to the training set and print\n",
    "svr_rbf.set_params(**dict(zip(hyp_names, svr_rbf_res.x))) #---- set parameters to best possible choice\n",
    "svr_rbf.fit(X_train, y_train.ravel()) #------------------------ fit again on the training set\n",
    "y_dev_pred = svr_rbf.predict(X_dev).reshape(-1,1) #------------ compute predictions\n",
    "\n",
    "print('Results for SVR (rbf kernel):\\n')\n",
    "print_hyperparameters(hyp_names, svr_rbf_res)\n",
    "print_metrics(y_dev, y_dev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results can then be completed by computing the confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interval = confidence_interval(y_dev, y_dev_pred, confidence=confidence)\n",
    "print('Confidence interval: [ {:.3f}, {:.3f} ]'.format(conf_interval[0].squeeze(), conf_interval[1].squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the results to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_res(svr_rbf, 'svr_rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we visualise the distribution of the variance of the errors in plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = error_visualisation(y_dev, y_dev_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'svr_rbf_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The introduction of the kernel trick seems to drastically improve the situation:\n",
    "\n",
    "- the value of the MSE is significantly reduces and the $R^2$ and explained variance show a better results,\n",
    "- the confidence interval peaked around the value of the MSE,\n",
    "- underestimating and overestimating errors seem to be balanced and restricted to smaller intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "We then consider the predictions given by a standalone forest of fully grown decision trees. In this case the number of tunable hyperparameters is quite high, but we can expect a shorter training time given the restricted amount of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost       as xgb\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, r2_score\n",
    "from skopt           import gp_minimize\n",
    "from skopt.space     import Real, Integer, Categorical\n",
    "from skopt.utils     import use_named_args\n",
    "\n",
    "# convert training data to DMatrix for training\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "ddev   = xgb.DMatrix(X_dev,   label=y_dev)\n",
    "dtest  = xgb.DMatrix(X_test,  label=y_test)\n",
    "\n",
    "# define the hyperparameter search space\n",
    "hyp_names  = ['max_depth', #----------------- max. depth of a tree [1, inf)\n",
    "              'min_child_weight', #---------- min. sum of instance weight (Hessian) needed in a child [0, inf)\n",
    "              'gamma', #--------------------- min. loss reduction required to make a further partition on a leaf node of the tree [0, inf)\n",
    "              'subsample', #----------------- subsample ratio of the training instances (0,1]\n",
    "              'colsample_bytree', #---------- subsample ratio of columns when constructing each tree (0,1]\n",
    "              'lambda', #-------------------- l2 regularisation\n",
    "              'alpha', #--------------------- l1 regularisation\n",
    "              'num_parallel_tree' #---------- number of trees in the forest\n",
    "             ]\n",
    "hyp_search = [Integer(3, 100, name=hyp_names[0]),\n",
    "              Real(1.0e-2, 1.0e-1, prior='log-uniform', name=hyp_names[1]),\n",
    "              Real(1.0e-2, 1.0e2, prior='log-uniform', name=hyp_names[2]),\n",
    "              Real(0.01, 1.0, name=hyp_names[3]),\n",
    "              Real(0.01, 1.0, name=hyp_names[4]),\n",
    "              Real(1.0e-6, 1.0e-1, prior='log-uniform', name=hyp_names[5]),\n",
    "              Real(1.0e-6, 1.0e-1, prior='log-uniform', name=hyp_names[6]),\n",
    "              Integer(2, 75, name=hyp_names[7])\n",
    "             ]\n",
    "log.info('Fitting RF.')\n",
    "\n",
    "# define the number of iterations of the optimisation\n",
    "n_calls    = 30\n",
    "\n",
    "# define the objective function\n",
    "@use_named_args(hyp_search)\n",
    "def objective(**params):\n",
    "    '''\n",
    "    Compute the mean squared error on the validation set.\n",
    "    \n",
    "    Required arguments:\n",
    "        **params:  the hyperparameters search space.\n",
    "        \n",
    "    Returns:\n",
    "        the mean squared error of the dev set.\n",
    "    '''\n",
    "    \n",
    "    # add more parameters\n",
    "    params['learning_rate'] = 1\n",
    "    params['objective']     = 'reg:squarederror'\n",
    "    params['tree_method']   = 'hist'\n",
    "    params['seed']          = RAND\n",
    "    \n",
    "    # fit the estimator\n",
    "    rnd_for = xgb.train(params, dtrain, num_boost_round=1, evals=[(ddev, 'eval'), (dtrain, 'train')], verbose_eval=False)\n",
    "    \n",
    "    # compute predictions on the dev set and release the memory\n",
    "    y_dev_pred = rnd_for.predict(ddev).reshape(-1,1)\n",
    "    \n",
    "    # compute the mean squared error\n",
    "    return mean_squared_error(y_dev, y_dev_pred)\n",
    "\n",
    "# save the results of the optimisation\n",
    "rnd_for_res = gp_minimize(objective, hyp_search, n_calls=n_calls, random_state=RAND, n_jobs=multi_thread)\n",
    "skp.dump(rnd_for_res, path.join(OUT_PATH, 'rnd_for_dev_opt_res.pkl.xz'), compress=('xz', 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the print the results and compute the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve best results, fit to the training set and print\n",
    "params = dict(zip(hyp_names, rnd_for_res.x)) #--------------------------- set parameters to best possible choice\n",
    "rnd_for = xgb.train(params,\n",
    "                    dtrain,\n",
    "                    num_boost_round=1,\n",
    "                    evals=[(ddev, 'eval'), (dtrain, 'train')],\n",
    "                    verbose_eval=False) #-------------------------------- fit again on the training set\n",
    "y_dev_pred = rnd_for.predict(ddev).reshape(-1,1) #----------------------- compute predictions\n",
    "\n",
    "print('Results for the random forest:\\n')\n",
    "print_hyperparameters(hyp_names, rnd_for_res)\n",
    "print_metrics(y_dev, y_dev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results can then be completed by computing the confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interval = confidence_interval(y_dev, y_dev_pred, confidence=confidence)\n",
    "print('Confidence interval: [ {:.3f}, {:.3f} ]'.format(conf_interval[0].squeeze(), conf_interval[1].squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the results to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_res(rnd_for, 'rnd_for')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we visualise the distribution of the variance of the errors in plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = error_visualisation(y_dev, y_dev_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'rnd_for_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees seem to slighlty improve the confidence of the results but do not significantly improve the situation:\n",
    "\n",
    "- the values of the MSE and the other scores are comparable with linear regression (w/o regularisation),\n",
    "- the confidence interval is definitely more peaked,\n",
    "- errors seem to be mainly underestimating the true values and are contained in small intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted Trees\n",
    "\n",
    "We then implement the gradient boosting of shallow trees to improve previous results. In this case we add boosting rounds while we keep only one tree per round:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost       as xgb\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, r2_score\n",
    "from skopt           import gp_minimize\n",
    "from skopt.space     import Real, Integer, Categorical\n",
    "from skopt.utils     import use_named_args\n",
    "\n",
    "# convert training data to DMatrix for training\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "ddev   = xgb.DMatrix(X_dev,   label=y_dev)\n",
    "\n",
    "# define the hyperparameter search space\n",
    "hyp_names  = ['max_depth', #----------------- max. depth of a tree [1, inf)\n",
    "              'min_child_weight', #---------- min. sum of instance weight (Hessian) needed in a child [0, inf)\n",
    "              'gamma', #--------------------- min. loss reduction required to make a further partition on a leaf node of the tree [0, inf)\n",
    "              'subsample', #----------------- subsample ratio of the training instances (0,1]\n",
    "              'colsample_bytree', #---------- subsample ratio of columns when constructing each tree (0,1]\n",
    "              'lambda', #-------------------- l2 regularisation\n",
    "              'alpha', #--------------------- l1 regularisation\n",
    "              'learning_rate', #------------- learning rate of gradient descent\n",
    "              'num_boost_round' #------------ number of boosting rounds\n",
    "             ]\n",
    "hyp_search = [Integer(1, 30, name=hyp_names[0]),\n",
    "              Real(1.0e-2, 1.0e-1, prior='log-uniform', name=hyp_names[1]),\n",
    "              Real(1.0e-2, 1.0e2, prior='log-uniform', name=hyp_names[2]),\n",
    "              Real(0.01, 1.0, name=hyp_names[3]),\n",
    "              Real(0.01, 1.0, name=hyp_names[4]),\n",
    "              Real(1.0e-6, 1.0e-1, prior='log-uniform', name=hyp_names[5]),\n",
    "              Real(1.0e-6, 1.0e-1, prior='log-uniform', name=hyp_names[6]),\n",
    "              Real(1.0e-5, 1.0e-1, prior='log-uniform', name=hyp_names[7]),\n",
    "              Integer(2, 1000, name=hyp_names[8])\n",
    "             ]\n",
    "log.info('Fitting BDT.')\n",
    "\n",
    "# define the number of iterations of the optimisation\n",
    "n_calls    = 30\n",
    "\n",
    "# define the objective function\n",
    "@use_named_args(hyp_search)\n",
    "def objective(**params):\n",
    "    '''\n",
    "    Compute the mean squared error on the validation set.\n",
    "    \n",
    "    Required arguments:\n",
    "        **params:  the hyperparameters search space.\n",
    "        \n",
    "    Returns:\n",
    "        the mean squared error of the dev set.\n",
    "    '''\n",
    "    # save the number of boosting rounds\n",
    "    num_boost_round = params['num_boost_round']\n",
    "    \n",
    "    # create new dictionary without num_boost_round\n",
    "    params = {key: value for key, value in params.items() if key != 'num_boost_round'}\n",
    "    params['seed'] = RAND\n",
    "    \n",
    "    # add more parameters\n",
    "    params['num_parallel_tree'] = 1\n",
    "    params['objective']         = 'reg:squarederror'\n",
    "    params['tree_method']       = 'hist'\n",
    "    \n",
    "    # fit the estimator\n",
    "    bst_tree = xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=[(ddev, 'eval'), (dtrain, 'train')], verbose_eval=False)\n",
    "    \n",
    "    # compute predictions on the dev set and release the memory\n",
    "    y_dev_pred = bst_tree.predict(ddev).reshape(-1,1)\n",
    "    del bst_tree\n",
    "    \n",
    "    # compute the mean squared error\n",
    "    return mean_squared_error(y_dev, y_dev_pred)\n",
    "\n",
    "# save the results of the optimisation\n",
    "bst_tree_res = gp_minimize(objective, hyp_search, n_calls=n_calls, random_state=RAND, n_jobs=multi_thread)\n",
    "skp.dump(bst_tree_res, path.join(OUT_PATH, 'bst_tree_dev_opt_res.pkl.xz'), compress=('xz', 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the print the results and compute the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve best results, fit to the training set and print\n",
    "params = dict(zip(hyp_names, bst_tree_res.x)) #------------------------------------------ set parameters to best possible choice\n",
    "num_boost_round = params['num_boost_round'] #-------------------------------------------- save the value of the boosting rounds\n",
    "params = {key: value for key, value in params.items() if key != 'num_boost_round'} #----- create new dictionary w/o boosting rounds\n",
    "bst_tree = xgb.train(params,\n",
    "                     dtrain,\n",
    "                     num_boost_round=num_boost_round,\n",
    "                     evals=[(ddev, 'eval'), (dtrain, 'train')],\n",
    "                     verbose_eval=False) #----------------------------------------------- fit again on the training set\n",
    "y_dev_pred = bst_tree.predict(ddev).reshape(-1,1) #-------------------------------------- compute predictions\n",
    "\n",
    "print('Results for the boosted trees:\\n')\n",
    "print_hyperparameters(hyp_names, bst_tree_res)\n",
    "print_metrics(y_dev, y_dev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results can then be completed by computing the confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interval = confidence_interval(y_dev, y_dev_pred, confidence=confidence)\n",
    "print('Confidence interval: [ {:.3f}, {:.3f} ]'.format(conf_interval[0].squeeze(), conf_interval[1].squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the results to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_res(bst_tree, 'bst_tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we visualise the distribution of the variance of the errors in plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = error_visualisation(y_dev, y_dev_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'bst_tree_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting shows a vast improvement on the previous decision tree implementation:\n",
    "\n",
    "- the values of the MSE and the other scores are comparable with SVR,\n",
    "- the confidence interval is definitely peaked,\n",
    "- errors seem to be mainly underestimating the true values and are contained in small intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted Forests\n",
    "\n",
    "As a curiosity we also implement boosted forests of decision trees to see whether this improves anything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost       as xgb\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, r2_score\n",
    "from skopt           import gp_minimize\n",
    "from skopt.space     import Real, Integer, Categorical\n",
    "from skopt.utils     import use_named_args\n",
    "\n",
    "# convert training data to DMatrix for training\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "ddev   = xgb.DMatrix(X_dev,   label=y_dev)\n",
    "\n",
    "# define the hyperparameter search space\n",
    "hyp_names  = ['max_depth', #----------------- max. depth of a tree [1, inf)\n",
    "              'min_child_weight', #---------- min. sum of instance weight (Hessian) needed in a child [0, inf)\n",
    "              'gamma', #--------------------- min. loss reduction required to make a further partition on a leaf node of the tree [0, inf)\n",
    "              'subsample', #----------------- subsample ratio of the training instances (0,1]\n",
    "              'colsample_bytree', #---------- subsample ratio of columns when constructing each tree (0,1]\n",
    "              'lambda', #-------------------- l2 regularisation\n",
    "              'alpha', #--------------------- l1 regularisation\n",
    "              'num_parallel_tree', #--------- number of trees in the forest\n",
    "              'learning_rate', #------------- learning rate of gradient descent\n",
    "              'num_boost_round' #------------ number of boosting rounds\n",
    "             ]\n",
    "hyp_search = [Integer(1, 30, name=hyp_names[0]),\n",
    "              Real(1.0e-2, 1.0e-1, prior='log-uniform', name=hyp_names[1]),\n",
    "              Real(1.0e-2, 1.0e2, prior='log-uniform', name=hyp_names[2]),\n",
    "              Real(0.01, 1.0, name=hyp_names[3]),\n",
    "              Real(0.01, 1.0, name=hyp_names[4]),\n",
    "              Real(1.0e-6, 1.0e-1, prior='log-uniform', name=hyp_names[5]),\n",
    "              Real(1.0e-6, 1.0e-1, prior='log-uniform', name=hyp_names[6]),\n",
    "              Integer(2, 10, name=hyp_names[7]),\n",
    "              Real(1.0e-5, 1.0e-1, prior='log-uniform', name=hyp_names[8]),\n",
    "              Integer(2, 1000, name=hyp_names[9])\n",
    "             ]\n",
    "log.info('Fitting BRF.')\n",
    "\n",
    "# define the number of iterations of the optimisation\n",
    "n_calls    = 20\n",
    "\n",
    "# define the objective function\n",
    "@use_named_args(hyp_search)\n",
    "def objective(**params):\n",
    "    '''\n",
    "    Compute the mean squared error on the validation set.\n",
    "    \n",
    "    Required arguments:\n",
    "        **params:  the hyperparameters search space.\n",
    "        \n",
    "    Returns:\n",
    "        the mean squared error of the dev set.\n",
    "    '''\n",
    "    # save the number of boosting rounds\n",
    "    num_boost_round = params['num_boost_round']\n",
    "    \n",
    "    # create new dictionary without num_boost_round\n",
    "    params = {key: value for key, value in params.items() if key != 'num_boost_round'}\n",
    "    \n",
    "    # add more parameters\n",
    "    params['objective']   = 'reg:squarederror'\n",
    "    params['tree_method'] = 'hist'\n",
    "    params['seed']        = RAND\n",
    "    \n",
    "    # fit the estimator\n",
    "    bst_for = xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=[(ddev, 'eval'), (dtrain, 'train')], verbose_eval=False)\n",
    "    \n",
    "    # compute predictions on the dev set and release the memory\n",
    "    y_dev_pred = bst_for.predict(ddev).reshape(-1,1)\n",
    "    del bst_for\n",
    "    \n",
    "    # compute the mean squared error\n",
    "    return mean_squared_error(y_dev, y_dev_pred)\n",
    "\n",
    "# save the results of the optimisation\n",
    "bst_for_res = gp_minimize(objective, hyp_search, n_calls=n_calls, random_state=RAND, n_jobs=multi_thread)\n",
    "skp.dump(bst_for_res, path.join(OUT_PATH, 'bst_for_dev_opt_res.pkl.xz'), compress=('xz', 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the print the results and compute the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve best results, fit to the training set and print\n",
    "params = dict(zip(hyp_names, bst_for_res.x)) #------------------------------------------- set parameters to best possible choice\n",
    "num_boost_round = params['num_boost_round'] #-------------------------------------------- save the value of the boosting rounds\n",
    "params = {key: value for key, value in params.items() if key != 'num_boost_round'} #----- create new dictionary w/o boosting rounds\n",
    "bst_for = xgb.train(params,\n",
    "                    dtrain,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    evals=[(ddev, 'eval'), (dtrain, 'train')],\n",
    "                    verbose_eval=False) #------------------------------------------------ fit again on the training set\n",
    "y_dev_pred = bst_for.predict(ddev).reshape(-1,1) #--------------------------------------- compute predictions\n",
    "\n",
    "print('Results for the boosted forest of trees:\\n')\n",
    "print_hyperparameters(hyp_names, bst_for_res)\n",
    "print_metrics(y_dev, y_dev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results can then be completed by computing the confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interval = confidence_interval(y_dev, y_dev_pred, confidence=confidence)\n",
    "print('Confidence interval: [ {:.3f}, {:.3f} ]'.format(conf_interval[0].squeeze(), conf_interval[1].squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the results to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_res(bst_for, 'bst_for')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we visualise the distribution of the variance of the errors in plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = error_visualisation(y_dev, y_dev_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'bst_for_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosted forests of trees seem to lead to the best results on the development set:\n",
    "\n",
    "- MSE and other scores are the best results so far,\n",
    "- the confidence interval is definitely peaked,\n",
    "- errors seem to be mainly underestimating the true values but the number of large errors is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "We finally implement an analysis based on ANNs. Given the small amount of sample data we try to keep the architecture as simple as possible. Given the distribution of the data we consider a fully connected (FC) ANN built using [Tensorflow](https://www.tensorflow.org/) and [Keras](https://www.tensorflow.org/guide/keras), its high level API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras              import Sequential, metrics, losses\n",
    "from tensorflow.keras.layers       import InputLayer, Dense, Activation, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers   import Adam\n",
    "\n",
    "# define the input shape\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# define the batch size for training\n",
    "batch_size = X_train.shape[0]\n",
    "\n",
    "# build the model\n",
    "def ann_model(n_layers=1,\n",
    "              n_units=10,\n",
    "              learning_rate=0.1,\n",
    "              epochs=10,\n",
    "              activation='relu',\n",
    "              slope=0.3,\n",
    "              dropout=True,\n",
    "              dropout_rate=0.2,\n",
    "              batch_normalization=True,\n",
    "              momentum=0.99,\n",
    "              callbacks=None,\n",
    "              verbose=0\n",
    "             ):\n",
    "    '''\n",
    "    Create and return a compiled Tensorflow model.\n",
    "    \n",
    "    Required parameters:\n",
    "    \n",
    "    Optional parameters:\n",
    "        n_layers:            the number of fully connected layers to insert,\n",
    "        n_units:             the number of units in each layer,\n",
    "        learning_rate:       the learning rate of gradient descent,\n",
    "        epochs:              the number of epochs for training,\n",
    "        activation:          the name of the activation function ('relu' for ReLU or else for LeakyReLU)\n",
    "        slope:               the slope of the LeakyReLU activation (ignored if ReLU),\n",
    "        dropout:             whether to use dropout,\n",
    "        dropout_rate:        the dropout rate (ignored if no dropout),\n",
    "        batch_normalization: whether to use batch normalization,\n",
    "        momentum:            the momentum of batch normalization,\n",
    "        callbacks:           the list of callbacks,\n",
    "        verbose:             verbosity level.\n",
    "    \n",
    "    Returns:\n",
    "        the fitted model and its history.\n",
    "    '''\n",
    "\n",
    "    # instantiate the model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = Sequential(name='sft_trunc') #----------------------------------------------- create the model\n",
    "    model.add(InputLayer(input_shape=input_shape, name='input')) #----------------------- add the input layer of shape X_train.shape[1:]\n",
    "    \n",
    "    # add FC layers\n",
    "    for n in range(n_layers):\n",
    "        model.add(Dense(units=n_units,\n",
    "                        kernel_initializer=glorot_uniform(seed=RAND),\n",
    "                        bias_initializer=tf.zeros_initializer(),\n",
    "                        name='dense_{:d}'.format(n)\n",
    "                       )\n",
    "                 ) #--------------------------------------------------------------------- add FC layer\n",
    "        if activation == 'relu':\n",
    "            model.add(Activation('relu',\n",
    "                                 name='activation_{:d}'.format(n)\n",
    "                                )\n",
    "                     )\n",
    "        else:\n",
    "            model.add(LeakyReLU(alpha=slope,\n",
    "                                name='activation_{:d}'.format(n)\n",
    "                               )\n",
    "                     ) #----------------------------------------------------------------- add activation layer\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization(momentum=momentum,\n",
    "                                         name='batch_norm_{:d}'.format(n)\n",
    "                                        )\n",
    "                     ) #----------------------------------------------------------------- add batch normalization layer\n",
    "        if dropout:\n",
    "            model.add(Dropout(rate=dropout_rate,\n",
    "                              seed=RAND,\n",
    "                              name='dropout_{:d}'.format(n)\n",
    "                             )\n",
    "                     ) #----------------------------------------------------------------- add dropout layer\n",
    "        \n",
    "    # add the output layer\n",
    "    model.add(Dense(units=1,\n",
    "                    kernel_initializer=glorot_uniform(seed=RAND),\n",
    "                    bias_initializer=tf.zeros_initializer(),\n",
    "                    name='output'\n",
    "                   )\n",
    "             ) #------------------------------------------------------------------------- add output layer\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizers=Adam(learning_rate=learning_rate),\n",
    "                  loss=losses.MeanSquaredError(),\n",
    "                  metrics=[metrics.MeanSquaredError(),\n",
    "                           metrics.RootMeanSquaredError(),\n",
    "                           metrics.MeanAbsoluteError()\n",
    "                          ]\n",
    "                 )\n",
    "    \n",
    "    # fit the model\n",
    "    model_history = model.fit(x=X_train,\n",
    "                              y=y_train,\n",
    "                              batch_size=batch_size,\n",
    "                              epochs=epochs,\n",
    "                              verbose=verbose,\n",
    "                              validation_data=(X_dev, y_dev),\n",
    "                              callbacks=callbacks\n",
    "                             )\n",
    "    \n",
    "    # return the compiled model\n",
    "    return model, model_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the objective function on which to perform the optimisation of the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt       import gp_minimize\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# define the hyperparameter search space\n",
    "hyp_names  = ['n_layers', #----------------- the number of fully connected layers to insert,\n",
    "              'n_units', #------------------ the number of units in each layer,\n",
    "              'learning_rate', #------------ the learning rate of gradient descent,\n",
    "              'epochs', #------------------- the number of epochs for training,\n",
    "              'activation', #--------------- the name of the activation function ('relu' for ReLU or else for LeakyReLU)\n",
    "              'slope', #-------------------- the slope of the LeakyReLU activation (ignored if ReLU),\n",
    "              'dropout', #------------------ whether to use dropout,\n",
    "              'dropout_rate', #------------- the dropout rate (ignored if no dropout),\n",
    "              'batch_normalization', #------ whether to use batch normalization,\n",
    "              'momentum', #----------------- the momentum of batch normalization\n",
    "             ]\n",
    "hyp_search = [Integer(1, 100, name=hyp_names[0]),\n",
    "              Integer(1, 30, name=hyp_names[1]),\n",
    "              Real(1.0e-4, 1.0e-1, prior='log-uniform', name=hyp_names[2]),\n",
    "              Integer(10, 3000, name=hyp_names[3]),\n",
    "              Categorical(['relu', 'leaky'], name=hyp_names[4]),\n",
    "              Real(1.0e-2, 1.0e1, prior='log-uniform', name=hyp_names[5]),\n",
    "              Integer(0, 1, name=hyp_names[6]),\n",
    "              Real(1.0e-3, 5e-1, prior='log-uniform', name=hyp_names[7]),\n",
    "              Integer(0, 1, name=hyp_names[8]),\n",
    "              Real(0.0, 0.999, name=hyp_names[9])\n",
    "             ]\n",
    "log.info('Fitting ANN.')\n",
    "\n",
    "# define the number of iterations of the optimisation\n",
    "n_calls    = 20\n",
    "\n",
    "# define the objective function\n",
    "@use_named_args(hyp_search)\n",
    "def objective(**params):\n",
    "    '''\n",
    "    Compute the mean squared error on the validation set.\n",
    "    \n",
    "    Required arguments:\n",
    "        **params:  the hyperparameters search space.\n",
    "        \n",
    "    Returns:\n",
    "        the loss function value of the dev set.\n",
    "    '''\n",
    "    # create the model\n",
    "    _, history = ann_model(**params)\n",
    "    \n",
    "    # return the minimum value of the loss achieved by the model during training\n",
    "    return np.min(history['val_loss'])\n",
    "\n",
    "# save the results of the optimisation\n",
    "ann_res = gp_minimize(objective, hyp_search, n_calls=n_calls, random_state=RAND, n_jobs=1)\n",
    "skp.dump(ann_res, path.join(OUT_PATH, 'ann_fc_dev_opt_res.pkl.xz'), compress=('xz', 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the print the results and compute the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# define a set of callbacks to save the best model\n",
    "callbacks = [ModelCheckpoint(filepath=path.join(MOD_PATH, 'ann_fc.h5'),\n",
    "                             monitor='val_loss',\n",
    "                             verbose=0,\n",
    "                             save_best_only=True\n",
    "                            )\n",
    "            ]\n",
    "\n",
    "# retrieve best results, fit to the training set and print\n",
    "params = dict(zip(hyp_names, ann_res.x)) #---------------------------------------------- set parameters to best possible choice\n",
    "ann_fc, ann_fc_history = ann_model(callbacks=callbacks, verbose=0, **params)\n",
    "\n",
    "# compute the predictions\n",
    "y_dev_pred = ann_fc.predict(X_dev).reshape(-1,1) #----------------------------------------- compute predictions\n",
    "\n",
    "# save predictions to file\n",
    "dev_predictions = {'y_dev_true': y_dev.squeeze().tolist(),\n",
    "                   'y_dev_pred': y_dev_pred.squeeze().tolist(),\n",
    "                   'y_dev_err':  (y_dev_pred - y_dev).squeeze().tolist()\n",
    "                  }\n",
    "joblib.dump(dev_predictions, path.join(OUT_PATH, 'ann_fc_dev_preds.pkl.xz'), compress=('xz', 9))\n",
    "\n",
    "# print the results\n",
    "print('Results for the ANN:\\n')\n",
    "print_hyperparameters(hyp_names, ann_res)\n",
    "print_metrics(y_dev, y_dev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results can then be completed by computing the confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interval = confidence_interval(y_dev, y_dev_pred, confidence=confidence)\n",
    "print('Confidence interval: [ {:.3f}, {:.3f} ]'.format(conf_interval[0].squeeze(), conf_interval[1].squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of neural networks, it may be a good idea to plot and study the loss and metrics functions during training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltools.libplot import Plot\n",
    "\n",
    "plot = Plot(rows=1, columns=4)\n",
    "\n",
    "plot.series2D(data=ann_fc_history['loss'],\n",
    "              axis=0,\n",
    "              title='Training and Validation Loss',\n",
    "              xlabel='epoch',\n",
    "              ylabel='loss',\n",
    "              legend='training',\n",
    "              ylog=False,\n",
    "              binstep=int(np.shape(ann_fc_history['loss'])[0] / 8)\n",
    "             )\\\n",
    "    .series2D(data=ann_fc_history['val_loss'],\n",
    "              axis=0,\n",
    "              title='Training and Validation Loss',\n",
    "              xlabel='epoch',\n",
    "              ylabel='loss',\n",
    "              legend='validation',\n",
    "              ylog=False,\n",
    "              binstep=int(np.shape(ann_fc_history['loss'])[0] / 8)\n",
    "             )\n",
    "plot.series2D(data=ann_fc_history['mean_squared_error'],\n",
    "              axis=1,\n",
    "              title='Training and Validation MSE',\n",
    "              xlabel='epoch',\n",
    "              ylabel='mse',\n",
    "              legend='training',\n",
    "              ylog=False,\n",
    "              binstep=int(np.shape(ann_fc_history['mean_squared_error'])[0] / 8)\n",
    "             )\\\n",
    "    .series2D(data=ann_fc_history['val_mean_squared_error'],\n",
    "              axis=1,\n",
    "              title='Training and Validation MSE',\n",
    "              xlabel='epoch',\n",
    "              ylabel='mse',\n",
    "              legend='validation',\n",
    "              ylog=False,\n",
    "              binstep=int(np.shape(ann_fc_history['mean_squared_error'])[0] / 8)\n",
    "             )\n",
    "plot.series2D(data=ann_fc_history['root_mean_squared_error'],\n",
    "              axis=2,\n",
    "              title='Training and Validation RMSE',\n",
    "              xlabel='epoch',\n",
    "              ylabel='rmse',\n",
    "              legend='training',\n",
    "              ylog=False,\n",
    "              binstep=int(np.shape(ann_fc_history['root_mean_squared_error'])[0] / 8)\n",
    "             )\\\n",
    "    .series2D(data=ann_fc_history['val_root_mean_squared_error'],\n",
    "              axis=2,\n",
    "              title='Training and Validation RMSE',\n",
    "              xlabel='epoch',\n",
    "              ylabel='rmse',\n",
    "              legend='validation',\n",
    "              ylog=False,\n",
    "              binstep=int(np.shape(ann_fc_history['root_mean_squared_error'])[0] / 8)\n",
    "             )\n",
    "plot.series2D(data=ann_fc_history['mean_absolute_error'],\n",
    "              axis=3,\n",
    "              title='Training and Validation MAE',\n",
    "              xlabel='epoch',\n",
    "              ylabel='mae',\n",
    "              legend='training',\n",
    "              ylog=False,\n",
    "              binstep=int(np.shape(ann_fc_history['mean_absolute_error'])[0] / 8)\n",
    "             )\\\n",
    "    .series2D(data=ann_fc_history['val_mean_absolute_error'],\n",
    "              axis=3,\n",
    "              title='Training and Validation MAE',\n",
    "              xlabel='epoch',\n",
    "              ylabel='mae',\n",
    "              legend='validation',\n",
    "              ylog=False,\n",
    "              binstep=int(np.shape(ann_fc_history['mean_absolute_error'])[0] / 8)\n",
    "             )\n",
    "\n",
    "plot.save_and_close(path.join(IMG_PATH, 'ann_fc_metrics_plot'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the results of the training to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "# write history file\n",
    "joblib.dump(ann_fc_history, path.join(OUT_PATH, 'ann_fc_history.pkl.xz'), compress=('xz',9))\n",
    "\n",
    "# write summary\n",
    "with open(path.join(OUT_PATH, 'ann_fc_config.json'), 'w') as f:\n",
    "    json.dump(ann_fc.get_config(), f)\n",
    "    \n",
    "# print summary\n",
    "ann_fc.summary()\n",
    "    \n",
    "# print model image\n",
    "ann_fc_dot = model_to_dot(model=ann_fc_model,\n",
    "                          show_shapes=True,\n",
    "                          show_layer_names=True,\n",
    "                          rankdir='TB',\n",
    "                          expand_nested=False,\n",
    "                          dpi=96,\n",
    "                          subgraph=False)\n",
    "ann_fc_dot.write_pdf(path.join(IMG_PATH, 'ann_fc_architecture.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we visualise the distribution of the variance of the errors in plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = error_visualisation(y_dev, y_dev_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'ann_fc_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ANN does not look to improve the results of the trees. However we can see that:\n",
    "\n",
    "- MSE and other metrics are comparable with previous linear regressoin models,\n",
    "- the confidence interval is smaller than linear regression models,\n",
    "- errors seem to be mainly underestimating the true values but the number of large errors is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Performance Evaluation\n",
    "\n",
    "We then show the predictions on the test set of the considered algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# compute the predictions\n",
    "y_test_pred = lin_reg.predict(X_test).reshape(-1,1)\n",
    "\n",
    "# print the metrics\n",
    "print('Results for LR:')\n",
    "print_metrics(y_test, y_test_pred)\n",
    "\n",
    "# save the predictions\n",
    "def save_res_test(short_name):\n",
    "    '''\n",
    "    Save results of the test predictions to file.\n",
    "    \n",
    "    Required arguments:\n",
    "        short_name: a short identifier of the estimator considered.\n",
    "    '''\n",
    "    # save predictions to file\n",
    "    test_predictions = {'y_dev_true': y_test.squeeze().tolist(),\n",
    "                        'y_dev_pred': y_test_pred.squeeze().tolist(),\n",
    "                        'y_dev_err':  (y_test_pred - y_test).squeeze().tolist()\n",
    "                       }\n",
    "    joblib.dump(test_predictions, path.join(OUT_PATH, short_name + '_test_preds.pkl.xz'), compress=('xz',9))\n",
    "\n",
    "save_res_test('lin_reg')\n",
    "log.debug('Linear regression results (test set) saved to file.')\n",
    "\n",
    "# visualise the errors\n",
    "plot = error_visualisation(y_test, y_test_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'lin_reg_test_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then show the same results for the elastic net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the predictions\n",
    "y_test_pred = el_net.predict(X_test).reshape(-1,1)\n",
    "\n",
    "# print the metrics\n",
    "print('Results for EN:')\n",
    "print_metrics(y_test, y_test_pred)\n",
    "\n",
    "# save the predictions\n",
    "save_res_test('el_net')\n",
    "\n",
    "# visualise the errors\n",
    "plot = error_visualisation(y_test, y_test_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'el_net_test_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the results in the presence of regularisation with the lasso regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the predictions\n",
    "y_test_pred = lasso.predict(X_test).reshape(-1,1)\n",
    "\n",
    "# print the metrics\n",
    "print('Results for LASSO:')\n",
    "print_metrics(y_test, y_test_pred)\n",
    "\n",
    "# save the predictions\n",
    "save_res_test('lasso')\n",
    "\n",
    "# visualise the errors\n",
    "plot = error_visualisation(y_test, y_test_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'lasso_test_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the move to the linear SVR (i.e. no kernel trick):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the predictions\n",
    "y_test_pred = lin_svr.predict(X_test).reshape(-1,1)\n",
    "\n",
    "# print the metrics\n",
    "print('Results for l-SVR:')\n",
    "print_metrics(y_test, y_test_pred)\n",
    "\n",
    "# save the predictions\n",
    "save_res_test('lin_svr')\n",
    "\n",
    "# visualise the errors\n",
    "plot = error_visualisation(y_test, y_test_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'lin_svr_test_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the improvements brought by the kernel trick in the SVR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the predictions\n",
    "y_test_pred = svr_rbf.predict(X_test).reshape(-1,1)\n",
    "\n",
    "# print the metrics\n",
    "print('Results for SVR:')\n",
    "print_metrics(y_test, y_test_pred)\n",
    "\n",
    "# save the predictions\n",
    "save_res_test('svr_rbf')\n",
    "\n",
    "# visualise the errors\n",
    "plot = error_visualisation(y_test, y_test_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'svr_rbf_test_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move to the implementation of the random forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the predictions\n",
    "y_test_pred = rnd_for.predict(dtest).reshape(-1,1)\n",
    "\n",
    "# print the metrics\n",
    "print('Results for RF:')\n",
    "print_metrics(y_test, y_test_pred)\n",
    "\n",
    "# save the predictions\n",
    "save_res_test('rnd_for')\n",
    "\n",
    "# visualise the errors\n",
    "plot = error_visualisation(y_test, y_test_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'rnd_for_test_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we show the boosted trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the predictions\n",
    "y_test_pred = bst_tree.predict(dtest).reshape(-1,1)\n",
    "\n",
    "# print the metrics\n",
    "print('Results for BDT:')\n",
    "print_metrics(y_test, y_test_pred)\n",
    "\n",
    "# save the predictions\n",
    "save_res_test('bst_tree')\n",
    "\n",
    "# visualise the errors\n",
    "plot = error_visualisation(y_test, y_test_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'bst_tree_test_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the predictions of the boosted forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the predictions\n",
    "y_test_pred = bst_for.predict(dtest).reshape(-1,1)\n",
    "\n",
    "# print the metrics\n",
    "print('Results for BRF:')\n",
    "print_metrics(y_test, y_test_pred)\n",
    "\n",
    "# save the predictions\n",
    "save_res_test('bst_for')\n",
    "\n",
    "# visualise the errors\n",
    "plot = error_visualisation(y_test, y_test_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'bst_for_test_errors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally show the predictions of the neural netowrk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the predictions\n",
    "y_test_pred = ann_fc.predict(X_test).reshape(-1,1)\n",
    "\n",
    "# print the metrics\n",
    "print('Results for the ANN:')\n",
    "print_metrics(y_test, y_test_pred)\n",
    "\n",
    "# save the predictions\n",
    "save_res_test('ann_fc')\n",
    "\n",
    "# visualise the errors\n",
    "plot = error_visualisation(y_test, y_test_pred)\n",
    "plot.save_and_close(path.join(IMG_PATH, 'ann_fc_test_errors'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
