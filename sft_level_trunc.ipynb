{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Level Truncation in String Field Theory\n",
    "\n",
    "Consider the data of lumps in bosonic String Field Theory (SFT) and extrapolate level-$\\infty$ predictions from finite level data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First of all we print the characteristics of the current setup (OS, cores, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current OS:                  Linux (kernel release: 5.6.7-arch1-1, architecture: x86_64)\n",
      "Number of available threads: 8\n",
      "Current CPU frequency:       2825 MHz (max: 3800 MHz)\n",
      "Available RAM memory:        5998 MB (tot: 15758 MB)\n"
     ]
    }
   ],
   "source": [
    "from mltools.libos import InfoOS\n",
    "\n",
    "print('Current OS:                  {} (kernel release: {}, architecture: {})'.format(InfoOS().os, InfoOS().kernel, InfoOS().arch))\n",
    "print('Number of available threads: {:d}'.format(InfoOS().threads))\n",
    "print('Current CPU frequency:       {:.0f} MHz (max: {:.0f} MHz)'.format(InfoOS().freq, InfoOS().freqm))\n",
    "print('Available RAM memory:        {:d} MB (tot: {:d} MB)'.format(InfoOS().vmav, InfoOS().vmtot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check the installed versions of the packages we are going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7\n",
      "Matplot version: 3.2.1\n",
      "Numpy version: 1.18.1\n",
      "Pandas version: 1.0.3\n",
      "Scikit-learn version: 0.22.2.post1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import random     as rnd\n",
    "import sklearn    as skl\n",
    "import numpy      as np\n",
    "import pandas     as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning) # ignore user warnings: nothing that I can really do anything about it...\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=12)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# print the version of the modules\n",
    "print('Python version: {:d}.{:d}'      .format(sys.version_info.major, sys.version_info.minor))\n",
    "print('Matplot version: {}'            .format(mpl.__version__))\n",
    "print('Numpy version: {}'              .format(np.__version__))\n",
    "print('Pandas version: {}'             .format(pd.__version__))\n",
    "print('Scikit-learn version: {}'       .format(skl.__version__))\n",
    "\n",
    "# fix random_seed\n",
    "RAND = 42\n",
    "rnd.seed(RAND)\n",
    "np.random.seed(RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Preparation\n",
    "\n",
    "In order to save the results of the analysis, we need to create the structure of directories in the current repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "\n",
    "ROOT_DIR = '.' #-------------------------------------------------- root directory\n",
    "IMG_DIR  = 'img' #------------------------------------------------ directory of images\n",
    "MOD_DIR  = 'models' #--------------------------------------------- directory of saved models\n",
    "LOG_DIR  = 'log' #------------------------------------------------ directory of logs\n",
    "\n",
    "DB_NAME = 'data_sft_dict' #--------------------------------------- name of the dataset\n",
    "DB_FILE = DB_NAME + '.json' #------------------------------------- full name with extension\n",
    "DB_PATH = path.join(ROOT_DIR, DB_FILE) #-------------------------- full path of the dataset\n",
    "\n",
    "# define full paths\n",
    "IMG_PATH = path.join(ROOT_DIR, IMG_DIR)\n",
    "MOD_PATH = path.join(ROOT_DIR, MOD_DIR)\n",
    "LOG_PATH = path.join(ROOT_DIR, LOG_DIR)\n",
    "\n",
    "# create directories if non existent\n",
    "if not path.isdir(IMG_PATH):\n",
    "    makedirs(IMG_PATH, exist_ok=True)\n",
    "if not path.isdir(MOD_PATH):\n",
    "    makedirs(MOD_PATH, exist_ok=True)\n",
    "if not path.isdir(LOG_PATH):\n",
    "    makedirs(LOG_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a logging session to store debug information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotating existing logs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 22:35:52,594: INFO ==> New logging session started. Log is at ./log/data_sft_dict.log.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from mltools.liblog import create_logfile\n",
    "\n",
    "path_to_log = path.join(LOG_PATH,\n",
    "                        DB_NAME + '.log'\n",
    "                       )\n",
    "log = create_logfile(path_to_log,\n",
    "                     name=DB_NAME,\n",
    "                     level=logging.DEBUG\n",
    "                    )\n",
    "\n",
    "# these lines provide the same setup also for the Jupyter logging\n",
    "logger = logging.getLogger() #------------------------------------------------- get the current logging session\n",
    "\n",
    "fmt = logging.Formatter('%(asctime)s: %(levelname)s ==> %(message)s') #-------- customise the formatting options\n",
    "\n",
    "handler = logging.StreamHandler() #-------------------------------------------- handle the stream to the default (stderr)\n",
    "handler.setLevel(logging.DEBUG) #---------------------------------------------- print everything\n",
    "handler.setFormatter(fmt) #---------------------------------------------------- set the formatting options\n",
    "\n",
    "logger.handlers = [handler] #-------------------------------------------------- override the default stream\n",
    "\n",
    "# we are ready to go!\n",
    "log.info('New logging session started. Log is at {}.'.format(path_to_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Database\n",
    "\n",
    "We then import the database from its JSON format and begin to analyse it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 22:35:52,614: DEBUG ==> Database loaded.\n"
     ]
    }
   ],
   "source": [
    "if path.isfile(DB_PATH):\n",
    "    df = pd.read_json(DB_PATH)\n",
    "    log.debug('Database loaded.')\n",
    "else:\n",
    "    print('Database is not in the file tree!')\n",
    "    log.error('Cannot find database!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then show the `dtypes` of each column to get an idea of the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init      object\n",
       "exp       object\n",
       "weight    object\n",
       "type      object\n",
       "2         object\n",
       "3         object\n",
       "4         object\n",
       "5         object\n",
       "6         object\n",
       "7         object\n",
       "8         object\n",
       "9         object\n",
       "10        object\n",
       "11        object\n",
       "12        object\n",
       "13        object\n",
       "14        object\n",
       "15        object\n",
       "16        object\n",
       "17        object\n",
       "18        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset we have the predictions of the position of lumps in bosonic SFT for finite levels in the numbered columns and the extrapolation for level-$\\infty$ in the column _exp_. We want to use known data (including the _weight_ and the _type_ of the input data) to predict the _exp_ labels (_init_ in principle can be left out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>init</th>\n",
       "      <th>exp</th>\n",
       "      <th>weight</th>\n",
       "      <th>type</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, -1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 4, 9, 0, 0.25, 1, 2.25, 4, 0, 0.25, ...</td>\n",
       "      <td>[2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.0001, 0, 1.0001, 1.0001, 1.0001, 1.0001, 0,...</td>\n",
       "      <td>[1, 0, -1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 4, 9, 0, 0.249950007499, 0.999800029...</td>\n",
       "      <td>[2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "      <td>[1.000099754465899, -4.382819109385611e-08, 0....</td>\n",
       "      <td>[1.000099754261711, -6.385189815988693e-08, 0....</td>\n",
       "      <td>[1.000099495309939, -1.9972775228453091e-07, 0...</td>\n",
       "      <td>[1.000099494726808, -1.724421881015622e-07, 0....</td>\n",
       "      <td>[1.000099223845491, -3.2173432889712715e-07, 0...</td>\n",
       "      <td>[1.000099222907449, -2.856173963606407e-07, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.000098951488667, -3.9768513795577186e-07, 0...</td>\n",
       "      <td>[1.000098684133785, -5.470716468222031e-07, 0....</td>\n",
       "      <td>[1.000098682609473, -5.081256574169557e-07, 0....</td>\n",
       "      <td>[1.000098418312006, -6.559751804689415e-07, 0....</td>\n",
       "      <td>[1.000098416531483, -6.169537248661669e-07, 0....</td>\n",
       "      <td>[1.000098155198292, -7.630807670831046e-07, 0....</td>\n",
       "      <td>[1.000098153176776, -7.242020485026188e-07, 0....</td>\n",
       "      <td>[1.000097894670832, -8.685182838696036e-07, 0....</td>\n",
       "      <td>[1.000097892420157, -8.29888184051414e-07, 0.9...</td>\n",
       "      <td>[1.000097636616163, -9.72346758033437e-07, 0.9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                init  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.0001, 0, 1.0001, 1.0001, 1.0001, 1.0001, 0,...   \n",
       "\n",
       "                                               exp  \\\n",
       "0  [1, 0, -1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]   \n",
       "1  [1, 0, -1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]   \n",
       "\n",
       "                                              weight  \\\n",
       "0  [0, 0, 1, 4, 9, 0, 0.25, 1, 2.25, 4, 0, 0.25, ...   \n",
       "1  [0, 0, 1, 4, 9, 0, 0.249950007499, 0.999800029...   \n",
       "\n",
       "                                            type  \\\n",
       "0  [2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]   \n",
       "1  [2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]   \n",
       "\n",
       "                                                   2  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000099754465899, -4.382819109385611e-08, 0....   \n",
       "\n",
       "                                                   3  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000099754261711, -6.385189815988693e-08, 0....   \n",
       "\n",
       "                                                   4  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000099495309939, -1.9972775228453091e-07, 0...   \n",
       "\n",
       "                                                   5  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000099494726808, -1.724421881015622e-07, 0....   \n",
       "\n",
       "                                                   6  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000099223845491, -3.2173432889712715e-07, 0...   \n",
       "\n",
       "                                                   7  ...  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]  ...   \n",
       "1  [1.000099222907449, -2.856173963606407e-07, 0....  ...   \n",
       "\n",
       "                                                   9  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098951488667, -3.9768513795577186e-07, 0...   \n",
       "\n",
       "                                                  10  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098684133785, -5.470716468222031e-07, 0....   \n",
       "\n",
       "                                                  11  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098682609473, -5.081256574169557e-07, 0....   \n",
       "\n",
       "                                                  12  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098418312006, -6.559751804689415e-07, 0....   \n",
       "\n",
       "                                                  13  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098416531483, -6.169537248661669e-07, 0....   \n",
       "\n",
       "                                                  14  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098155198292, -7.630807670831046e-07, 0....   \n",
       "\n",
       "                                                  15  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098153176776, -7.242020485026188e-07, 0....   \n",
       "\n",
       "                                                  16  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000097894670832, -8.685182838696036e-07, 0....   \n",
       "\n",
       "                                                  17  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000097892420157, -8.29888184051414e-07, 0.9...   \n",
       "\n",
       "                                                  18  \n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]  \n",
       "1  [1.000097636616163, -9.72346758033437e-07, 0.9...  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "We then extract the features to restore them from the sparse format in which they are currently stored in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltools.libtransformer import ExtractTensor\n",
    "\n",
    "for feature in df:\n",
    "    df[feature] = ExtractTensor(flatten=True).fit_transform(df[feature])\n",
    "\n",
    "# compute the length of each feature array\n",
    "length = np.max(df['exp'].apply(np.shape))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then reorder the dataset to have the label in the last position and define the training features we use. We build two different datasets to be compared with and without the initial values (column _init_ of the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features      = ['type', 'weight', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18']\n",
    "features_full = ['init'] + features\n",
    "columns       = features + ['exp']\n",
    "columns_full  = features_full + ['exp']\n",
    "\n",
    "# reorder the columns (avoid the first entry since the init values are too symmetryic across the levels)\n",
    "df_red  = df[columns].iloc[1:] #---------- 'reduced list' w/o the initial value\n",
    "df_full = df[columns_full].iloc[1:] #----- 'full list' w/ the initial value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every value in any column we want to extrapolate its level-$\\infty$ predictions. We therefore need a way to extract each value from each column and treat the set of extracted numbers as different datasets to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>weight</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000100</td>\n",
       "      <td>1.000100</td>\n",
       "      <td>1.000099</td>\n",
       "      <td>1.000099</td>\n",
       "      <td>1.000099</td>\n",
       "      <td>1.000099</td>\n",
       "      <td>1.000099</td>\n",
       "      <td>1.000099</td>\n",
       "      <td>1.000099</td>\n",
       "      <td>1.000099</td>\n",
       "      <td>1.000098</td>\n",
       "      <td>1.000098</td>\n",
       "      <td>1.000098</td>\n",
       "      <td>1.000098</td>\n",
       "      <td>1.000098</td>\n",
       "      <td>1.000098</td>\n",
       "      <td>1.000098</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000976</td>\n",
       "      <td>1.000976</td>\n",
       "      <td>1.000953</td>\n",
       "      <td>1.000952</td>\n",
       "      <td>1.000930</td>\n",
       "      <td>1.000929</td>\n",
       "      <td>1.000908</td>\n",
       "      <td>1.000908</td>\n",
       "      <td>1.000888</td>\n",
       "      <td>1.000887</td>\n",
       "      <td>1.000869</td>\n",
       "      <td>1.000868</td>\n",
       "      <td>1.000852</td>\n",
       "      <td>1.000851</td>\n",
       "      <td>1.000835</td>\n",
       "      <td>1.000834</td>\n",
       "      <td>1.000819</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  weight         2         3         4         5         6         7  \\\n",
       "0     2     0.0  1.000100  1.000100  1.000099  1.000099  1.000099  1.000099   \n",
       "1     2     0.0  1.000976  1.000976  1.000953  1.000952  1.000930  1.000929   \n",
       "\n",
       "          8         9        10        11        12        13        14  \\\n",
       "0  1.000099  1.000099  1.000099  1.000099  1.000098  1.000098  1.000098   \n",
       "1  1.000908  1.000908  1.000888  1.000887  1.000869  1.000868  1.000852   \n",
       "\n",
       "         15        16        17        18  exp  \n",
       "0  1.000098  1.000098  1.000098  1.000098    1  \n",
       "1  1.000851  1.000835  1.000834  1.000819    1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract each correspondent values and \"expand\" them vertically\n",
    "df_red_stack  = pd.concat([pd.DataFrame({f: df_red[f].str[n].values for f in df_red}) for n in range(length)], axis=0).reset_index(drop=True)\n",
    "df_full_stack = pd.concat([pd.DataFrame({f: df_full[f].str[n].values for f in df_full}) for n in range(length)], axis=0).reset_index(drop=True)\n",
    "\n",
    "# show a couple of samples for reference\n",
    "df_red_stack.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set Prearation and Validation Strategy\n",
    "\n",
    "For the training we use a **cross-validation strategy** even though the no. of samples is very restricted. Using a single holdout validation set would however be prone to overfit the validation set and not generalise well. We first conduct a preliminary study to determine a good split for training and validation set, keeping however a separated 20% of the dataset as **test set** (this should remain fixed as every evaluation should be performed on the validation set instead to avoid overfitting the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_red_train, df_red_test, df_full_train, df_full_test  = train_test_split(df_red_stack, df_full_stack,\n",
    "                                                                           test_size=0.2, shuffle=True, random_state=RAND\n",
    "                                                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then consider the _reduced_ dataset to experiment on the best validation split size for the analysis. We split the training set into an effective training set and a **holdout validation** set with several different sizes and use the **mean squared error** (MSE) as a metric to evaluate the algorithm against the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 22:36:00,183: DEBUG ==> Plot saved in ./img/mse_train_size.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics      import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble     import RandomForestRegressor\n",
    "from mltools.libplot      import Plot\n",
    "\n",
    "# choose an estimator\n",
    "estimators  = [LinearRegression(),\n",
    "               RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=RAND)\n",
    "              ]\n",
    "\n",
    "# define the size of the train set we want to experiment\n",
    "train_sizes = np.linspace(20, 90,\n",
    "                          num=15,\n",
    "                          dtype=np.int\n",
    "                         ).reshape(1,-1) / 100.0 #------------------------------------------ training sizes\n",
    "mse_values  = np.zeros((np.shape(estimators)[0], train_sizes.shape[1])) #------------------- mse values (shape: n_estimators x train_sizes)\n",
    "\n",
    "# iterate over the training set sizes\n",
    "for n in range(train_sizes.shape[1]):\n",
    "    # split the set and select the features\n",
    "    train, val = train_test_split(df_red_train,\n",
    "                                  train_size=train_sizes[:,n].reshape(-1,),\n",
    "                                  shuffle=False\n",
    "                                 ) #-------------------------------------------------------- split the training set\n",
    "    train_features = train.drop(columns='exp') #-------------------------------------------- training features\n",
    "    val_features   = val.drop(columns='exp') #---------------------------------------------- validation features\n",
    "\n",
    "    train_label    = train['exp'] #--------------------------------------------------------- training label\n",
    "    val_label      = val['exp'] #----------------------------------------------------------- validation label\n",
    "\n",
    "    # evaluate the algorithm\n",
    "    for m in range(np.shape(estimators)[0]):\n",
    "        estimators[m].fit(train_features, train_label) #------------------------------------ fit the estimator_1\n",
    "        mse_values[m,n] = mean_squared_error(y_true=val_label,\n",
    "                                             y_pred=estimators[m].predict(val_features)\n",
    "                                            ) #--------------------------------------------- store the values of the MSE\n",
    "\n",
    "# plot the MSE values\n",
    "plot = Plot()\n",
    "\n",
    "for m in range(np.shape(estimators)[0]):\n",
    "    plot.series2D(data=mse_values[m,:],\n",
    "                    title='MSE as Function of the Training Set Size',\n",
    "                    xlabel='Train Set Size',\n",
    "                    ylabel='MSE',\n",
    "                    legend=estimators[m].__class__.__name__,\n",
    "                    ylog=True,\n",
    "                    labels=train_sizes.reshape(-1,)\n",
    "                   )\n",
    "\n",
    "plot.save_and_close(path.join(IMG_PATH, 'mse_train_size'))\n",
    "log.debug('Plot saved in {}.'.format(path.join(IMG_PATH, 'mse_train_size')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot we show the progression of the MSE as a function of the training size. A good compromise between the no. of samples in the training set and the MSE of different kinds of algorithms seems to be around 80% to 85% of the dataset.\n",
    "\n",
    "We will therefore divide the original training set as follows:\n",
    "\n",
    "- 20% into a **test set** (already split),\n",
    "- 33% into **cross-validation** splits (around 20% of the training dataset),\n",
    "- 67% into the effective **training set** (around 80% of the training dataset).\n",
    "\n",
    "When we implement cross-validation in _Scikit_ we will therefore use 5 splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Analysis\n",
    "\n",
    "We now move to the ML analysis of the dataset. Given the size of the dataframe, we will try different approaches to the extrapolation of the level-$\\infty$ predictions. Specifically we will look at:\n",
    "\n",
    "- [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to have a solid baseline for comparison,\n",
    "- [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) to implement **L1** and/or **L2** regularization,\n",
    "- [Linear SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html) to use **support vector machines** to improve the predictions,\n",
    "- [SVR (Gaussian kernel)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) to hopefully find better results,\n",
    "- [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) to use the power of **decision trees** and **ensemble** learning,\n",
    "- [Histogram Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html) to improve the predictive abilities of a single decision tree through successive improvement.\n",
    "\n",
    "For the hyperparameter optimization we use a [**Bayesan** approach](https://en.wikipedia.org/wiki/Bayesian_optimization) in the [_Scikit-optimize_](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html) library: this will provide a better approach to the minimization of the cost functions during cross-validation with respect to a randomized search (apart from the linear regression where we will test all possible values of the hyperparameters). In order to print the output of the parameters dictionaries we implement a function to pretty print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(dct, indent=True):\n",
    "    '''\n",
    "    Pretty print the dictionary of best parameters.\n",
    "    \n",
    "    Required argument:\n",
    "        dct:    the dictionary to pretty print.\n",
    "        \n",
    "    Optional argument:\n",
    "        indent: whether to indent the printed output.\n",
    "    '''\n",
    "    \n",
    "    for key, value in dct.items():\n",
    "        if indent:\n",
    "            print('    {} = {}'.format(key, value))\n",
    "        else:\n",
    "            print('{} = {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Labels Extraction\n",
    "\n",
    "We then extract the training features and the labels. If needed we can implement the scaling of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "# features\n",
    "features_red  = df_red_train.drop(columns='exp')\n",
    "features_full = df_full_train.drop(columns='exp')\n",
    "\n",
    "# scale the features\n",
    "red_scaler  = StandardScaler()\n",
    "full_scaler = StandardScaler()\n",
    "\n",
    "if red_scaler is not None:\n",
    "    features_red  = red_scaler.fit_transform(features_red)\n",
    "if full_scaler is not None:\n",
    "    features_full = full_scaler.fit_transform(features_full)\n",
    "\n",
    "# labels\n",
    "label_red  = df_red_train['exp']\n",
    "label_full = df_full_train['exp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will often refer to _reduced_ and _full_ datasets: the first does not contain the initialization values `init`, while the latter does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "We first consider a linear regression algorithm. In this case the number of hyperparameters to be tuned is small and we can use a **grid search** to try out every combination:\n",
    "\n",
    "- `fit_intercept` $\\in \\lbrace 0, 1 \\rbrace$,\n",
    "- `normalize` $\\in \\lbrace 0, 1 \\rbrace$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 22:36:00,237: INFO ==> Fitting the LinearRegression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters with the reduced dataset for LinearRegression:\n",
      "\n",
      "    fit_intercept = 1\n",
      "    normalize = 1\n",
      "\n",
      "RMSE of LinearRegression on the validation set with the reduced dataset: 0.415 ± 0.038\n",
      "\n",
      "Best parameters with the full dataset for LinearRegression:\n",
      "\n",
      "    fit_intercept = 1\n",
      "    normalize = 0\n",
      "\n",
      "RMSE of LinearRegression on the validation set with the full dataset: 0.409 ± 0.037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "from mltools.libscore        import ViewCV\n",
    "from mltools.libplot         import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params  = {'fit_intercept': [0, 1],\n",
    "                  'normalize':     [0, 1]\n",
    "                 } #----------------------------------------------------------------------------------- define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "estimator      = LinearRegression()\n",
    "cv_hypersearch = GridSearchCV(estimator=estimator,\n",
    "                              param_grid=search_params,\n",
    "                              scoring='neg_mean_squared_error',\n",
    "                              n_jobs=-1,\n",
    "                              refit=True,\n",
    "                              cv=cv\n",
    "                             ) #----------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the {}...'.format(estimator.__class__.__name__))\n",
    "\n",
    "est_red_fit  = cv_hypersearch.fit(features_red, label_red) #------------------------------------------- fit the estimator (reduced dataset)\n",
    "joblib.dump(est_red_fit.best_estimator_, path.join(MOD_PATH, '{}_red.joblib'.format(estimator.__class__.__name__)))\n",
    "\n",
    "results_red  = ViewCV(est_red_fit) #------------------------------------------------------------------- get the scoring results (reduced dataset)\n",
    "print('\\nBest parameters with the reduced dataset for {}:\\n'.format(estimator.__class__.__name__))\n",
    "pretty(results_red.best_parameters)\n",
    "\n",
    "print('\\nRMSE of {} on the validation set with the reduced dataset: {:.3f} ± {:.3f}'.format(estimator.__class__.__name__,\n",
    "                                                                                            np.sqrt(-results_red.test_mean()),\n",
    "                                                                                            results_red.test_std()\n",
    "                                                                                           )\n",
    "     )\n",
    "\n",
    "est_full_fit = cv_hypersearch.fit(features_full, label_full) #----------------------------------------- fit the estimator (full dataset)\n",
    "joblib.dump(est_full_fit.best_estimator_, path.join(MOD_PATH, '{}_full.joblib'.format(estimator.__class__.__name__)))\n",
    "\n",
    "results_full = ViewCV(est_full_fit) #------------------------------------------------------------------ get the scoring results (full dataset)\n",
    "print('\\nBest parameters with the full dataset for {}:\\n'.format(estimator.__class__.__name__))\n",
    "pretty(results_full.best_parameters)\n",
    "\n",
    "print('\\nRMSE of {} on the validation set with the full dataset: {:.3f} ± {:.3f}'.format(estimator.__class__.__name__,\n",
    "                                                                                         np.sqrt(-results_full.test_mean()),\n",
    "                                                                                         results_full.test_std()\n",
    "                                                                                        )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n",
    "\n",
    "We then try to implement the regularized version of the simple linear regression: we use an **elastic net** because it implements both **L1** and **L2** regularizations and can be a guide in choosing the appropriate values. In fact the hyperparameters in this case allow to separately control the two different regularizations. The additional term (with respect to the linear regression) to the cost function is of the form $\\Delta J(w) = \\alpha l_{1\\,ratio} \\vert\\vert w \\vert\\vert + \\frac{\\alpha}{2} (1 - l_{1\\,ratio}) \\vert\\vert w \\vert\\vert^2$. We will then study the following hyperparamter space:\n",
    "\n",
    "- `fit_intercept` $\\in \\lbrace 0, 1 \\rbrace$,\n",
    "- `normalize` $\\in \\lbrace 0, 1 \\rbrace$,\n",
    "- `alpha` $\\in \\left[ 0.0, 2.0 \\right]$,\n",
    "- `l1_ratio` $\\in \\left[ 0.0, 1.0 \\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 22:36:02,490: INFO ==> Fitting the ElasticNet...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters with the reduced dataset for ElasticNet:\n",
      "\n",
      "    alpha = 0.0\n",
      "    fit_intercept = 1\n",
      "    l1_ratio = 0.41155444531141877\n",
      "    normalize = 0\n",
      "\n",
      "RMSE of ElasticNet on the validation set with the reduced dataset: 0.464 ± 0.030\n",
      "\n",
      "Best parameters with the full dataset for ElasticNet:\n",
      "\n",
      "    alpha = 0.0\n",
      "    fit_intercept = 1\n",
      "    l1_ratio = 0.0\n",
      "    normalize = 1\n",
      "\n",
      "RMSE of ElasticNet on the validation set with the full dataset: 0.458 ± 0.028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from skopt                import BayesSearchCV\n",
    "from skopt.space          import Real, Integer, Categorical\n",
    "from sklearn.metrics      import mean_squared_error\n",
    "from mltools.libscore     import ViewCV\n",
    "from mltools.libplot      import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params  = {'fit_intercept': Integer(0, 1),\n",
    "                  'normalize':     Integer(0, 1),\n",
    "                  'alpha':         Real(0.0, 2.0, prior='uniform'),\n",
    "                  'l1_ratio':      Real(0.0, 1.0, prior='uniform')\n",
    "                 } #----------------------------------------------------------------------------------- define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "estimator      = ElasticNet(max_iter=1e5, random_state=RAND)\n",
    "cv_hypersearch = BayesSearchCV(estimator=estimator,\n",
    "                               search_spaces=search_params,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               n_jobs=-1,\n",
    "                               refit=True,\n",
    "                               n_iter=100,\n",
    "                               random_state=RAND,\n",
    "                               cv=cv\n",
    "                              ) #---------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the {}...'.format(estimator.__class__.__name__))\n",
    "\n",
    "est_red_fit  = cv_hypersearch.fit(features_red, label_red) #------------------------------------------- fit the estimator (reduced dataset)\n",
    "joblib.dump(est_red_fit.best_estimator_, path.join(MOD_PATH, '{}_red.joblib'.format(estimator.__class__.__name__)))\n",
    "\n",
    "results_red  = ViewCV(est_red_fit) #------------------------------------------------------------------- get the scoring results (reduced dataset)\n",
    "print('\\nBest parameters with the reduced dataset for {}:\\n'.format(estimator.__class__.__name__))\n",
    "pretty(results_red.best_parameters)\n",
    "\n",
    "print('\\nRMSE of {} on the validation set with the reduced dataset: {:.3f} ± {:.3f}'.format(estimator.__class__.__name__,\n",
    "                                                                                            np.sqrt(-results_red.test_mean()),\n",
    "                                                                                            results_red.test_std()\n",
    "                                                                                           )\n",
    "     )\n",
    "\n",
    "est_full_fit = cv_hypersearch.fit(features_full, label_full) #----------------------------------------- fit the estimator (full dataset)\n",
    "joblib.dump(est_full_fit.best_estimator_, path.join(MOD_PATH, '{}_full.joblib'.format(estimator.__class__.__name__)))\n",
    "\n",
    "results_full = ViewCV(est_full_fit) #------------------------------------------------------------------ get the scoring results (full dataset)\n",
    "print('\\nBest parameters with the full dataset for {}:\\n'.format(estimator.__class__.__name__))\n",
    "pretty(results_full.best_parameters)\n",
    "      \n",
    "print('\\nRMSE of {} on the validation set with the full dataset: {:.3f} ± {:.3f}'.format(estimator.__class__.__name__,\n",
    "                                                                                       np.sqrt(-results_full.test_mean()),\n",
    "                                                                                       results_full.test_std()\n",
    "                                                                                      )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that after the optimization iterations the best choice for $\\alpha$ is identically vanishing: the algorithm works best without regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Linear SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html)\n",
    "\n",
    "We move to the **support vector machines** (SVM) algorithms and start without a **kernel** function. We use the linear implementation of the support vector regressor with hyperparameters:\n",
    "\n",
    "- `fit_intercept` $\\in \\lbrace 0, 1 \\rbrace$,\n",
    "- `epsilon` $\\in \\left[ 0.0, 1.0 \\right]$,\n",
    "- `C` $\\in \\left[ 10^{-1}, 10^3 \\right]$,\n",
    "- `loss` $\\in \\lbrace epsilon\\_insensitive, squared\\_epsilon\\_insensitive \\rbrace$,\n",
    "- `intercept_scaling` $\\in \\left[ 10^{-2}, 10^2 \\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 22:54:33,590: INFO ==> Fitting the LinearSVR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters with the reduced dataset for LinearSVR:\n",
      "\n",
      "    C = 1000.0\n",
      "    epsilon = 0.1336555761255731\n",
      "    fit_intercept = 1\n",
      "    intercept_scaling = 5.732226482460041\n",
      "    loss = squared_epsilon_insensitive\n",
      "\n",
      "RMSE of LinearSVR on the validation set with the reduced dataset: 0.480 ± 0.016\n",
      "\n",
      "Best parameters with the full dataset for LinearSVR:\n",
      "\n",
      "    C = 1000.0\n",
      "    epsilon = 0.0\n",
      "    fit_intercept = 1\n",
      "    intercept_scaling = 0.01\n",
      "    loss = squared_epsilon_insensitive\n",
      "\n",
      "RMSE of LinearSVR on the validation set with the full dataset: 0.452 ± 0.021\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm      import LinearSVR\n",
    "from skopt            import BayesSearchCV\n",
    "from skopt.space      import Real, Integer, Categorical\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from mltools.libscore import ViewCV\n",
    "from mltools.libplot  import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params  = {'fit_intercept':     Integer(0, 1),\n",
    "                  'epsilon':           Real(0.0, 1.0,  prior='uniform'),\n",
    "                  'C':                 Real(1.0e-1, 1.0e3, prior='log-uniform'),\n",
    "                  'intercept_scaling': Real(1.0e-2, 1.0e2, prior='log-uniform'),\n",
    "                  'loss':              Categorical(['squared_epsilon_insensitive', 'epsilon_insensitive'])\n",
    "                 } #----------------------------------------------------------------------------------- define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "estimator      = LinearSVR(max_iter=1e5, random_state=RAND)\n",
    "cv_hypersearch = BayesSearchCV(estimator=estimator,\n",
    "                               search_spaces=search_params,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               n_jobs=-1,\n",
    "                               refit=True,\n",
    "                               n_iter=100,\n",
    "                               random_state=RAND,\n",
    "                               cv=cv\n",
    "                              ) #---------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the {}...'.format(estimator.__class__.__name__))\n",
    "\n",
    "est_red_fit  = cv_hypersearch.fit(features_red, label_red) #------------------------------------------- fit the estimator (reduced dataset)\n",
    "joblib.dump(est_red_fit.best_estimator_, path.join(MOD_PATH, '{}_red.joblib'.format(estimator.__class__.__name__)))\n",
    "\n",
    "results_red  = ViewCV(est_red_fit) #------------------------------------------------------------------- get the scoring results (reduced dataset)\n",
    "print('\\nBest parameters with the reduced dataset for {}:\\n'.format(estimator.__class__.__name__))\n",
    "pretty(results_red.best_parameters)\n",
    "\n",
    "print('\\nRMSE of {} on the validation set with the reduced dataset: {:.3f} ± {:.3f}'.format(estimator.__class__.__name__,\n",
    "                                                                                            np.sqrt(-results_red.test_mean()),\n",
    "                                                                                            results_red.test_std()\n",
    "                                                                                           )\n",
    "     )\n",
    "\n",
    "est_full_fit = cv_hypersearch.fit(features_full, label_full) #----------------------------------------- fit the estimator (full dataset)\n",
    "joblib.dump(est_full_fit.best_estimator_, path.join(MOD_PATH, '{}_full.joblib'.format(estimator.__class__.__name__)))\n",
    "\n",
    "results_full = ViewCV(est_full_fit) #------------------------------------------------------------------ get the scoring results (full dataset)\n",
    "print('\\nBest parameters with the full dataset for {}:\\n'.format(estimator.__class__.__name__))\n",
    "pretty(results_full.best_parameters)\n",
    "\n",
    "print('\\nRMSE of {} on the validation set with the full dataset: {:.3f} ± {:.3f}'.format(estimator.__class__.__name__,\n",
    "                                                                                         np.sqrt(-results_full.test_mean()),\n",
    "                                                                                         results_full.test_std()\n",
    "                                                                                        )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) (Gaussian Kernel)\n",
    "\n",
    "We now implement a kernel function in the SVM. In particular we use the Gaussian kernel (_rbf_) and try to approximate the level-$\\infty$ predictions using these hyperparameters:\n",
    "\n",
    "- `gamma` $\\in \\left[ 10^{-1}, 10^1 \\right]$,\n",
    "- `C` $\\in \\left[ 10^{-1}, 5 \\times 10^2 \\right]$,\n",
    "- `epsilon` $\\in \\left[ 10^{-4}, 10^{-1} \\right]$,\n",
    "- `shrinking` $\\in \\lbrace 0, 1 \\rbrace$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 23:26:42,247: INFO ==> Fitting the SVR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters with the reduced dataset for SVR:\n",
      "\n",
      "    C = 202.26798030840013\n",
      "    epsilon = 0.0001\n",
      "    gamma = 1.5501598035989352\n",
      "    shrinking = 0\n",
      "\n",
      "RMSE of SVR on the validation set with the reduced dataset: 0.161 ± 0.011\n",
      "\n",
      "Best parameters with the full dataset for SVR:\n",
      "\n",
      "    C = 103.47067100437063\n",
      "    epsilon = 0.0001\n",
      "    gamma = 0.325347825685753\n",
      "    shrinking = 1\n",
      "\n",
      "RMSE of SVR on the validation set with the full dataset: 0.118 ± 0.005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm      import SVR\n",
    "from skopt            import BayesSearchCV\n",
    "from skopt.space      import Real, Integer, Categorical\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from mltools.libscore import ViewCV\n",
    "from mltools.libplot  import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params  = {'epsilon':   Real(1.0e-4, 1.0e-1, prior='log-uniform'),\n",
    "                  'C':         Real(1.0e-1, 5.0e2,  prior='log-uniform'),\n",
    "                  'gamma':     Real(1.0e-1, 1.0e1,  prior='log-uniform'),\n",
    "                  'shrinking': Integer(0, 1)\n",
    "                 } #----------------------------------------------------------------------------------- define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "estimator      = SVR(kernel='rbf')\n",
    "cv_hypersearch = BayesSearchCV(estimator=estimator,\n",
    "                               search_spaces=search_params,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               n_jobs=-1,\n",
    "                               refit=True,\n",
    "                               n_iter=100,\n",
    "                               random_state=RAND,\n",
    "                               cv=cv\n",
    "                              ) #---------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the {}...'.format(estimator.__class__.__name__))\n",
    "\n",
    "est_red_fit  = cv_hypersearch.fit(features_red, label_red) #------------------------------------------- fit the estimator (reduced dataset)\n",
    "joblib.dump(est_red_fit.best_estimator_, path.join(MOD_PATH, '{}_red.joblib'.format(estimator.__class__.__name__)))\n",
    "\n",
    "results_red  = ViewCV(est_red_fit) #------------------------------------------------------------------- get the scoring results (reduced dataset)\n",
    "print('\\nBest parameters with the reduced dataset for {}:\\n'.format(estimator.__class__.__name__))\n",
    "pretty(results_red.best_parameters)\n",
    "\n",
    "print('\\nRMSE of {} on the validation set with the reduced dataset: {:.3f} ± {:.3f}'.format(estimator.__class__.__name__,\n",
    "                                                                                            np.sqrt(-results_red.test_mean()),\n",
    "                                                                                            results_red.test_std()\n",
    "                                                                                           )\n",
    "     )\n",
    "\n",
    "est_full_fit = cv_hypersearch.fit(features_full, label_full) #----------------------------------------- fit the estimator (full dataset)\n",
    "joblib.dump(est_full_fit.best_estimator_, path.join(MOD_PATH, '{}_full.joblib'.format(estimator.__class__.__name__)))\n",
    "\n",
    "results_full = ViewCV(est_full_fit) #------------------------------------------------------------------ get the scoring results (full dataset)\n",
    "print('\\nBest parameters with the full dataset for {}:\\n'.format(estimator.__class__.__name__))\n",
    "pretty(results_full.best_parameters)\n",
    "\n",
    "print('\\nRMSE of {} on the validation set with the full dataset: {:.3f} ± {:.3f}'.format(estimator.__class__.__name__,\n",
    "                                                                                         np.sqrt(-results_full.test_mean()),\n",
    "                                                                                         results_full.test_std()\n",
    "                                                                                        )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "\n",
    "Moving on to the **decision tree** family of algorithms, we implement the **random forest** regression in the _Scikit_ implementation. We will try to intervene on as many hyperparameters as possible both to avoid overfitting the training set and to be able to generalise to new results. We use the following hyperparameters:\n",
    "\n",
    "- `n_estimators` $\\in \\left[ 10, 10^3 \\right]$,\n",
    "- `criterion` $\\in \\lbrace mse, mae \\rbrace$,\n",
    "- `max_depth` $\\in \\left[ 2, 100 \\right]$,\n",
    "- `min_samples_split` $\\in \\left[ 2, 100 \\right]$,\n",
    "- `min_samples_leaf` $\\in \\left[ 1, 100 \\right]$,\n",
    "- `min_weight_fraction_leaf` $\\in \\left[ 0, 0.5\\right]$,\n",
    "- `max_leaf_nodes` $\\in \\left[ 2, 100 \\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 23:40:58,376: INFO ==> Fitting the RandomForestRegressor...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters with the reduced dataset for RandomForestRegressor:\n",
      "\n",
      "    criterion = mse\n",
      "    max_depth = 34\n",
      "    max_leaf_nodes = 30\n",
      "    min_samples_leaf = 1\n",
      "    min_samples_split = 96\n",
      "    min_weight_fraction_leaf = 0.0\n",
      "    n_estimators = 1000\n",
      "\n",
      "RMSE of RandomForestRegressor on the validation set with the reduced dataset: 0.255 ± 0.008\n",
      "\n",
      "Best parameters with the full dataset for RandomForestRegressor:\n",
      "\n",
      "    criterion = mse\n",
      "    max_depth = 100\n",
      "    max_leaf_nodes = 89\n",
      "    min_samples_leaf = 1\n",
      "    min_samples_split = 20\n",
      "    min_weight_fraction_leaf = 0.0\n",
      "    n_estimators = 1000\n",
      "\n",
      "RMSE of RandomForestRegressor on the validation set with the full dataset: 0.150 ± 0.008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from skopt            import BayesSearchCV\n",
    "from skopt.space      import Real, Integer, Categorical\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from mltools.libscore import ViewCV\n",
    "from mltools.libplot  import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params = {'n_estimators':             Integer(1.0e1, 1.0e3, prior='log-uniform'),\n",
    "                 'max_depth':                Integer(2, 100, prior='uniform'),\n",
    "                 'min_samples_split':        Integer(2, 100, prior='uniform'),\n",
    "                 'min_samples_leaf':         Integer(1, 100, prior='uniform'),\n",
    "                 'max_leaf_nodes':           Integer(2, 100, prior='uniform'),\n",
    "                 'min_weight_fraction_leaf': Real(0.0, 0.5, prior='uniform'),\n",
    "                 'criterion':                Categorical(['mse', 'mae'])\n",
    "                } #------------------------------------------------------------------------------------ define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "estimator      = RandomForestRegressor(n_jobs=-1, random_state=RAND)\n",
    "cv_hypersearch = BayesSearchCV(estimator=estimator,\n",
    "                               search_spaces=search_params,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               n_jobs=-1,\n",
    "                               refit=True,\n",
    "                               n_iter=20,\n",
    "                               random_state=RAND,\n",
    "                               cv=cv\n",
    "                              ) #---------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the {}...'.format(estimator.__class__.__name__))\n",
    "\n",
    "est_red_fit  = cv_hypersearch.fit(features_red, label_red) #------------------------------------------- fit the estimator (reduced dataset)\n",
    "joblib.dump(est_red_fit.best_estimator_, path.join(MOD_PATH, '{}_red.joblib'.format(estimator.__class__.__name__)))\n",
    "\n",
    "results_red  = ViewCV(est_red_fit) #------------------------------------------------------------------- get the scoring results (reduced dataset)\n",
    "print('\\nBest parameters with the reduced dataset for {}:\\n'.format(estimator.__class__.__name__))\n",
    "pretty(results_red.best_parameters)\n",
    "\n",
    "print('\\nRMSE of {} on the validation set with the reduced dataset: {:.3f} ± {:.3f}'.format(estimator.__class__.__name__,\n",
    "                                                                                            np.sqrt(-results_red.test_mean()),\n",
    "                                                                                            results_red.test_std()\n",
    "                                                                                           )\n",
    "     )\n",
    "\n",
    "est_full_fit = cv_hypersearch.fit(features_full, label_full) #----------------------------------------- fit the estimator (full dataset)\n",
    "joblib.dump(est_full_fit.best_estimator_, path.join(MOD_PATH, '{}_full.joblib'.format(estimator.__class__.__name__)))\n",
    "\n",
    "results_full = ViewCV(est_full_fit) #------------------------------------------------------------------ get the scoring results (full dataset)\n",
    "print('\\nBest parameters with the full dataset for {}:\\n'.format(estimator.__class__.__name__))\n",
    "pretty(results_full.best_parameters)\n",
    "\n",
    "print('\\nRMSE of {} on the validation set with the full dataset: {:.3f} ± {:.3f}'.format(estimator.__class__.__name__,\n",
    "                                                                                         np.sqrt(-results_full.test_mean()),\n",
    "                                                                                         results_full.test_std()\n",
    "                                                                                        )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Histogram Based Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)\n",
    "\n",
    "We consider the new (still _experimental_) version of _Scikit_ **gradient boosting** algorithm, which is **histogram bases**. This should automatically handle _NaN_ gradients and be quite faster than the usual implementation. We will consider the followin hyperparameters:\n",
    "\n",
    "- `loss` $\\in \\lbrace least\\_squares, least\\_absolute\\_deviation \\rbrace$,\n",
    "- `learning_rate` $\\in \\left[ 10^{-4}, 10^{-1} \\right]$,\n",
    "- `max_iter` $\\in \\left[ 10, 10^3 \\right]$,\n",
    "- `max_depth` $\\in \\left[ 2, 100 \\right]$,\n",
    "- `min_samples_leaf` $\\in \\left[ 10, 100 \\right]$,\n",
    "- `l2_regularization` $\\in \\left[ 10^{-6}, 10^2 \\right]$,\n",
    "- `max_leaf_nodes` $\\in \\left[ 2, 50 \\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 23:49:35,468: INFO ==> Fitting the HistGradientBoostingRegressor...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters with the reduced dataset for HistGradientBoostingRegressor:\n",
      "\n",
      "    l2_regularization = 100.0\n",
      "    learning_rate = 0.05731995114756429\n",
      "    loss = least_squares\n",
      "    max_depth = 30\n",
      "    max_iter = 1000\n",
      "    max_leaf_nodes = 33\n",
      "    min_samples_leaf = 16\n",
      "\n",
      "RMSE of HistGradientBoostingRegressor on the validation set with the reduced dataset: 0.109 ± 0.009\n",
      "\n",
      "Best parameters with the full dataset for HistGradientBoostingRegressor:\n",
      "\n",
      "    l2_regularization = 5.001575677983704\n",
      "    learning_rate = 0.044662788252242826\n",
      "    loss = least_squares\n",
      "    max_depth = 95\n",
      "    max_iter = 865\n",
      "    max_leaf_nodes = 5\n",
      "    min_samples_leaf = 22\n",
      "\n",
      "RMSE of HistGradientBoostingRegressor on the validation set with the full dataset: 0.101 ± 0.006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble     import HistGradientBoostingRegressor\n",
    "from skopt                import BayesSearchCV\n",
    "from skopt.space          import Real, Integer, Categorical\n",
    "from sklearn.metrics      import mean_squared_error\n",
    "from mltools.libscore     import ViewCV\n",
    "from mltools.libplot      import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params = {'max_iter':          Integer(1.0e1, 1.0e3, prior='log-uniform'),\n",
    "                 'max_depth':         Integer(2, 100,  prior='uniform'),\n",
    "                 'max_leaf_nodes':    Integer(2, 50,   prior='uniform'),\n",
    "                 'min_samples_leaf':  Integer(10, 100, prior='uniform'),\n",
    "                 'learning_rate':     Real(1.0e-4, 1.0e-1, prior='log-uniform'),\n",
    "                 'l2_regularization': Real(1.0e-6, 1.0e2,  prior='log-uniform'),\n",
    "                 'loss':              Categorical(['least_squares', 'least_absolute_deviation'])\n",
    "                } #------------------------------------------------------------------------------------- define the hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "estimator      = HistGradientBoostingRegressor(scoring='loss', validation_fraction=None, n_iter_no_change=10, random_state=RAND)\n",
    "cv_hypersearch = BayesSearchCV(estimator=estimator,\n",
    "                               search_spaces=search_params,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               n_jobs=-1,\n",
    "                               refit=True,\n",
    "                               n_iter=20,\n",
    "                               random_state=RAND,\n",
    "                               cv=cv\n",
    "                              ) #---------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the {}...'.format(estimator.__class__.__name__))\n",
    "\n",
    "est_red_fit  = cv_hypersearch.fit(features_red, label_red) #------------------------------------------- fit the estimator (reduced dataset)\n",
    "joblib.dump(est_red_fit.best_estimator_, path.join(MOD_PATH, '{}_red.joblib'.format(estimator.__class__.__name__)))\n",
    "\n",
    "results_red  = ViewCV(est_red_fit) #------------------------------------------------------------------- get the scoring results (reduced dataset)\n",
    "print('\\nBest parameters with the reduced dataset for {}:\\n'.format(estimator.__class__.__name__))\n",
    "pretty(results_red.best_parameters)\n",
    "\n",
    "print('\\nRMSE of {} on the validation set with the reduced dataset: {:.3f} ± {:.3f}'.format(estimator.__class__.__name__,\n",
    "                                                                                            np.sqrt(-results_red.test_mean()),\n",
    "                                                                                            results_red.test_std()\n",
    "                                                                                           )\n",
    "     )\n",
    "\n",
    "est_full_fit = cv_hypersearch.fit(features_full, label_full) #----------------------------------------- fit the estimator (full dataset)\n",
    "joblib.dump(est_full_fit.best_estimator_, path.join(MOD_PATH, '{}_full.joblib'.format(estimator.__class__.__name__)))\n",
    "\n",
    "results_full = ViewCV(est_full_fit) #------------------------------------------------------------------ get the scoring results (full dataset)\n",
    "print('\\nBest parameters with the full dataset for {}:\\n'.format(estimator.__class__.__name__))\n",
    "pretty(results_full.best_parameters)\n",
    "\n",
    "print('\\nRMSE of {} on the validation set with the full dataset: {:.3f} ± {:.3f}'.format(estimator.__class__.__name__,\n",
    "                                                                                         np.sqrt(-results_full.test_mean()),\n",
    "                                                                                         results_full.test_std()\n",
    "                                                                                        )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and Test Set Performance\n",
    "\n",
    "It seems that in general the **linear models** perform at best when **no regularization** is involved, thus the simple **linear regression** model represents a very good baseline for improvement. The **support vector machines** perform better in the presence of a kernel function (**Gaussian** in this case) and improve the results of the linear regression. In principle also the **random forest** regression is a very good prediction algorithm in this case even though it is strongly influenced by the presence of the initial values in the dataset. However, in the family of decision trees, the **boosted trees** vastly outperform the random forest, especially when the initial values are not present. \n",
    "\n",
    "Notice that we applied a scaler at the beginning in order to improve the convergence of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 23:50:59,262: DEBUG ==> Load linear regression for the reduced dataset.\n",
      "2020-04-29 23:50:59,264: DEBUG ==> Load elastic net for the reduced dataset.\n",
      "2020-04-29 23:50:59,265: DEBUG ==> Load linear svr for the reduced dataset.\n",
      "2020-04-29 23:50:59,267: DEBUG ==> Load svr for the reduced dataset.\n",
      "2020-04-29 23:50:59,519: DEBUG ==> Load random forest for the reduced dataset.\n",
      "2020-04-29 23:50:59,572: DEBUG ==> Load boosted trees for the reduced dataset.\n",
      "2020-04-29 23:50:59,573: DEBUG ==> Load linear regression for the full dataset.\n",
      "2020-04-29 23:50:59,574: DEBUG ==> Load elastic net for the full dataset.\n",
      "2020-04-29 23:50:59,575: DEBUG ==> Load linear svr for the full dataset.\n",
      "2020-04-29 23:50:59,577: DEBUG ==> Load svr for the full dataset.\n",
      "2020-04-29 23:50:59,773: DEBUG ==> Load random forest for the full dataset.\n",
      "2020-04-29 23:50:59,815: DEBUG ==> Load boosted trees for the full dataset.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# load the models for the reduced dataset\n",
    "if path.isfile(path.join(MOD_PATH, 'LinearRegression_red.joblib')):\n",
    "    lin_reg_red = joblib.load(path.join(MOD_PATH, 'LinearRegression_red.joblib'))\n",
    "    log.debug('Load linear regression for the reduced dataset.')\n",
    "else:\n",
    "    log.error('Cannot find linear regression model for the reduced dataset!')\n",
    "\n",
    "if path.isfile(path.join(MOD_PATH, 'ElasticNet_red.joblib')):\n",
    "    el_net_red = joblib.load(path.join(MOD_PATH, 'ElasticNet_red.joblib'))\n",
    "    log.debug('Load elastic net for the reduced dataset.')\n",
    "else:\n",
    "    log.error('Cannot find elastic net model for the reduced dataset!')\n",
    "\n",
    "if path.isfile(path.join(MOD_PATH, 'LinearSVR_red.joblib')):\n",
    "    lin_svr_red = joblib.load(path.join(MOD_PATH, 'LinearSVR_red.joblib'))\n",
    "    log.debug('Load linear svr for the reduced dataset.')\n",
    "else:\n",
    "    log.error('Cannot find linear svr model for the reduced dataset!')\n",
    "\n",
    "if path.isfile(path.join(MOD_PATH, 'SVR_red.joblib')):\n",
    "    svr_red = joblib.load(path.join(MOD_PATH, 'SVR_red.joblib'))\n",
    "    log.debug('Load svr for the reduced dataset.')\n",
    "else:\n",
    "    log.error('Cannot find svr model for the reduced dataset!')\n",
    "\n",
    "if path.isfile(path.join(MOD_PATH, 'RandomForestRegressor_red.joblib')):\n",
    "    rnd_for_red = joblib.load(path.join(MOD_PATH, 'RandomForestRegressor_red.joblib'))\n",
    "    log.debug('Load random forest for the reduced dataset.')\n",
    "else:\n",
    "    log.error('Cannot find random forest model for the reduced dataset!')\n",
    "\n",
    "if path.isfile(path.join(MOD_PATH, 'HistGradientBoostingRegressor_red.joblib')):\n",
    "    grd_boost_red = joblib.load(path.join(MOD_PATH, 'HistGradientBoostingRegressor_red.joblib'))\n",
    "    log.debug('Load boosted trees for the reduced dataset.')\n",
    "else:\n",
    "    log.error('Cannot find boosted trees model for the reduced dataset!')\n",
    "    \n",
    "# load the models for the full dataset\n",
    "if path.isfile(path.join(MOD_PATH, 'LinearRegression_full.joblib')):\n",
    "    lin_reg_full = joblib.load(path.join(MOD_PATH, 'LinearRegression_full.joblib'))\n",
    "    log.debug('Load linear regression for the full dataset.')\n",
    "else:\n",
    "    log.error('Cannot find linear regression model for the full dataset!')\n",
    "\n",
    "if path.isfile(path.join(MOD_PATH, 'ElasticNet_full.joblib')):\n",
    "    el_net_full = joblib.load(path.join(MOD_PATH, 'ElasticNet_full.joblib'))\n",
    "    log.debug('Load elastic net for the full dataset.')\n",
    "else:\n",
    "    log.error('Cannot find elastic net model for the full dataset!')\n",
    "\n",
    "if path.isfile(path.join(MOD_PATH, 'LinearSVR_full.joblib')):\n",
    "    lin_svr_full = joblib.load(path.join(MOD_PATH, 'LinearSVR_full.joblib'))\n",
    "    log.debug('Load linear svr for the full dataset.')\n",
    "else:\n",
    "    log.error('Cannot find linear svr model for the full dataset!')\n",
    "\n",
    "if path.isfile(path.join(MOD_PATH, 'SVR_full.joblib')):\n",
    "    svr_full = joblib.load(path.join(MOD_PATH, 'SVR_full.joblib'))\n",
    "    log.debug('Load svr for the full dataset.')\n",
    "else:\n",
    "    log.error('Cannot find svr model for the full dataset!')\n",
    "\n",
    "if path.isfile(path.join(MOD_PATH, 'RandomForestRegressor_full.joblib')):\n",
    "    rnd_for_full = joblib.load(path.join(MOD_PATH, 'RandomForestRegressor_full.joblib'))\n",
    "    log.debug('Load random forest for the full dataset.')\n",
    "else:\n",
    "    log.error('Cannot find random forest model for the full dataset!')\n",
    "\n",
    "if path.isfile(path.join(MOD_PATH, 'HistGradientBoostingRegressor_full.joblib')):\n",
    "    grd_boost_full = joblib.load(path.join(MOD_PATH, 'HistGradientBoostingRegressor_full.joblib'))\n",
    "    log.debug('Load boosted trees for the full dataset.')\n",
    "else:\n",
    "    log.error('Cannot find boosted trees model for the full dataset!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now show the performance on the test set, which basically confirms what we could have expected from the evaluation on the cross-validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE of the LinearRegression on the test set of the reduced dataset: 0.328\n",
      "RMSE of the ElasticNet on the test set of the reduced dataset: 0.357\n",
      "RMSE of the LinearSVR on the test set of the reduced dataset: 0.392\n",
      "RMSE of the SVR on the test set of the reduced dataset: 0.173\n",
      "RMSE of the RandomForestRegressor on the test set of the reduced dataset: 0.191\n",
      "RMSE of the HistGradientBoostingRegressor on the test set of the reduced dataset: 0.068\n",
      "\n",
      "RMSE of the LinearRegression on the test set of the full dataset: 0.326\n",
      "RMSE of the ElasticNet on the test set of the full dataset: 0.357\n",
      "RMSE of the LinearSVR on the test set of the full dataset: 0.336\n",
      "RMSE of the SVR on the test set of the full dataset: 0.096\n",
      "RMSE of the RandomForestRegressor on the test set of the full dataset: 0.097\n",
      "RMSE of the HistGradientBoostingRegressor on the test set of the full dataset: 0.080\n"
     ]
    }
   ],
   "source": [
    "# features\n",
    "features_test_red  = df_red_test.drop(columns='exp')\n",
    "if isinstance(red_scaler, (skl.preprocessing.RobustScaler, skl.preprocessing.StandardScaler, skl.preprocessing.MinMaxScaler)):\n",
    "    features_test_red = red_scaler.transform(features_test_red)\n",
    "    \n",
    "features_test_full = df_full_test.drop(columns='exp')\n",
    "if isinstance(full_scaler, (skl.preprocessing.RobustScaler, skl.preprocessing.StandardScaler, skl.preprocessing.MinMaxScaler)):\n",
    "    features_test_full = full_scaler.transform(features_test_full)\n",
    "\n",
    "# labels\n",
    "label_test_red  = df_red_test['exp']\n",
    "label_test_full = df_full_test['exp']\n",
    "\n",
    "# compute performance\n",
    "print('\\nRMSE of the {} on the test set of the reduced dataset: {:.3f}'.format(lin_reg_red.__class__.__name__, np.sqrt(mean_squared_error(y_true=label_test_red, y_pred=lin_reg_red.predict(features_test_red)))))\n",
    "print('RMSE of the {} on the test set of the reduced dataset: {:.3f}'.format(el_net_red.__class__.__name__, np.sqrt(mean_squared_error(y_true=label_test_red, y_pred=el_net_red.predict(features_test_red)))))\n",
    "print('RMSE of the {} on the test set of the reduced dataset: {:.3f}'.format(lin_svr_red.__class__.__name__, np.sqrt(mean_squared_error(y_true=label_test_red, y_pred=lin_svr_red.predict(features_test_red)))))\n",
    "print('RMSE of the {} on the test set of the reduced dataset: {:.3f}'.format(svr_red.__class__.__name__, np.sqrt(mean_squared_error(y_true=label_test_red, y_pred=svr_red.predict(features_test_red)))))\n",
    "print('RMSE of the {} on the test set of the reduced dataset: {:.3f}'.format(rnd_for_red.__class__.__name__, np.sqrt(mean_squared_error(y_true=label_test_red, y_pred=rnd_for_red.predict(features_test_red)))))\n",
    "print('RMSE of the {} on the test set of the reduced dataset: {:.3f}'.format(grd_boost_red.__class__.__name__, np.sqrt(mean_squared_error(y_true=label_test_red, y_pred=grd_boost_red.predict(features_test_red)))))\n",
    "\n",
    "print('\\nRMSE of the {} on the test set of the full dataset: {:.3f}'.format(lin_reg_red.__class__.__name__, np.sqrt(mean_squared_error(y_true=label_test_full, y_pred=lin_reg_full.predict(features_test_full)))))\n",
    "print('RMSE of the {} on the test set of the full dataset: {:.3f}'.format(el_net_red.__class__.__name__, np.sqrt(mean_squared_error(y_true=label_test_full, y_pred=el_net_full.predict(features_test_full)))))\n",
    "print('RMSE of the {} on the test set of the full dataset: {:.3f}'.format(lin_svr_red.__class__.__name__, np.sqrt(mean_squared_error(y_true=label_test_full, y_pred=lin_svr_full.predict(features_test_full)))))\n",
    "print('RMSE of the {} on the test set of the full dataset: {:.3f}'.format(svr_red.__class__.__name__, np.sqrt(mean_squared_error(y_true=label_test_full, y_pred=svr_full.predict(features_test_full)))))\n",
    "print('RMSE of the {} on the test set of the full dataset: {:.3f}'.format(rnd_for_red.__class__.__name__, np.sqrt(mean_squared_error(y_true=label_test_full, y_pred=rnd_for_full.predict(features_test_full)))))\n",
    "print('RMSE of the {} on the test set of the full dataset: {:.3f}'.format(grd_boost_red.__class__.__name__, np.sqrt(mean_squared_error(y_true=label_test_full, y_pred=grd_boost_full.predict(features_test_full)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking and Performance Improvement\n",
    "\n",
    "We now consider the **stacking ensemble** using the results on the **validation sets** in the previous analysis. We select the algorithms based on their performance and the fact that they should be as different as possible among them. We will therefore select:\n",
    "\n",
    "- `LinearRegression` (if necessary),\n",
    "- `SVR`,\n",
    "- `HistGradientBoostingRegressor`.\n",
    "\n",
    "We split the original training set in two to create the two training levels and we use a `LinearRegression` as meta-learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from skopt                   import BayesSearchCV\n",
    "from skopt.space             import Real, Integer, Categorical\n",
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.svm             import SVR\n",
    "from sklearn.experimental    import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble        import HistGradientBoostingRegressor\n",
    "\n",
    "# split the training set\n",
    "lv1_train_red, lv2_train_red, lv1_train_lab_red, lv2_train_lab_red     = train_test_split(features_red, label_red, test_size=0.5, shuffle=False)\n",
    "lv1_train_full, lv2_train_full, lv1_train_lab_full, lv2_train_lab_full = train_test_split(features_full, label_full, test_size=0.5, shuffle=False)\n",
    "test_red, test_full, test_label_red, test_label_full                   = features_test_red, features_test_full, label_test_red, label_test_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from the _reduced_ dataset and compute the RMSE on the prediction using the stacking ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-30 00:10:07,776: INFO ==> Training lv. 1 estimators.\n",
      "2020-04-30 00:15:02,716: INFO ==> Training meta estimator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of the stacked ensemble on the test set: 0.128\n"
     ]
    }
   ],
   "source": [
    "# define the algorithms\n",
    "lin_reg = LinearRegression()\n",
    "svr_rbf = SVR(kernel='rbf')\n",
    "grd_bst = HistGradientBoostingRegressor(scoring='loss', validation_fraction=None, n_iter_no_change=10, random_state=RAND)\n",
    "\n",
    "# create search spaces for each of them\n",
    "lin_reg_params  = {'fit_intercept': [0, 1],\n",
    "                   'normalize':     [0, 1]\n",
    "                  }\n",
    "svr_rbf_params  = {'epsilon':   Real(1.0e-4, 1.0e-1, prior='log-uniform'),\n",
    "                   'C':         Real(1.0e-1, 1.0e3,  prior='log-uniform'),\n",
    "                   'gamma':     Real(1.0e-1, 1.0e1,  prior='log-uniform'),\n",
    "                   'shrinking': Integer(0, 1)\n",
    "                  }\n",
    "grd_bst_params = {'max_iter':          Integer(1.0e1, 1.0e3, prior='log-uniform'),\n",
    "                  'max_depth':         Integer(2, 100,  prior='uniform'),\n",
    "                  'max_leaf_nodes':    Integer(2, 50,   prior='uniform'),\n",
    "                  'min_samples_leaf':  Integer(10, 100, prior='uniform'),\n",
    "                  'learning_rate':     Real(1.0e-4, 1.0e-1, prior='log-uniform'),\n",
    "                  'l2_regularization': Real(1.0e-6, 1.0e2,  prior='log-uniform'),\n",
    "                  'loss':              Categorical(['least_squares', 'least_absolute_deviation'])\n",
    "                 }\n",
    "\n",
    "# fit the algorithms on the first level for the reduced dataset\n",
    "log.info('Training lv. 1 estimators.')\n",
    "lin_reg_est = GridSearchCV(estimator=lin_reg,\n",
    "                           param_grid=lin_reg_params,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           n_jobs=-1,\n",
    "                           refit=True,\n",
    "                           cv=cv\n",
    "                          )\n",
    "lin_reg_fit = lin_reg_est.fit(lv1_train_red, lv1_train_lab_red)\n",
    "\n",
    "svr_rbf_est = BayesSearchCV(estimator=svr_rbf,\n",
    "                            search_spaces=svr_rbf_params,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            n_jobs=-1,\n",
    "                            refit=True,\n",
    "                            n_iter=100,\n",
    "                            random_state=RAND,\n",
    "                            cv=cv\n",
    "                           )\n",
    "svr_rbf_fit = svr_rbf_est.fit(lv1_train_red, lv1_train_lab_red)\n",
    "\n",
    "grd_bst_est = BayesSearchCV(estimator=grd_bst,\n",
    "                            search_spaces=grd_bst_params,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            n_jobs=-1,\n",
    "                            refit=True,\n",
    "                            n_iter=20,\n",
    "                            random_state=RAND,\n",
    "                            cv=cv\n",
    "                           )\n",
    "grd_bst_fit = grd_bst_est.fit(lv1_train_red, lv1_train_lab_red)\n",
    "\n",
    "# compute the predictions on the second level of the reduced dataset\n",
    "lin_reg_lv2_pred = lin_reg_fit.best_estimator_.predict(lv2_train_red)\n",
    "svr_rbf_lv2_pred = svr_rbf_fit.best_estimator_.predict(lv2_train_red)\n",
    "grd_bst_lv2_pred = grd_bst_fit.best_estimator_.predict(lv2_train_red)\n",
    "stacked_lv2_pred = np.c_[#lin_reg_lv2_pred,\n",
    "                         svr_rbf_lv2_pred,\n",
    "                         grd_bst_lv2_pred\n",
    "                        ]\n",
    "\n",
    "# compute the predictions on the test data of the reduced dataset\n",
    "lin_reg_test_pred = lin_reg_fit.best_estimator_.predict(test_red)\n",
    "svr_rbf_test_pred = svr_rbf_fit.best_estimator_.predict(test_red)\n",
    "grd_bst_test_pred = grd_bst_fit.best_estimator_.predict(test_red)\n",
    "stacked_test_pred = np.c_[#lin_reg_test_pred,\n",
    "                          svr_rbf_test_pred,\n",
    "                          grd_bst_test_pred\n",
    "                         ]\n",
    "\n",
    "# define the meta learner\n",
    "log.info('Training meta estimator.')\n",
    "meta_learner         = LinearRegression()\n",
    "meta_learner_params  = {'fit_intercept': [0, 1],\n",
    "                        'normalize':     [0, 1]\n",
    "                       }\n",
    "meta_learner_est     = GridSearchCV(estimator=meta_learner,\n",
    "                               param_grid=meta_learner_params,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               n_jobs=-1,\n",
    "                               refit=True,\n",
    "                               cv=cv\n",
    "                              )\n",
    "meta_learner_fit     = meta_learner_est.fit(stacked_lv2_pred, lv2_train_lab_red)\n",
    "\n",
    "# compute the final predictions\n",
    "red_predictions = meta_learner_fit.best_estimator_.predict(stacked_test_pred)\n",
    "print('RMSE of the stacked ensemble on the test set: {:.3f}'.format(np.sqrt(mean_squared_error(y_true=test_label_red, y_pred=red_predictions))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then repeat the same analysis also for the _full_ dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-30 00:00:05,336: INFO ==> Training lv. 1 estimators.\n",
      "2020-04-30 00:05:07,136: INFO ==> Training meta estimator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of the stacked ensemble on the test set: 0.103\n"
     ]
    }
   ],
   "source": [
    "# define the algorithms\n",
    "lin_reg = LinearRegression()\n",
    "svr_rbf = SVR(kernel='rbf')\n",
    "grd_bst = HistGradientBoostingRegressor(scoring='loss', validation_fraction=None, n_iter_no_change=10, random_state=RAND)\n",
    "\n",
    "# create search spaces for each of them\n",
    "lin_reg_params  = {'fit_intercept': [0, 1],\n",
    "                   'normalize':     [0, 1]\n",
    "                  }\n",
    "svr_rbf_params  = {'epsilon':   Real(1.0e-4, 1.0e-1, prior='log-uniform'),\n",
    "                   'C':         Real(1.0e-1, 1.0e3,  prior='log-uniform'),\n",
    "                   'gamma':     Real(1.0e-1, 1.0e1,  prior='log-uniform'),\n",
    "                   'shrinking': Integer(0, 1)\n",
    "                  }\n",
    "grd_bst_params = {'max_iter':          Integer(1.0e1, 1.0e3, prior='log-uniform'),\n",
    "                  'max_depth':         Integer(2, 100,  prior='uniform'),\n",
    "                  'max_leaf_nodes':    Integer(2, 50,   prior='uniform'),\n",
    "                  'min_samples_leaf':  Integer(10, 100, prior='uniform'),\n",
    "                  'learning_rate':     Real(1.0e-4, 1.0e-1, prior='log-uniform'),\n",
    "                  'l2_regularization': Real(1.0e-6, 1.0e2,  prior='log-uniform'),\n",
    "                  'loss':              Categorical(['least_squares', 'least_absolute_deviation'])\n",
    "                 }\n",
    "\n",
    "# fit the algorithms on the first level for the reduced dataset\n",
    "log.info('Training lv. 1 estimators.')\n",
    "lin_reg_est = GridSearchCV(estimator=lin_reg,\n",
    "                           param_grid=lin_reg_params,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           n_jobs=-1,\n",
    "                           refit=True,\n",
    "                           cv=cv\n",
    "                          )\n",
    "lin_reg_fit = lin_reg_est.fit(lv1_train_full, lv1_train_lab_full)\n",
    "\n",
    "svr_rbf_est = BayesSearchCV(estimator=svr_rbf,\n",
    "                            search_spaces=svr_rbf_params,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            n_jobs=-1,\n",
    "                            refit=True,\n",
    "                            n_iter=100,\n",
    "                            random_state=RAND,\n",
    "                            cv=cv\n",
    "                           )\n",
    "svr_rbf_fit = svr_rbf_est.fit(lv1_train_full, lv1_train_lab_full)\n",
    "\n",
    "grd_bst_est = BayesSearchCV(estimator=grd_bst,\n",
    "                            search_spaces=grd_bst_params,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            n_jobs=-1,\n",
    "                            refit=True,\n",
    "                            n_iter=20,\n",
    "                            random_state=RAND,\n",
    "                            cv=cv\n",
    "                           )\n",
    "grd_bst_fit = grd_bst_est.fit(lv1_train_full, lv1_train_lab_full)\n",
    "\n",
    "# compute the predictions on the second level of the reduced dataset\n",
    "lin_reg_lv2_pred = lin_reg_fit.best_estimator_.predict(lv2_train_full)\n",
    "svr_rbf_lv2_pred = svr_rbf_fit.best_estimator_.predict(lv2_train_full)\n",
    "grd_bst_lv2_pred = grd_bst_fit.best_estimator_.predict(lv2_train_full)\n",
    "stacked_lv2_pred = np.c_[#lin_reg_lv2_pred,\n",
    "                         svr_rbf_lv2_pred,\n",
    "                         grd_bst_lv2_pred\n",
    "                        ]\n",
    "\n",
    "# compute the predictions on the test data of the reduced dataset\n",
    "lin_reg_test_pred = lin_reg_fit.best_estimator_.predict(test_full)\n",
    "svr_rbf_test_pred = svr_rbf_fit.best_estimator_.predict(test_full)\n",
    "grd_bst_test_pred = grd_bst_fit.best_estimator_.predict(test_full)\n",
    "stacked_test_pred = np.c_[#lin_reg_test_pred,\n",
    "                          svr_rbf_test_pred,\n",
    "                          grd_bst_test_pred\n",
    "                         ]\n",
    "\n",
    "# define the meta learner\n",
    "log.info('Training meta estimator.')\n",
    "meta_learner         = LinearRegression()\n",
    "meta_learner_params  = {'fit_intercept': [0, 1],\n",
    "                        'normalize':     [0, 1]\n",
    "                       }\n",
    "meta_learner_est     = GridSearchCV(estimator=meta_learner,\n",
    "                               param_grid=meta_learner_params,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               n_jobs=-1,\n",
    "                               refit=True,\n",
    "                               cv=cv\n",
    "                              )\n",
    "meta_learner_fit     = meta_learner_est.fit(stacked_lv2_pred, lv2_train_lab_red)\n",
    "\n",
    "# compute the final predictions\n",
    "full_predictions = meta_learner_fit.best_estimator_.predict(stacked_test_pred)\n",
    "print('RMSE of the stacked ensemble on the test set: {:.3f}'.format(np.sqrt(mean_squared_error(y_true=test_label_full, y_pred=full_predictions))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which again shows an improvement in the determination of the level-$\\infty$ predictions with respect to the isolate learner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
