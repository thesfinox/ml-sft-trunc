{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Level Truncation in String Field Theory\n",
    "\n",
    "Consider the data of lumps in bosonic String Field Theory (SFT) and extrapolate level-$\\infty$ predictions from finite level data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First of all we print the characteristics of the current setup (OS, cores, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current OS:                  Linux (kernel release: 5.6.11-arch1-1, architecture: x86_64)\n",
      "Number of available threads: 8\n",
      "Current CPU frequency:       2825 MHz (max: 3800 MHz)\n",
      "Available RAM memory:        8179 MB (tot: 15758 MB)\n"
     ]
    }
   ],
   "source": [
    "from mltools.libos import InfoOS\n",
    "\n",
    "print('Current OS:                  {} (kernel release: {}, architecture: {})'.format(InfoOS().os, InfoOS().kernel, InfoOS().arch))\n",
    "print('Number of available threads: {:d}'.format(InfoOS().threads))\n",
    "print('Current CPU frequency:       {:.0f} MHz (max: {:.0f} MHz)'.format(InfoOS().freq, InfoOS().freqm))\n",
    "print('Available RAM memory:        {:d} MB (tot: {:d} MB)'.format(InfoOS().vmav, InfoOS().vmtot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We establish early in the notebook the amount of cores we want to use in order to parallelize computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = int(InfoOS().threads / 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check the installed versions of the packages we are going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7\n",
      "Matplot version: 3.2.1\n",
      "Numpy version: 1.18.4\n",
      "Pandas version: 1.0.3\n",
      "Scikit-learn version: 0.22.2.post1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import random     as rnd\n",
    "import sklearn    as skl\n",
    "import numpy      as np\n",
    "import pandas     as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning) # ignore user warnings: nothing that I can really do anything about it...\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=12)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# print the version of the modules\n",
    "print('Python version: {:d}.{:d}'      .format(sys.version_info.major, sys.version_info.minor))\n",
    "print('Matplot version: {}'            .format(mpl.__version__))\n",
    "print('Numpy version: {}'              .format(np.__version__))\n",
    "print('Pandas version: {}'             .format(pd.__version__))\n",
    "print('Scikit-learn version: {}'       .format(skl.__version__))\n",
    "\n",
    "# fix random_seed\n",
    "RAND = 42\n",
    "rnd.seed(RAND)\n",
    "np.random.seed(RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Preparation\n",
    "\n",
    "In order to save the results of the analysis, we need to create the structure of directories in the current repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "\n",
    "ROOT_DIR = '.' #-------------------------------------------------- root directory\n",
    "IMG_DIR  = 'img' #------------------------------------------------ directory of images\n",
    "MOD_DIR  = 'models' #--------------------------------------------- directory of saved models\n",
    "LOG_DIR  = 'log' #------------------------------------------------ directory of logs\n",
    "OUT_DIR  = 'output' #--------------------------------------------- directory for saved predictions, relevant output, etc.\n",
    "\n",
    "DB_NAME = 'data_sft_dict' #--------------------------------------- name of the dataset\n",
    "DB_FILE = DB_NAME + '.json' #------------------------------------- full name with extension\n",
    "DB_PATH = path.join(ROOT_DIR, DB_FILE) #-------------------------- full path of the dataset\n",
    "\n",
    "# define full paths\n",
    "IMG_PATH = path.join(ROOT_DIR, IMG_DIR)\n",
    "MOD_PATH = path.join(ROOT_DIR, MOD_DIR)\n",
    "LOG_PATH = path.join(ROOT_DIR, LOG_DIR)\n",
    "OUT_PATH = path.join(ROOT_DIR, OUT_DIR)\n",
    "\n",
    "# create directories if non existent\n",
    "if not path.isdir(IMG_PATH):\n",
    "    makedirs(IMG_PATH, exist_ok=True)\n",
    "if not path.isdir(MOD_PATH):\n",
    "    makedirs(MOD_PATH, exist_ok=True)\n",
    "if not path.isdir(LOG_PATH):\n",
    "    makedirs(LOG_PATH, exist_ok=True)\n",
    "if not path.isdir(OUT_PATH):\n",
    "    makedirs(OUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a logging session to store debug information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotating existing logs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:38:01,529: INFO ==> New logging session started. Log is at ./log/data_sft_dict.log.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from mltools.liblog import create_logfile\n",
    "\n",
    "path_to_log = path.join(LOG_PATH,\n",
    "                        DB_NAME + '.log'\n",
    "                       ) #----------------------------------------------------- log path\n",
    "log = create_logfile(path_to_log,\n",
    "                     name=DB_NAME,\n",
    "                     level=logging.DEBUG\n",
    "                    ) #-------------------------------------------------------- create log file and session\n",
    "\n",
    "# these lines provide the same setup also for the Jupyter logging\n",
    "logger = logging.getLogger() #------------------------------------------------- get the current logging session\n",
    "\n",
    "fmt = logging.Formatter('%(asctime)s: %(levelname)s ==> %(message)s') #-------- customise the formatting options\n",
    "\n",
    "handler = logging.StreamHandler() #-------------------------------------------- handle the stream to the default (stderr)\n",
    "handler.setLevel(logging.DEBUG) #---------------------------------------------- print everything\n",
    "handler.setFormatter(fmt) #---------------------------------------------------- set the formatting options\n",
    "\n",
    "logger.handlers = [handler] #-------------------------------------------------- override the default stream\n",
    "\n",
    "# we are ready to go!\n",
    "log.info('New logging session started. Log is at {}.'.format(path_to_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Database\n",
    "\n",
    "We then import the database from its JSON format and begin to analyse it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:38:01,557: DEBUG ==> Database loaded.\n"
     ]
    }
   ],
   "source": [
    "if path.isfile(DB_PATH):\n",
    "    df = pd.read_json(DB_PATH)\n",
    "    log.debug('Database loaded.')\n",
    "else:\n",
    "    print('Database is not in the file tree!')\n",
    "    log.error('Cannot find database!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then show the `dtypes` of each column to get an idea of the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init      object\n",
       "exp       object\n",
       "weight    object\n",
       "type      object\n",
       "2         object\n",
       "3         object\n",
       "4         object\n",
       "5         object\n",
       "6         object\n",
       "7         object\n",
       "8         object\n",
       "9         object\n",
       "10        object\n",
       "11        object\n",
       "12        object\n",
       "13        object\n",
       "14        object\n",
       "15        object\n",
       "16        object\n",
       "17        object\n",
       "18        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset we have the predictions of the position of lumps in bosonic SFT for finite levels in the numbered columns and the extrapolation for level-$\\infty$ in the column _exp_. We want to use known data (including the _weight_ and the _type_ of the input data) to predict the _exp_ labels (_init_ in principle can be left out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>init</th>\n",
       "      <th>exp</th>\n",
       "      <th>weight</th>\n",
       "      <th>type</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, -1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 4, 9, 0, 0.25, 1, 2.25, 4, 0, 0.25, ...</td>\n",
       "      <td>[2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.0001, 0, 1.0001, 1.0001, 1.0001, 1.0001, 0,...</td>\n",
       "      <td>[1, 0, -1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 4, 9, 0, 0.249950007499, 0.999800029...</td>\n",
       "      <td>[2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "      <td>[1.000099754465899, -4.382819109385611e-08, 0....</td>\n",
       "      <td>[1.000099754261711, -6.385189815988693e-08, 0....</td>\n",
       "      <td>[1.000099495309939, -1.9972775228453091e-07, 0...</td>\n",
       "      <td>[1.000099494726808, -1.724421881015622e-07, 0....</td>\n",
       "      <td>[1.000099223845491, -3.2173432889712715e-07, 0...</td>\n",
       "      <td>[1.000099222907449, -2.856173963606407e-07, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.000098951488667, -3.9768513795577186e-07, 0...</td>\n",
       "      <td>[1.000098684133785, -5.470716468222031e-07, 0....</td>\n",
       "      <td>[1.000098682609473, -5.081256574169557e-07, 0....</td>\n",
       "      <td>[1.000098418312006, -6.559751804689415e-07, 0....</td>\n",
       "      <td>[1.000098416531483, -6.169537248661669e-07, 0....</td>\n",
       "      <td>[1.000098155198292, -7.630807670831046e-07, 0....</td>\n",
       "      <td>[1.000098153176776, -7.242020485026188e-07, 0....</td>\n",
       "      <td>[1.000097894670832, -8.685182838696036e-07, 0....</td>\n",
       "      <td>[1.000097892420157, -8.29888184051414e-07, 0.9...</td>\n",
       "      <td>[1.000097636616163, -9.72346758033437e-07, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.001, 0, 1.001, 1.001, 1.001, 1.001, 0, 0, 0...</td>\n",
       "      <td>[1, 0, -1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 4, 9, 0, 0.24950074900124802, 0.9980...</td>\n",
       "      <td>[2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "      <td>[1.000976232275641, -3.8820163895943e-06, 0.90...</td>\n",
       "      <td>[1.000976049237533, -5.9682788728782085e-06, 0...</td>\n",
       "      <td>[1.000952815386108, -1.689178344782202e-05, 0....</td>\n",
       "      <td>[1.000952352120989, -1.5047680126166993e-05, 0...</td>\n",
       "      <td>[1.000929855311967, -2.5515953552563565e-05, 0...</td>\n",
       "      <td>[1.000929190755388, -2.3264350581688953e-05, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.0009075350494, -3.03802946004187e-05, 0.664...</td>\n",
       "      <td>[1.000888236249875, -3.860013055708573e-05, 0....</td>\n",
       "      <td>[1.000887354078869, -3.657126395737419e-05, 0....</td>\n",
       "      <td>[1.000869431716091, -4.387998808033297e-05, 0....</td>\n",
       "      <td>[1.000868490272024, -4.199905525467674e-05, 0....</td>\n",
       "      <td>[1.000851773655717, -4.853491335211209e-05, 0....</td>\n",
       "      <td>[1.000850791489764, -4.6788451470318466e-05, 0...</td>\n",
       "      <td>[1.000835137520457, -5.266383019452565e-05, 0....</td>\n",
       "      <td>[1.000834127701389, -5.1036790056833293e-05, 0...</td>\n",
       "      <td>[1.000819417460843, -5.634321216588246e-05, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                init  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.0001, 0, 1.0001, 1.0001, 1.0001, 1.0001, 0,...   \n",
       "2  [1.001, 0, 1.001, 1.001, 1.001, 1.001, 0, 0, 0...   \n",
       "\n",
       "                                               exp  \\\n",
       "0  [1, 0, -1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]   \n",
       "1  [1, 0, -1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]   \n",
       "2  [1, 0, -1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]   \n",
       "\n",
       "                                              weight  \\\n",
       "0  [0, 0, 1, 4, 9, 0, 0.25, 1, 2.25, 4, 0, 0.25, ...   \n",
       "1  [0, 0, 1, 4, 9, 0, 0.249950007499, 0.999800029...   \n",
       "2  [0, 0, 1, 4, 9, 0, 0.24950074900124802, 0.9980...   \n",
       "\n",
       "                                            type  \\\n",
       "0  [2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]   \n",
       "1  [2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]   \n",
       "2  [2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]   \n",
       "\n",
       "                                                   2  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000099754465899, -4.382819109385611e-08, 0....   \n",
       "2  [1.000976232275641, -3.8820163895943e-06, 0.90...   \n",
       "\n",
       "                                                   3  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000099754261711, -6.385189815988693e-08, 0....   \n",
       "2  [1.000976049237533, -5.9682788728782085e-06, 0...   \n",
       "\n",
       "                                                   4  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000099495309939, -1.9972775228453091e-07, 0...   \n",
       "2  [1.000952815386108, -1.689178344782202e-05, 0....   \n",
       "\n",
       "                                                   5  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000099494726808, -1.724421881015622e-07, 0....   \n",
       "2  [1.000952352120989, -1.5047680126166993e-05, 0...   \n",
       "\n",
       "                                                   6  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000099223845491, -3.2173432889712715e-07, 0...   \n",
       "2  [1.000929855311967, -2.5515953552563565e-05, 0...   \n",
       "\n",
       "                                                   7  ...  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]  ...   \n",
       "1  [1.000099222907449, -2.856173963606407e-07, 0....  ...   \n",
       "2  [1.000929190755388, -2.3264350581688953e-05, 0...  ...   \n",
       "\n",
       "                                                   9  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098951488667, -3.9768513795577186e-07, 0...   \n",
       "2  [1.0009075350494, -3.03802946004187e-05, 0.664...   \n",
       "\n",
       "                                                  10  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098684133785, -5.470716468222031e-07, 0....   \n",
       "2  [1.000888236249875, -3.860013055708573e-05, 0....   \n",
       "\n",
       "                                                  11  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098682609473, -5.081256574169557e-07, 0....   \n",
       "2  [1.000887354078869, -3.657126395737419e-05, 0....   \n",
       "\n",
       "                                                  12  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098418312006, -6.559751804689415e-07, 0....   \n",
       "2  [1.000869431716091, -4.387998808033297e-05, 0....   \n",
       "\n",
       "                                                  13  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098416531483, -6.169537248661669e-07, 0....   \n",
       "2  [1.000868490272024, -4.199905525467674e-05, 0....   \n",
       "\n",
       "                                                  14  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098155198292, -7.630807670831046e-07, 0....   \n",
       "2  [1.000851773655717, -4.853491335211209e-05, 0....   \n",
       "\n",
       "                                                  15  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000098153176776, -7.242020485026188e-07, 0....   \n",
       "2  [1.000850791489764, -4.6788451470318466e-05, 0...   \n",
       "\n",
       "                                                  16  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000097894670832, -8.685182838696036e-07, 0....   \n",
       "2  [1.000835137520457, -5.266383019452565e-05, 0....   \n",
       "\n",
       "                                                  17  \\\n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]   \n",
       "1  [1.000097892420157, -8.29888184051414e-07, 0.9...   \n",
       "2  [1.000834127701389, -5.1036790056833293e-05, 0...   \n",
       "\n",
       "                                                  18  \n",
       "0      [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]  \n",
       "1  [1.000097636616163, -9.72346758033437e-07, 0.9...  \n",
       "2  [1.000819417460843, -5.634321216588246e-05, 0....  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "We first add a column representing the index of the system in which the data are inserted. We also reorder the dataset to have the labels as last column and the index as first. We drop the `init` value as we do not intend to use it for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['system'] = [[i] for i in df.index]\n",
    "df = df[['system', 'type', 'weight', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', 'exp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the first entry as it looks _suspiciously too perfect_ and might spoil the predictions. After that we reset the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[1:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then shuffle the database a first time to keep each system together but in different order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract each feature and pad them to the same length in order to stack all entries on top of each others (we will take care of artificially created **duplicates** later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltools.libtransformer import ExtractTensor\n",
    "\n",
    "for n in range(df.shape[0]):\n",
    "    df['system'].iloc[n] = df['system'].iloc[n] \\\n",
    "                         * np.prod(np.shape(df['type'].iloc[n])) #----------------------- reshape `system` to have same length as other columns in the same row\n",
    "    \n",
    "for feature in df:\n",
    "    df[feature] = ExtractTensor(flatten=True).fit_transform(df[feature]) #--------------- extract features in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every value in any column we want to extrapolate its level-$\\infty$ predictions. We therefore need a way to extract each value from each column and treat the set of extracted numbers as different datasets to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:38:01,926: DEBUG ==> Length of the spurious dataset: 900 samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system</th>\n",
       "      <th>type</th>\n",
       "      <th>weight</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.169090</td>\n",
       "      <td>1.103953</td>\n",
       "      <td>1.049652</td>\n",
       "      <td>1.042422</td>\n",
       "      <td>1.025771</td>\n",
       "      <td>1.023925</td>\n",
       "      <td>1.016453</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011730</td>\n",
       "      <td>1.011495</td>\n",
       "      <td>1.008954</td>\n",
       "      <td>1.008871</td>\n",
       "      <td>1.007160</td>\n",
       "      <td>1.007141</td>\n",
       "      <td>1.005919</td>\n",
       "      <td>1.00593</td>\n",
       "      <td>1.005018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.133441</td>\n",
       "      <td>1.073850</td>\n",
       "      <td>1.041536</td>\n",
       "      <td>1.033556</td>\n",
       "      <td>1.022857</td>\n",
       "      <td>1.020416</td>\n",
       "      <td>1.015351</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011415</td>\n",
       "      <td>1.010882</td>\n",
       "      <td>1.009026</td>\n",
       "      <td>1.008720</td>\n",
       "      <td>1.007436</td>\n",
       "      <td>1.007245</td>\n",
       "      <td>1.006306</td>\n",
       "      <td>1.00618</td>\n",
       "      <td>1.005465</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   system  type  weight         2         3         4         5         6  \\\n",
       "0      40     2     0.0  1.169090  1.103953  1.049652  1.042422  1.025771   \n",
       "1      26     2     0.0  1.133441  1.073850  1.041536  1.033556  1.022857   \n",
       "\n",
       "          7         8  ...        10        11        12        13        14  \\\n",
       "0  1.023925  1.016453  ...  1.011730  1.011495  1.008954  1.008871  1.007160   \n",
       "1  1.020416  1.015351  ...  1.011415  1.010882  1.009026  1.008720  1.007436   \n",
       "\n",
       "         15        16       17        18  exp  \n",
       "0  1.007141  1.005919  1.00593  1.005018    1  \n",
       "1  1.007245  1.006306  1.00618  1.005465    1  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the length of each entry after the extraction\n",
    "length = np.max(df['exp'].apply(np.shape))[0]\n",
    "\n",
    "# extract each correspondent values and \"expand\" them vertically\n",
    "df = pd.concat([pd.DataFrame({f: df[f].str[n].values for f in df}) for n in range(length)], axis=0).reset_index(drop=True)\n",
    "log.debug('Length of the spurious dataset: {:d} samples.'.format(df.shape[0]))\n",
    "\n",
    "# show a couple of samples for reference\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Duplicates and Spurious Entries\n",
    "\n",
    "Before proceeding any further we must then make sure that there are no **duplicate** rows in the database which would spoil the results as well as no **spurious** entries (namely those with `system` $= 0$ which were artificially introduced with the padding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:38:01,971: DEBUG ==> Length of the correct dataset: 718 samples.\n"
     ]
    }
   ],
   "source": [
    "# delete spurious elements\n",
    "df = df.loc[df['system'] > 0] #---------------------------- keep only system > 0\n",
    "\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates(subset=None, keep='first') #------- do not use a subset of columns and keep only the first appearance\n",
    "\n",
    "log.debug('Length of the correct dataset: {:d} samples.'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system</th>\n",
       "      <th>type</th>\n",
       "      <th>weight</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>7.180000e+02</td>\n",
       "      <td>718.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.314763</td>\n",
       "      <td>3.749304</td>\n",
       "      <td>1.981686</td>\n",
       "      <td>-1.649589</td>\n",
       "      <td>-1.817170</td>\n",
       "      <td>7.981259</td>\n",
       "      <td>8.445939</td>\n",
       "      <td>-34.541055</td>\n",
       "      <td>-36.169106</td>\n",
       "      <td>172.934603</td>\n",
       "      <td>...</td>\n",
       "      <td>-752.288433</td>\n",
       "      <td>-776.523313</td>\n",
       "      <td>3061.210057</td>\n",
       "      <td>3147.716636</td>\n",
       "      <td>-11562.460857</td>\n",
       "      <td>-11855.258680</td>\n",
       "      <td>40629.386999</td>\n",
       "      <td>41566.669584</td>\n",
       "      <td>-1.334843e+05</td>\n",
       "      <td>0.541783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.023788</td>\n",
       "      <td>0.662688</td>\n",
       "      <td>2.337013</td>\n",
       "      <td>4.548304</td>\n",
       "      <td>5.005473</td>\n",
       "      <td>21.558408</td>\n",
       "      <td>22.815235</td>\n",
       "      <td>111.039170</td>\n",
       "      <td>116.338079</td>\n",
       "      <td>613.372450</td>\n",
       "      <td>...</td>\n",
       "      <td>2852.135166</td>\n",
       "      <td>2942.302665</td>\n",
       "      <td>11912.541567</td>\n",
       "      <td>12240.685674</td>\n",
       "      <td>45391.801105</td>\n",
       "      <td>46512.063723</td>\n",
       "      <td>160087.941813</td>\n",
       "      <td>163698.216993</td>\n",
       "      <td>5.278422e+05</td>\n",
       "      <td>0.706857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-19.744040</td>\n",
       "      <td>-21.893983</td>\n",
       "      <td>-0.754568</td>\n",
       "      <td>-0.782633</td>\n",
       "      <td>-514.984097</td>\n",
       "      <td>-538.627792</td>\n",
       "      <td>-0.909456</td>\n",
       "      <td>...</td>\n",
       "      <td>-13321.170445</td>\n",
       "      <td>-13781.246472</td>\n",
       "      <td>-8.850113</td>\n",
       "      <td>-12.265769</td>\n",
       "      <td>-211473.396816</td>\n",
       "      <td>-216475.644423</td>\n",
       "      <td>-44.356923</td>\n",
       "      <td>-66.596211</td>\n",
       "      <td>-2.489024e+06</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.155340</td>\n",
       "      <td>-0.819357</td>\n",
       "      <td>-1.048764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.924887</td>\n",
       "      <td>-0.941950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.122517</td>\n",
       "      <td>-2.123912</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>-3.214377</td>\n",
       "      <td>-6.185564</td>\n",
       "      <td>0.045728</td>\n",
       "      <td>0.042314</td>\n",
       "      <td>-2.569285e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923914</td>\n",
       "      <td>0.935052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.974117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.987326</td>\n",
       "      <td>0.987066</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.004821</td>\n",
       "      <td>1.004445</td>\n",
       "      <td>1.001084</td>\n",
       "      <td>9.732395e-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.213367</td>\n",
       "      <td>0.795133</td>\n",
       "      <td>0.913984</td>\n",
       "      <td>1.408763</td>\n",
       "      <td>1.552803</td>\n",
       "      <td>0.960644</td>\n",
       "      <td>0.984305</td>\n",
       "      <td>2.341926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991467</td>\n",
       "      <td>0.996329</td>\n",
       "      <td>4.397037</td>\n",
       "      <td>6.811820</td>\n",
       "      <td>0.997875</td>\n",
       "      <td>1.001248</td>\n",
       "      <td>16.098058</td>\n",
       "      <td>16.131990</td>\n",
       "      <td>1.003658e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.239384</td>\n",
       "      <td>1.358098</td>\n",
       "      <td>122.931347</td>\n",
       "      <td>131.675490</td>\n",
       "      <td>2.275741</td>\n",
       "      <td>2.712998</td>\n",
       "      <td>2823.630938</td>\n",
       "      <td>...</td>\n",
       "      <td>5.243298</td>\n",
       "      <td>6.283092</td>\n",
       "      <td>56115.100219</td>\n",
       "      <td>57592.698860</td>\n",
       "      <td>16.106978</td>\n",
       "      <td>23.077325</td>\n",
       "      <td>731718.332090</td>\n",
       "      <td>748286.961169</td>\n",
       "      <td>1.033588e+02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           system        type      weight           2           3           4  \\\n",
       "count  718.000000  718.000000  718.000000  718.000000  718.000000  718.000000   \n",
       "mean    24.314763    3.749304    1.981686   -1.649589   -1.817170    7.981259   \n",
       "std     13.023788    0.662688    2.337013    4.548304    5.005473   21.558408   \n",
       "min      1.000000    2.000000    0.000000  -19.744040  -21.893983   -0.754568   \n",
       "25%     13.000000    4.000000    0.155340   -0.819357   -1.048764    0.000000   \n",
       "50%     25.000000    4.000000    1.000100    0.000000    0.000000    0.923914   \n",
       "75%     36.000000    4.000000    3.213367    0.795133    0.913984    1.408763   \n",
       "max     45.000000    4.000000    9.000000    1.239384    1.358098  122.931347   \n",
       "\n",
       "                5           6           7            8  ...            10  \\\n",
       "count  718.000000  718.000000  718.000000   718.000000  ...    718.000000   \n",
       "mean     8.445939  -34.541055  -36.169106   172.934603  ...   -752.288433   \n",
       "std     22.815235  111.039170  116.338079   613.372450  ...   2852.135166   \n",
       "min     -0.782633 -514.984097 -538.627792    -0.909456  ... -13321.170445   \n",
       "25%      0.000000   -0.924887   -0.941950     0.000000  ...     -1.122517   \n",
       "50%      0.935052    0.000000    0.000000     0.974117  ...      0.001407   \n",
       "75%      1.552803    0.960644    0.984305     2.341926  ...      0.991467   \n",
       "max    131.675490    2.275741    2.712998  2823.630938  ...      5.243298   \n",
       "\n",
       "                 11            12            13             14             15  \\\n",
       "count    718.000000    718.000000    718.000000     718.000000     718.000000   \n",
       "mean    -776.523313   3061.210057   3147.716636  -11562.460857  -11855.258680   \n",
       "std     2942.302665  11912.541567  12240.685674   45391.801105   46512.063723   \n",
       "min   -13781.246472     -8.850113    -12.265769 -211473.396816 -216475.644423   \n",
       "25%       -2.123912      0.001518      0.001679      -3.214377      -6.185564   \n",
       "50%        0.001712      0.987326      0.987066       0.001634       0.004821   \n",
       "75%        0.996329      4.397037      6.811820       0.997875       1.001248   \n",
       "max        6.283092  56115.100219  57592.698860      16.106978      23.077325   \n",
       "\n",
       "                  16             17            18         exp  \n",
       "count     718.000000     718.000000  7.180000e+02  718.000000  \n",
       "mean    40629.386999   41566.669584 -1.334843e+05    0.541783  \n",
       "std    160087.941813  163698.216993  5.278422e+05    0.706857  \n",
       "min       -44.356923     -66.596211 -2.489024e+06   -1.000000  \n",
       "25%         0.045728       0.042314 -2.569285e+01    0.000000  \n",
       "50%         1.004445       1.001084  9.732395e-02    1.000000  \n",
       "75%        16.098058      16.131990  1.003658e+00    1.000000  \n",
       "max    731718.332090  748286.961169  1.033588e+02    1.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe() #-------- this is a meaningful representation because we do not have categorical or object-like columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a comment we must notice that in same cases there are _minimum_ and _maximum_ values which look like outliers (their absolute value seems to be too large). For the moment we will keep them in the dataset.\n",
    "\n",
    "We save the dataset for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf(path.join(ROOT_DIR, 'data_sft_analysis.h5'), key='data_sft', complevel=9, complib='bzip2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set Preparation and Validation Strategy\n",
    "\n",
    "In this case the division into training and test set can be a bit tricky. In fact we want to separate the samples according to their reference `system` in order to keep entries coming from the same \"family\" together. We will then keep 25% of the `system` values as test set, without regards to the effective number of samples in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "system_train, system_test = train_test_split(df['system'].unique(), test_size=0.25, shuffle=True, random_state=RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then form the training and test sets using `system_train` and `system_test` as index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:38:02,357: DEBUG ==> Length of the training dataset: 519 samples.\n",
      "2020-05-11 15:38:02,361: DEBUG ==> Length of the test dataset: 199 samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 72% of the dataset.\n",
      "Test data:     27% of the dataset.\n"
     ]
    }
   ],
   "source": [
    "df_train = df.loc[df['system'].isin(system_train)]\n",
    "log.debug('Length of the training dataset: {:d} samples.'.format(df_train.shape[0]))\n",
    "\n",
    "df_test  = df.loc[df['system'].isin(system_test)]\n",
    "log.debug('Length of the test dataset: {:d} samples.'.format(df_test.shape[0]))\n",
    "\n",
    "print('Training data: {:d}% of the dataset.'.format(int(100 * df_train.shape[0] / df.shape[0])))\n",
    "print('Test data:     {:d}% of the dataset.'.format(int(100 * df_test.shape[0] / df.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then experiment on the best validation split size for the analysis. We split the training set into an effective training set and a **holdout validation** set with several different sizes and use the **mean squared error** (MSE) as a metric to evaluate the algorithm against the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:38:06,211: DEBUG ==> Plot saved in ./img/rmse_train_size.pdf.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics      import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble     import RandomForestRegressor\n",
    "from mltools.libplot      import Plot\n",
    "\n",
    "# choose an estimator\n",
    "estimators  = [LinearRegression(),\n",
    "               RandomForestRegressor(n_estimators=50, n_jobs=-1, random_state=RAND)\n",
    "              ]\n",
    "\n",
    "# define the size of the train set we want to experiment\n",
    "train_sizes = np.linspace(30, 90,\n",
    "                          num=11,\n",
    "                          dtype=np.int\n",
    "                         ).reshape(1,-1) / 100.0 #------------------------------------------------- training sizes\n",
    "rmse_values  = np.zeros((np.shape(estimators)[0], train_sizes.shape[1])) #------------------------- mse values (shape: n_estimators x train_sizes)\n",
    "\n",
    "# iterate over the training set sizes\n",
    "for n in range(train_sizes.shape[1]):\n",
    "    # split the set and select the features\n",
    "    train, val = train_test_split(df_train.drop(columns='system'), #------------------------------- drop the system column\n",
    "                                  train_size=train_sizes[:,n].reshape(-1,),\n",
    "                                  shuffle=True,\n",
    "                                  random_state=RAND\n",
    "                                 ) #--------------------------------------------------------------- split the training set\n",
    "    train_features = train.drop(columns='exp') #--------------------------------------------------- training features\n",
    "    val_features   = val.drop(columns='exp') #----------------------------------------------------- validation features\n",
    "\n",
    "    train_label    = train['exp'] #---------------------------------------------------------------- training label\n",
    "    val_label      = val['exp'] #------------------------------------------------------------------ validation label\n",
    "\n",
    "    # evaluate the algorithm\n",
    "    for m in range(np.shape(estimators)[0]):\n",
    "        estimators[m].fit(train_features, train_label) #------------------------------------------- fit the estimator_1\n",
    "        rmse_values[m,n] = np.sqrt(mean_squared_error(y_true=val_label,\n",
    "                                                      y_pred=estimators[m].predict(val_features)\n",
    "                                                     )\n",
    "                                  )#--------------------------------------------------------------- store the values of the MSE\n",
    "\n",
    "# plot the MSE values\n",
    "plot = Plot(rows=1, columns=np.shape(estimators)[0])\n",
    "\n",
    "for m in range(np.shape(estimators)[0]):\n",
    "    plot.series2D(data=rmse_values[m,:],\n",
    "                  axis=m,\n",
    "                  title='Validation Error as Function of the Training Set Size',\n",
    "                  xlabel='Train Set Size',\n",
    "                  ylabel='RMSE',\n",
    "                  legend=estimators[m].__class__.__name__,\n",
    "                  labels=train_sizes.reshape(-1,)\n",
    "                 )\n",
    "\n",
    "plot.save_and_close(path.join(IMG_PATH, 'rmse_train_size'))\n",
    "log.debug('Plot saved in {}.'.format(path.join(IMG_PATH, 'rmse_train_size.pdf')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot we show the progression of the MSE as a function of the training size. A good compromise between the no. of samples in the training set and the MSE of different kinds of algorithms seems to be around 60% to 70% of the dataset.\n",
    "\n",
    "We will therefore divide the original training set as follows:\n",
    "\n",
    "- 27% into a **test set** (already split),\n",
    "- 25% into **cross-validation** splits (around 34% of the training dataset),\n",
    "- 48% into the effective **training set** (around 66% of the training dataset).\n",
    "\n",
    "When we implement cross-validation in _Scikit_ we will therefore use 3 splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Analysis\n",
    "\n",
    "We now move to the ML analysis of the dataset. Given the size of the dataframe, we will try different approaches to the extrapolation of the level-$\\infty$ predictions. Specifically we will look at:\n",
    "\n",
    "- [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to have a solid baseline for comparison,\n",
    "- [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) to implement **L1** and **L2** regularizations,\n",
    "- [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) to implement **L2** regularization,\n",
    "- [Linear SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html) to use **support vector machines** to improve the predictions,\n",
    "- [SVR (Gaussian kernel)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) to hopefully find better results,\n",
    "- [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) to use the power of **decision trees** and **ensemble** learning,\n",
    "- [Histogram Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html) to improve the predictive abilities of a single decision tree through successive improvement.\n",
    "\n",
    "For the hyperparameter optimization we use a [**Bayesan** approach](https://en.wikipedia.org/wiki/Bayesian_optimization) in the [_Scikit-optimize_](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html) library: this will provide a better approach to the minimization of the cost functions during cross-validation with respect to a randomized search (apart from the linear regression where we will test all possible values of the hyperparameters). In order to print the output of the parameters dictionaries we implement a function to pretty print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(dct, indent=True):\n",
    "    '''\n",
    "    Pretty print the dictionary of best parameters.\n",
    "    \n",
    "    Required argument:\n",
    "        dct:    the dictionary to pretty print.\n",
    "        \n",
    "    Optional argument:\n",
    "        indent: whether to indent the printed output.\n",
    "    '''\n",
    "    \n",
    "    for key, value in dct.items():\n",
    "        if indent:\n",
    "            print('    {} = {}'.format(key, value))\n",
    "        else:\n",
    "            print('{} = {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Labels Extraction\n",
    "\n",
    "We then extract the training features and the labels. If needed we can implement the scaling of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "# drop the `system` column\n",
    "df_train = df_train.drop(columns='system')\n",
    "df_test  = df_test.drop(columns='system')\n",
    "\n",
    "# features\n",
    "features_train = df_train.drop(columns='exp')\n",
    "features_test  = df_test.drop(columns='exp')\n",
    "\n",
    "# scale the features\n",
    "features_scaler = StandardScaler()\n",
    "\n",
    "if features_scaler is not None:\n",
    "    features_train = features_scaler.fit_transform(features_train)\n",
    "    features_test  = features_scaler.transform(features_test)\n",
    "\n",
    "# labels\n",
    "labels_train = df_train['exp']\n",
    "labels_test  = df_test['exp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "We first consider a linear regression algorithm. In this case the number of hyperparameters to be tuned is small and we can use a **grid search** to try out every combination:\n",
    "\n",
    "- `fit_intercept` $\\in \\lbrace 0, 1 \\rbrace$,\n",
    "- `normalize` $\\in \\lbrace 0, 1 \\rbrace$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:38:06,369: INFO ==> LINEAR REGRESSION\n",
      "2020-05-11 15:38:06,371: INFO ==> Fitting the estimator...\n",
      "2020-05-11 15:38:08,308: INFO ==> Evaluating the estimator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "\n",
      "    fit_intercept = 1\n",
      "    normalize = 1\n",
      "\n",
      "RMSE on the validation set: 0.463 ± 0.159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "from mltools.libscore        import ViewCV\n",
    "from mltools.libplot         import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params  = {'fit_intercept': [0, 1],\n",
    "                  'normalize':     [0, 1]\n",
    "                 } #----------------------------------------------------------------------------- define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "log.info('LINEAR REGRESSION')\n",
    "lin_reg = GridSearchCV(estimator=LinearRegression(),\n",
    "                       param_grid=search_params,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       n_jobs=-1,\n",
    "                       refit=True,\n",
    "                       cv=cv\n",
    "                      ) #----------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the estimator...')\n",
    "lin_reg.fit(features_train, labels_train)\n",
    "\n",
    "# evaluate the estimator\n",
    "log.info('Evaluating the estimator...')\n",
    "cv_results = ViewCV(lin_reg)\n",
    "\n",
    "print('\\nBest parameters:\\n')\n",
    "pretty(cv_results.best_parameters)\n",
    "\n",
    "print('\\nRMSE on the validation set: {:.3f} ± {:.3f}'.format(np.sqrt(-cv_results.test_mean()),\n",
    "                                                             np.sqrt(cv_results.test_std())\n",
    "                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n",
    "\n",
    "We then try to implement the regularized version of the simple linear regression: we use an **elastic net** because it implements both **L1** and **L2** regularizations and can be a guide in choosing the appropriate values. In fact the hyperparameters in this case allow to separately control the two different regularizations. The additional term (with respect to the linear regression) to the cost function is of the form $\\Delta J(w) = \\alpha l_{1\\,ratio} \\vert\\vert w \\vert\\vert + \\frac{\\alpha}{2} (1 - l_{1\\,ratio}) \\vert\\vert w \\vert\\vert^2$. We will then study the following hyperparamter space:\n",
    "\n",
    "- `fit_intercept` $\\in \\lbrace 0, 1 \\rbrace$,\n",
    "- `normalize` $\\in \\lbrace 0, 1 \\rbrace$,\n",
    "- `alpha` $\\in \\left[ 10^{-2}, 10^2 \\right]$,\n",
    "- `l1_ratio` $\\in \\left[ 0.0, 1.0 \\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:38:08,424: INFO ==> ELASTIC NET\n",
      "2020-05-11 15:38:08,425: INFO ==> Fitting the estimator...\n",
      "2020-05-11 15:40:46,551: INFO ==> Evaluating the estimator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "\n",
      "    alpha = 0.01\n",
      "    fit_intercept = 1\n",
      "    l1_ratio = 0.0\n",
      "    normalize = 0\n",
      "\n",
      "RMSE on the validation set: 0.579 ± 0.272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from skopt                import BayesSearchCV\n",
    "from skopt.space          import Real, Integer, Categorical\n",
    "from sklearn.metrics      import mean_squared_error\n",
    "from mltools.libscore     import ViewCV\n",
    "from mltools.libplot      import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params  = {'fit_intercept': Integer(0, 1),\n",
    "                  'normalize':     Integer(0, 1),\n",
    "                  'alpha':         Real(1.0e-2, 1.0e2, prior='log-uniform'),\n",
    "                  'l1_ratio':      Real(0.0, 1.0, prior='uniform')\n",
    "                 } #--------------------------------------------------------------------------- define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "log.info('ELASTIC NET')\n",
    "el_net = BayesSearchCV(estimator=ElasticNet(max_iter=1e5, random_state=RAND),\n",
    "                       search_spaces=search_params,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       n_jobs=-1,\n",
    "                       refit=True,\n",
    "                       n_iter=50,\n",
    "                       random_state=RAND,\n",
    "                       cv=cv\n",
    "                      ) #---------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the estimator...')\n",
    "el_net.fit(features_train, labels_train)\n",
    "\n",
    "# evaluate the estimator\n",
    "log.info('Evaluating the estimator...')\n",
    "cv_results = ViewCV(el_net)\n",
    "\n",
    "print('\\nBest parameters:\\n')\n",
    "pretty(cv_results.best_parameters)\n",
    "\n",
    "print('\\nRMSE on the validation set: {:.3f} ± {:.3f}'.format(np.sqrt(-cv_results.test_mean()),\n",
    "                                                             np.sqrt(cv_results.test_std())\n",
    "                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the values of $\\alpha$ and $l_{1~ratio}$ seem to suggest the need for **L2** regularization procided by the **Ridge** regression which we implement as the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "\n",
    "We try to implement the pure **L2** regularized linear model. We explore the hyperparameter space:\n",
    "\n",
    "- `fit_intercept` $\\in \\lbrace 0, 1 \\rbrace$,\n",
    "- `normalize` $\\in \\lbrace 0, 1 \\rbrace$,\n",
    "- `alpha` $\\in \\left[ 10^{-2}, 10^2 \\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:40:46,594: INFO ==> RIDGE\n",
      "2020-05-11 15:40:46,597: INFO ==> Fitting the estimator...\n",
      "2020-05-11 15:42:30,419: INFO ==> Evaluating the estimator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "\n",
      "    alpha = 0.01\n",
      "    fit_intercept = 1\n",
      "    normalize = 0\n",
      "\n",
      "RMSE on the validation set: 0.531 ± 0.236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from skopt                import BayesSearchCV\n",
    "from skopt.space          import Real, Integer, Categorical\n",
    "from sklearn.metrics      import mean_squared_error\n",
    "from mltools.libscore     import ViewCV\n",
    "from mltools.libplot      import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params  = {'fit_intercept': Integer(0, 1),\n",
    "                  'normalize':     Integer(0, 1),\n",
    "                  'alpha':         Real(1.0e-2, 1.0e2, prior='log-uniform')\n",
    "                 } #--------------------------------------------------------------------------- define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "log.info('RIDGE')\n",
    "ridge = BayesSearchCV(estimator=Ridge(random_state=RAND),\n",
    "                      search_spaces=search_params,\n",
    "                      scoring='neg_mean_squared_error',\n",
    "                      n_jobs=-1,\n",
    "                      refit=True,\n",
    "                      n_iter=50,\n",
    "                      random_state=RAND,\n",
    "                      cv=cv\n",
    "                     ) #---------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the estimator...')\n",
    "ridge.fit(features_train, labels_train)\n",
    "\n",
    "# evaluate the estimator\n",
    "log.info('Evaluating the estimator...')\n",
    "cv_results = ViewCV(ridge)\n",
    "\n",
    "print('\\nBest parameters:\\n')\n",
    "pretty(cv_results.best_parameters)\n",
    "\n",
    "print('\\nRMSE on the validation set: {:.3f} ± {:.3f}'.format(np.sqrt(-cv_results.test_mean()),\n",
    "                                                             np.sqrt(cv_results.test_std())\n",
    "                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Linear SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html)\n",
    "\n",
    "We move to the **support vector machines** (SVM) algorithms and start without a **kernel** function. We use the linear implementation of the support vector regressor with hyperparameters:\n",
    "\n",
    "- `fit_intercept` $\\in \\lbrace 0, 1 \\rbrace$,\n",
    "- `epsilon` $\\in \\left[ 0.0, 1.0 \\right]$,\n",
    "- `C` $\\in \\left[ 10^{-1}, 10^3 \\right]$,\n",
    "- `loss` $\\in \\lbrace epsilon\\_insensitive, squared\\_epsilon\\_insensitive \\rbrace$,\n",
    "- `intercept_scaling` $\\in \\left[ 10^{-2}, 10^2 \\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:42:30,470: INFO ==> LINEAR SVR\n",
      "2020-05-11 15:42:30,472: INFO ==> Fitting the estimator...\n",
      "2020-05-11 15:51:24,043: INFO ==> Evaluating the estimator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "\n",
      "    C = 291.0760784596301\n",
      "    epsilon = 0.20475887943766544\n",
      "    fit_intercept = 1\n",
      "    intercept_scaling = 5.8001415263948015\n",
      "    loss = squared_epsilon_insensitive\n",
      "\n",
      "RMSE on the validation set: 0.525 ± 0.192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm      import LinearSVR\n",
    "from skopt            import BayesSearchCV\n",
    "from skopt.space      import Real, Integer, Categorical\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from mltools.libscore import ViewCV\n",
    "from mltools.libplot  import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params  = {'fit_intercept':     Integer(0, 1),\n",
    "                  'epsilon':           Real(0.0, 1.0,  prior='uniform'),\n",
    "                  'C':                 Real(1.0e-1, 1.0e3, prior='log-uniform'),\n",
    "                  'intercept_scaling': Real(1.0e-2, 1.0e2, prior='log-uniform'),\n",
    "                  'loss':              Categorical(['squared_epsilon_insensitive', 'epsilon_insensitive'])\n",
    "                 } #----------------------------------------------------------------------------------- define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "log.info('LINEAR SVR')\n",
    "lin_svr = BayesSearchCV(LinearSVR(max_iter=1e5, random_state=RAND),\n",
    "                        search_spaces=search_params,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        n_jobs=-1,\n",
    "                        refit=True,\n",
    "                        n_iter=50,\n",
    "                        random_state=RAND,\n",
    "                        cv=cv\n",
    "                       ) #---------------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the estimator...')\n",
    "lin_svr.fit(features_train, labels_train)\n",
    "\n",
    "# evaluate the estimator\n",
    "log.info('Evaluating the estimator...')\n",
    "cv_results = ViewCV(lin_svr)\n",
    "\n",
    "print('\\nBest parameters:\\n')\n",
    "pretty(cv_results.best_parameters)\n",
    "\n",
    "print('\\nRMSE on the validation set: {:.3f} ± {:.3f}'.format(np.sqrt(-cv_results.test_mean()),\n",
    "                                                             np.sqrt(cv_results.test_std())\n",
    "                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) (Gaussian Kernel)\n",
    "\n",
    "We now implement a kernel function in the SVM. In particular we use the Gaussian kernel (_rbf_) and try to approximate the level-$\\infty$ predictions using these hyperparameters:\n",
    "\n",
    "- `gamma` $\\in \\left[ 10^{-1}, 10^1 \\right]$,\n",
    "- `C` $\\in \\left[ 10^{-1}, 5 \\times 10^2 \\right]$,\n",
    "- `epsilon` $\\in \\left[ 10^{-4}, 10^{-1} \\right]$,\n",
    "- `shrinking` $\\in \\lbrace 0, 1 \\rbrace$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:51:24,071: INFO ==> SVR\n",
      "2020-05-11 15:51:24,074: INFO ==> Fitting the estimator...\n",
      "2020-05-11 15:55:16,622: INFO ==> Evaluating the estimator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "\n",
      "    C = 499.99999999999994\n",
      "    epsilon = 0.0001\n",
      "    gamma = 1.8158720638335621\n",
      "    shrinking = 1\n",
      "\n",
      "RMSE on the validation set: 0.200 ± 0.124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm      import SVR\n",
    "from skopt            import BayesSearchCV\n",
    "from skopt.space      import Real, Integer, Categorical\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from mltools.libscore import ViewCV\n",
    "from mltools.libplot  import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params  = {'epsilon':   Real(1.0e-4, 1.0e-1, prior='log-uniform'),\n",
    "                  'C':         Real(1.0e-1, 5.0e2,  prior='log-uniform'),\n",
    "                  'gamma':     Real(1.0e-1, 1.0e1,  prior='log-uniform'),\n",
    "                  'shrinking': Integer(0, 1)\n",
    "                 } #----------------------------------------------------------------------------------- define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "log.info('SVR')\n",
    "svr = BayesSearchCV(SVR(kernel='rbf'),\n",
    "                    search_spaces=search_params,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    n_jobs=-1,\n",
    "                    refit=True,\n",
    "                    n_iter=50,\n",
    "                    random_state=RAND,\n",
    "                    cv=cv\n",
    "                   ) #-------------------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the estimator...')\n",
    "svr.fit(features_train, labels_train)\n",
    "\n",
    "# evaluate the estimator\n",
    "log.info('Evaluating the estimator...')\n",
    "cv_results = ViewCV(svr)\n",
    "\n",
    "print('\\nBest parameters:\\n')\n",
    "pretty(cv_results.best_parameters)\n",
    "\n",
    "print('\\nRMSE on the validation set: {:.3f} ± {:.3f}'.format(np.sqrt(-cv_results.test_mean()),\n",
    "                                                             np.sqrt(cv_results.test_std())\n",
    "                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "\n",
    "Moving on to the **decision tree** family of algorithms, we implement the **random forest** regression in the _Scikit_ implementation. We will try to intervene on as many hyperparameters as possible both to avoid overfitting the training set and to be able to generalise to new results. We use the following hyperparameters:\n",
    "\n",
    "- `n_estimators` $\\in \\left[ 10, 300 \\right]$,\n",
    "- `criterion` $\\in \\lbrace mse, mae \\rbrace$,\n",
    "- `max_depth` $\\in \\left[ 2, 100 \\right]$,\n",
    "- `min_samples_split` $\\in \\left[ 2, 100 \\right]$,\n",
    "- `min_samples_leaf` $\\in \\left[ 1, 100 \\right]$,\n",
    "- `min_weight_fraction_leaf` $\\in \\left[ 0, 0.5\\right]$,\n",
    "- `max_leaf_nodes` $\\in \\left[ 2, 100 \\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:55:16,657: INFO ==> RANDOM FOREST\n",
      "2020-05-11 15:55:16,659: INFO ==> Fitting the estimator...\n",
      "2020-05-11 15:56:29,173: INFO ==> Evaluating the estimator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "\n",
      "    criterion = mse\n",
      "    max_depth = 32\n",
      "    max_leaf_nodes = 18\n",
      "    min_samples_leaf = 1\n",
      "    min_samples_split = 14\n",
      "    min_weight_fraction_leaf = 0.0\n",
      "    n_estimators = 170\n",
      "\n",
      "RMSE on the validation set: 0.166 ± 0.060\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from skopt            import BayesSearchCV\n",
    "from skopt.space      import Real, Integer, Categorical\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from mltools.libscore import ViewCV\n",
    "from mltools.libplot  import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params = {'n_estimators':             Integer(1.0e1, 3.0e2, prior='log-uniform'),\n",
    "                 'max_depth':                Integer(2, 100, prior='uniform'),\n",
    "                 'min_samples_split':        Integer(2, 100, prior='uniform'),\n",
    "                 'min_samples_leaf':         Integer(1, 100, prior='uniform'),\n",
    "                 'max_leaf_nodes':           Integer(2, 100, prior='uniform'),\n",
    "                 'min_weight_fraction_leaf': Real(0.0, 0.5, prior='uniform'),\n",
    "                 'criterion':                Categorical(['mse', 'mae'])\n",
    "                } #------------------------------------------------------------------------------------ define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "log.info('RANDOM FOREST')\n",
    "rnd_for = BayesSearchCV(RandomForestRegressor(n_jobs=-1, random_state=RAND),\n",
    "                        search_spaces=search_params,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        n_jobs=-1,\n",
    "                        refit=True,\n",
    "                        n_iter=25,\n",
    "                        random_state=RAND,\n",
    "                        cv=cv\n",
    "                       ) #---------------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the estimator...')\n",
    "rnd_for.fit(features_train, labels_train)\n",
    "\n",
    "# evaluate the estimator\n",
    "log.info('Evaluating the estimator...')\n",
    "cv_results = ViewCV(rnd_for)\n",
    "\n",
    "print('\\nBest parameters:\\n')\n",
    "pretty(cv_results.best_parameters)\n",
    "\n",
    "print('\\nRMSE on the validation set: {:.3f} ± {:.3f}'.format(np.sqrt(-cv_results.test_mean()),\n",
    "                                                             np.sqrt(cv_results.test_std())\n",
    "                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Histogram Based Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)\n",
    "\n",
    "We consider the new (still _experimental_) version of _Scikit_ **gradient boosting** algorithm, which is **histogram bases**. This should automatically handle _NaN_ gradients and be quite faster than the usual implementation. We will consider the followin hyperparameters:\n",
    "\n",
    "- `loss` $\\in \\lbrace least\\_squares, least\\_absolute\\_deviation \\rbrace$,\n",
    "- `learning_rate` $\\in \\left[ 10^{-4}, 10^{-1} \\right]$,\n",
    "- `max_iter` $\\in \\left[ 10, 300 \\right]$,\n",
    "- `max_depth` $\\in \\left[ 2, 100 \\right]$,\n",
    "- `min_samples_leaf` $\\in \\left[ 10, 100 \\right]$,\n",
    "- `l2_regularization` $\\in \\left[ 10^{-6}, 10^2 \\right]$,\n",
    "- `max_leaf_nodes` $\\in \\left[ 2, 50 \\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:56:29,209: INFO ==> BOOSTED TREES\n",
      "2020-05-11 15:56:29,213: INFO ==> Fitting the estimator...\n",
      "2020-05-11 16:07:55,619: INFO ==> Evaluating the estimator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "\n",
      "    l2_regularization = 0.00524404052618982\n",
      "    learning_rate = 0.05234588205580838\n",
      "    loss = least_squares\n",
      "    max_depth = 41\n",
      "    max_iter = 92\n",
      "    max_leaf_nodes = 16\n",
      "    min_samples_leaf = 10\n",
      "\n",
      "RMSE on the validation set: 0.139 ± 0.072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble     import HistGradientBoostingRegressor\n",
    "from skopt                import BayesSearchCV\n",
    "from skopt.space          import Real, Integer, Categorical\n",
    "from sklearn.metrics      import mean_squared_error\n",
    "from mltools.libscore     import ViewCV\n",
    "from mltools.libplot      import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params = {'max_iter':          Integer(1.0e1, 3.0e2, prior='log-uniform'),\n",
    "                 'max_depth':         Integer(2, 100,  prior='uniform'),\n",
    "                 'max_leaf_nodes':    Integer(2, 50,   prior='uniform'),\n",
    "                 'min_samples_leaf':  Integer(10, 100, prior='uniform'),\n",
    "                 'learning_rate':     Real(1.0e-4, 1.0e-1, prior='log-uniform'),\n",
    "                 'l2_regularization': Real(1.0e-6, 1.0e2,  prior='log-uniform'),\n",
    "                 'loss':              Categorical(['least_squares', 'least_absolute_deviation'])\n",
    "                } #------------------------------------------------------------------------------------- define the hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "log.info('BOOSTED TREES')\n",
    "grd_boost = BayesSearchCV(HistGradientBoostingRegressor(scoring='loss', validation_fraction=None, n_iter_no_change=10, random_state=RAND),\n",
    "                          search_spaces=search_params,\n",
    "                          scoring='neg_mean_squared_error',\n",
    "                          n_jobs=-1,\n",
    "                          refit=True,\n",
    "                          n_iter=25,\n",
    "                          random_state=RAND,\n",
    "                          cv=cv\n",
    "                         ) #--------------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the estimator...')\n",
    "grd_boost.fit(features_train, labels_train)\n",
    "\n",
    "# evaluate the estimator\n",
    "log.info('Evaluating the estimator...')\n",
    "cv_results = ViewCV(grd_boost)\n",
    "\n",
    "print('\\nBest parameters:\\n')\n",
    "pretty(cv_results.best_parameters)\n",
    "\n",
    "print('\\nRMSE on the validation set: {:.3f} ± {:.3f}'.format(np.sqrt(-cv_results.test_mean()),\n",
    "                                                             np.sqrt(cv_results.test_std())\n",
    "                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and Test Set Performance\n",
    "\n",
    "It seems that in general the **linear models** perform at best when **no regularization** is involved, thus the simple **linear regression** model represents a very good baseline for improvement. The **support vector machines** perform better in the presence of a kernel function (**Gaussian** in this case) and improve the results of the linear regression. In principle also the **random forest** regression is a very good prediction algorithm in this case even though it is strongly influenced by the presence of the initial values in the dataset. However, in the family of decision trees, the **boosted trees** vastly outperform the random forest, especially when the initial values are not present. \n",
    "\n",
    "Notice that we applied a scaler at the beginning in order to improve the convergence of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of the linear regression on the test set: 0.376\n",
      "RMSE of the elastic net on the test set:       0.522\n",
      "RMSE of the ridge on the test set:             0.454\n",
      "RMSE of the linear svr on the test set:        0.479\n",
      "RMSE of the svr on the test set:               0.099\n",
      "RMSE of the random forest on the test set:     0.035\n",
      "RMSE of the boosted trees on the test set:     0.025\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('RMSE of the linear regression on the test set: {:.3f}'.format(np.sqrt(mean_squared_error(y_true=labels_test, y_pred=lin_reg.best_estimator_.predict(features_test)))))\n",
    "print('RMSE of the elastic net on the test set:       {:.3f}'.format(np.sqrt(mean_squared_error(y_true=labels_test, y_pred=el_net.best_estimator_.predict(features_test)))))\n",
    "print('RMSE of the ridge on the test set:             {:.3f}'.format(np.sqrt(mean_squared_error(y_true=labels_test, y_pred=ridge.best_estimator_.predict(features_test)))))\n",
    "print('RMSE of the linear svr on the test set:        {:.3f}'.format(np.sqrt(mean_squared_error(y_true=labels_test, y_pred=lin_svr.best_estimator_.predict(features_test)))))\n",
    "print('RMSE of the svr on the test set:               {:.3f}'.format(np.sqrt(mean_squared_error(y_true=labels_test, y_pred=svr.best_estimator_.predict(features_test)))))\n",
    "print('RMSE of the random forest on the test set:     {:.3f}'.format(np.sqrt(mean_squared_error(y_true=labels_test, y_pred=rnd_for.best_estimator_.predict(features_test)))))\n",
    "print('RMSE of the boosted trees on the test set:     {:.3f}'.format(np.sqrt(mean_squared_error(y_true=labels_test, y_pred=grd_boost.best_estimator_.predict(features_test)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the predictions to a JSON file for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 16:08:08,965: DEBUG ==> Predictions saved to ./output/sft_level_trunc_predictions.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "predictions = {'labels': labels_test.values.tolist(),\n",
    "               'lin_reg': lin_reg.predict(features_test).tolist(),\n",
    "               'el_net':  el_net.predict(features_test).tolist(),\n",
    "               'ridge':   ridge.predict(features_test).tolist(),\n",
    "               'lin_svr': lin_svr.predict(features_test).tolist(),\n",
    "               'svr_rbf': svr.predict(features_test).tolist(),\n",
    "               'rnd_for': rnd_for.predict(features_test).tolist(),\n",
    "               'grd_boost': grd_boost.predict(features_test).tolist(),\n",
    "              }\n",
    "\n",
    "with open(path.join(OUT_PATH, 'sft_level_trunc_predictions.json'), 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "    \n",
    "log.debug('Predictions saved to {}'.format(path.join(OUT_PATH, 'sft_level_trunc_predictions.json')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
