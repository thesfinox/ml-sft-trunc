{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Level Truncation in String Field Theory\n",
    "\n",
    "Consider the data of lumps in bosonic String Field Theory (SFT) and extrapolate level-$\\infty$ predictions from finite level data.\n",
    "\n",
    "In this notebook we use **stacking ensemble** learning to improve the predictions: we use two levels of learning algorithms splitting the training set into two subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First of all we print the characteristics of the current setup (OS, cores, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current OS:                  Linux (kernel release: 5.6.11-arch1-1, architecture: x86_64)\n",
      "Number of available threads: 8\n",
      "Current CPU frequency:       2991 MHz (max: 3800 MHz)\n",
      "Available RAM memory:        7761 MB (tot: 15758 MB)\n"
     ]
    }
   ],
   "source": [
    "from mltools.libos import InfoOS\n",
    "\n",
    "print('Current OS:                  {} (kernel release: {}, architecture: {})'.format(InfoOS().os, InfoOS().kernel, InfoOS().arch))\n",
    "print('Number of available threads: {:d}'.format(InfoOS().threads))\n",
    "print('Current CPU frequency:       {:.0f} MHz (max: {:.0f} MHz)'.format(InfoOS().freq, InfoOS().freqm))\n",
    "print('Available RAM memory:        {:d} MB (tot: {:d} MB)'.format(InfoOS().vmav, InfoOS().vmtot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We establish early in the notebook the amount of cores we want to use in order to parallelize computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = int(InfoOS().threads / 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check the installed versions of the packages we are going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7\n",
      "Matplot version: 3.2.1\n",
      "Numpy version: 1.18.4\n",
      "Pandas version: 1.0.3\n",
      "Scikit-learn version: 0.22.2.post1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import random     as rnd\n",
    "import sklearn    as skl\n",
    "import numpy      as np\n",
    "import pandas     as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning) # ignore user warnings: nothing that I can really do anything about it...\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=12)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# print the version of the modules\n",
    "print('Python version: {:d}.{:d}'      .format(sys.version_info.major, sys.version_info.minor))\n",
    "print('Matplot version: {}'            .format(mpl.__version__))\n",
    "print('Numpy version: {}'              .format(np.__version__))\n",
    "print('Pandas version: {}'             .format(pd.__version__))\n",
    "print('Scikit-learn version: {}'       .format(skl.__version__))\n",
    "\n",
    "# fix random_seed\n",
    "RAND = 42\n",
    "rnd.seed(RAND)\n",
    "np.random.seed(RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Preparation\n",
    "\n",
    "In order to save the results of the analysis, we need to create the structure of directories in the current repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "\n",
    "ROOT_DIR = '.' #-------------------------------------------------- root directory\n",
    "IMG_DIR  = 'img' #------------------------------------------------ directory of images\n",
    "MOD_DIR  = 'models' #--------------------------------------------- directory of saved models\n",
    "LOG_DIR  = 'log' #------------------------------------------------ directory of logs\n",
    "OUT_DIR  = 'output' #--------------------------------------------- directory for saved predictions, relevant output, etc.\n",
    "\n",
    "DB_NAME = 'data_sft_analysis' #----------------------------------- name of the dataset\n",
    "DB_FILE = DB_NAME + '.h5' #--------------------------------------- full name with extension\n",
    "DB_PATH = path.join(ROOT_DIR, DB_FILE) #-------------------------- full path of the dataset\n",
    "\n",
    "# define full paths\n",
    "IMG_PATH = path.join(ROOT_DIR, IMG_DIR)\n",
    "MOD_PATH = path.join(ROOT_DIR, MOD_DIR)\n",
    "LOG_PATH = path.join(ROOT_DIR, LOG_DIR)\n",
    "OUT_PATH = path.join(ROOT_DIR, OUT_DIR)\n",
    "\n",
    "# create directories if non existent\n",
    "if not path.isdir(IMG_PATH):\n",
    "    makedirs(IMG_PATH, exist_ok=True)\n",
    "if not path.isdir(MOD_PATH):\n",
    "    makedirs(MOD_PATH, exist_ok=True)\n",
    "if not path.isdir(LOG_PATH):\n",
    "    makedirs(LOG_PATH, exist_ok=True)\n",
    "if not path.isdir(OUT_PATH):\n",
    "    makedirs(OUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a logging session to store debug information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotating existing logs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:41:22,316: INFO ==> New logging session started. Log is at ./log/data_sft_analysis_stacking.log.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from mltools.liblog import create_logfile\n",
    "\n",
    "path_to_log = path.join(LOG_PATH,\n",
    "                        DB_NAME + '_stacking.log'\n",
    "                       ) #----------------------------------------------------- log path\n",
    "log = create_logfile(path_to_log,\n",
    "                     name=DB_NAME,\n",
    "                     level=logging.DEBUG\n",
    "                    ) #-------------------------------------------------------- create log file and session\n",
    "\n",
    "# these lines provide the same setup also for the Jupyter logging\n",
    "logger = logging.getLogger() #------------------------------------------------- get the current logging session\n",
    "\n",
    "fmt = logging.Formatter('%(asctime)s: %(levelname)s ==> %(message)s') #-------- customise the formatting options\n",
    "\n",
    "handler = logging.StreamHandler() #-------------------------------------------- handle the stream to the default (stderr)\n",
    "handler.setLevel(logging.DEBUG) #---------------------------------------------- print everything\n",
    "handler.setFormatter(fmt) #---------------------------------------------------- set the formatting options\n",
    "\n",
    "logger.handlers = [handler] #-------------------------------------------------- override the default stream\n",
    "\n",
    "# we are ready to go!\n",
    "log.info('New logging session started. Log is at {}.'.format(path_to_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Database\n",
    "\n",
    "We then import the database from its JSON format and begin to analyse it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:41:22,807: DEBUG ==> Database loaded.\n"
     ]
    }
   ],
   "source": [
    "if path.isfile(DB_PATH):\n",
    "    df = pd.read_hdf(DB_PATH)\n",
    "    log.debug('Database loaded.')\n",
    "else:\n",
    "    print('Database is not in the file tree!')\n",
    "    log.error('Cannot find database!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then show the `dtypes` of each column to get an idea of the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "system      int64\n",
       "type        int64\n",
       "weight    float64\n",
       "2         float64\n",
       "3         float64\n",
       "4         float64\n",
       "5         float64\n",
       "6         float64\n",
       "7         float64\n",
       "8         float64\n",
       "9         float64\n",
       "10        float64\n",
       "11        float64\n",
       "12        float64\n",
       "13        float64\n",
       "14        float64\n",
       "15        float64\n",
       "16        float64\n",
       "17        float64\n",
       "18        float64\n",
       "exp         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset we have the predictions of the position of lumps in bosonic SFT for finite levels in the numbered columns and the extrapolation for level-$\\infty$ in the column _exp_. We want to use known data (including the _weight_ and the _type_ of the input data) to predict the _exp_ labels (_init_ in principle can be left out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system</th>\n",
       "      <th>type</th>\n",
       "      <th>weight</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.169090</td>\n",
       "      <td>1.103953</td>\n",
       "      <td>1.049652</td>\n",
       "      <td>1.042422</td>\n",
       "      <td>1.025771</td>\n",
       "      <td>1.023925</td>\n",
       "      <td>1.016453</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011730</td>\n",
       "      <td>1.011495</td>\n",
       "      <td>1.008954</td>\n",
       "      <td>1.008871</td>\n",
       "      <td>1.007160</td>\n",
       "      <td>1.007141</td>\n",
       "      <td>1.005919</td>\n",
       "      <td>1.005930</td>\n",
       "      <td>1.005018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.133441</td>\n",
       "      <td>1.073850</td>\n",
       "      <td>1.041536</td>\n",
       "      <td>1.033556</td>\n",
       "      <td>1.022857</td>\n",
       "      <td>1.020416</td>\n",
       "      <td>1.015351</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011415</td>\n",
       "      <td>1.010882</td>\n",
       "      <td>1.009026</td>\n",
       "      <td>1.008720</td>\n",
       "      <td>1.007436</td>\n",
       "      <td>1.007245</td>\n",
       "      <td>1.006306</td>\n",
       "      <td>1.006180</td>\n",
       "      <td>1.005465</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.140716</td>\n",
       "      <td>1.076342</td>\n",
       "      <td>1.042333</td>\n",
       "      <td>1.034286</td>\n",
       "      <td>1.023112</td>\n",
       "      <td>1.020708</td>\n",
       "      <td>1.015447</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011446</td>\n",
       "      <td>1.010940</td>\n",
       "      <td>1.009026</td>\n",
       "      <td>1.008740</td>\n",
       "      <td>1.007419</td>\n",
       "      <td>1.007244</td>\n",
       "      <td>1.006281</td>\n",
       "      <td>1.006167</td>\n",
       "      <td>1.005435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   system  type  weight         2         3         4         5         6  \\\n",
       "0      40     2     0.0  1.169090  1.103953  1.049652  1.042422  1.025771   \n",
       "1      26     2     0.0  1.133441  1.073850  1.041536  1.033556  1.022857   \n",
       "2      27     2     0.0  1.140716  1.076342  1.042333  1.034286  1.023112   \n",
       "\n",
       "          7         8  ...        10        11        12        13        14  \\\n",
       "0  1.023925  1.016453  ...  1.011730  1.011495  1.008954  1.008871  1.007160   \n",
       "1  1.020416  1.015351  ...  1.011415  1.010882  1.009026  1.008720  1.007436   \n",
       "2  1.020708  1.015447  ...  1.011446  1.010940  1.009026  1.008740  1.007419   \n",
       "\n",
       "         15        16        17        18  exp  \n",
       "0  1.007141  1.005919  1.005930  1.005018    1  \n",
       "1  1.007245  1.006306  1.006180  1.005465    1  \n",
       "2  1.007244  1.006281  1.006167  1.005435    1  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set Prearation and Validation Strategy\n",
    "\n",
    "In this case the division into training and test set can be a bit tricky. In fact we want to separate the samples according to their reference `system` in order to keep entries coming from the same \"family\" together. We will then keep 20% of the `system` values as test set, with no regards to the effective number of samples in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "system_train, system_test = train_test_split(df['system'].unique(), test_size=0.20, shuffle=True, random_state=RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the training set into two subsets. In the first level set we keep around 70% of the remaining samples, while we keep only 30% of them in the second level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_train_lv1, system_train_lv2 = train_test_split(system_train, test_size=0.3, shuffle=True, random_state=RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples are therefore inserted in their sets as:\n",
    "\n",
    "- 20% in the test set,\n",
    "- 56% in the first level set (70% of the total training set),\n",
    "- 24% in the second level set (30% of the total training set).\n",
    "\n",
    "We then form the training and test sets using `system_train_lv1`, `system_train_lv2` and `system_test` as index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:41:23,020: DEBUG ==> Length of the lv1 training dataset: 398 samples.\n",
      "2020-05-11 15:41:23,026: DEBUG ==> Length of the lv2 training dataset: 171 samples.\n",
      "2020-05-11 15:41:23,030: DEBUG ==> Length of the test dataset: 149 samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data (lv1): 55% of the dataset.\n",
      "Training data (lv2): 23% of the dataset.\n",
      "Test data:           20% of the dataset.\n"
     ]
    }
   ],
   "source": [
    "df_train_lv1 = df.loc[df['system'].isin(system_train_lv1)]\n",
    "df_train_lv2 = df.loc[df['system'].isin(system_train_lv2)]\n",
    "log.debug('Length of the lv1 training dataset: {:d} samples.'.format(df_train_lv1.shape[0]))\n",
    "log.debug('Length of the lv2 training dataset: {:d} samples.'.format(df_train_lv2.shape[0]))\n",
    "\n",
    "df_test  = df.loc[df['system'].isin(system_test)]\n",
    "log.debug('Length of the test dataset: {:d} samples.'.format(df_test.shape[0]))\n",
    "\n",
    "print('Training data (lv1): {:d}% of the dataset.'.format(int(100 * df_train_lv1.shape[0] / df.shape[0])))\n",
    "print('Training data (lv2): {:d}% of the dataset.'.format(int(100 * df_train_lv2.shape[0] / df.shape[0])))\n",
    "print('Test data:           {:d}% of the dataset.'.format(int(100 * df_test.shape[0] / df.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the number of samples in each split has been reduced with respect to the previous analysis, we keep using **cross-validation** to score the algorithms. However we have to reduce the number of splits to produce a meaningful prediction. We will therefore divide the original training set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv_lv1 = KFold(n_splits=3, shuffle=True, random_state=RAND)\n",
    "cv_lv2 = KFold(n_splits=2, shuffle=True, random_state=RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Analysis\n",
    "\n",
    "We now move to the ML analysis of the dataset for the **first level** learning. We consider:\n",
    "\n",
    "- [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to have a solid baseline for comparison,\n",
    "- [SVR (Gaussian kernel)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) to hopefully find better results,\n",
    "- [Histogram Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html) to improve the predictive abilities of a single decision tree through successive improvement.\n",
    "\n",
    "For the hyperparameter optimization we use a [**Bayesan** approach](https://en.wikipedia.org/wiki/Bayesian_optimization) in the [_Scikit-optimize_](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html) library: this will provide a better approach to the minimization of the cost functions during cross-validation with respect to a randomized search (apart from the linear regression where we will test all possible values of the hyperparameters). In order to print the output of the parameters dictionaries we implement a function to pretty print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(dct, indent=True):\n",
    "    '''\n",
    "    Pretty print the dictionary of best parameters.\n",
    "    \n",
    "    Required argument:\n",
    "        dct:    the dictionary to pretty print.\n",
    "        \n",
    "    Optional argument:\n",
    "        indent: whether to indent the printed output.\n",
    "    '''\n",
    "    \n",
    "    for key, value in dct.items():\n",
    "        if indent:\n",
    "            print('    {} = {}'.format(key, value))\n",
    "        else:\n",
    "            print('{} = {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Labels Extraction\n",
    "\n",
    "We then extract the training features and the labels. If needed we can implement the scaling of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "# drop the `system` column\n",
    "df_train_lv1 = df_train_lv1.drop(columns='system')\n",
    "df_train_lv2 = df_train_lv2.drop(columns='system')\n",
    "df_test      = df_test.drop(columns='system')\n",
    "\n",
    "# features\n",
    "features_train_lv1 = df_train_lv1.drop(columns='exp')\n",
    "features_train_lv2 = df_train_lv2.drop(columns='exp')\n",
    "features_test      = df_test.drop(columns='exp')\n",
    "\n",
    "# labels\n",
    "labels_train_lv1 = df_train_lv1['exp']\n",
    "labels_train_lv2 = df_train_lv2['exp']\n",
    "labels_test      = df_test['exp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "We first consider a linear regression algorithm. In this case the number of hyperparameters to be tuned is small and we can use a **grid search** to try out every combination:\n",
    "\n",
    "- `fit_intercept` $\\in \\lbrace 0, 1 \\rbrace$,\n",
    "- `normalize` $\\in \\lbrace 0, 1 \\rbrace$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:41:23,243: INFO ==> LINEAR REGRESSION\n",
      "2020-05-11 15:41:23,245: INFO ==> Fitting the estimator...\n",
      "2020-05-11 15:41:26,068: INFO ==> Evaluating the estimator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "\n",
      "    fit_intercept = 1\n",
      "    normalize = 0\n",
      "\n",
      "RMSE on the validation set: 0.480 ± 0.282\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "from mltools.libscore        import ViewCV\n",
    "from mltools.libplot         import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params  = {'fit_intercept': [0, 1],\n",
    "                  'normalize':     [0, 1]\n",
    "                 } #----------------------------------------------------------------------------- define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "log.info('LINEAR REGRESSION')\n",
    "lin_reg = GridSearchCV(estimator=LinearRegression(),\n",
    "                       param_grid=search_params,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       n_jobs=-1,\n",
    "                       refit=True,\n",
    "                       cv=cv_lv1\n",
    "                      ) #----------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the estimator...')\n",
    "lin_reg.fit(features_train_lv1, labels_train_lv1)\n",
    "\n",
    "# evaluate the estimator\n",
    "log.info('Evaluating the estimator...')\n",
    "cv_results = ViewCV(lin_reg)\n",
    "\n",
    "print('\\nBest parameters:\\n')\n",
    "pretty(cv_results.best_parameters)\n",
    "\n",
    "print('\\nRMSE on the validation set: {:.3f} ± {:.3f}'.format(np.sqrt(-cv_results.test_mean()),\n",
    "                                                             np.sqrt(cv_results.test_std())\n",
    "                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) (Gaussian Kernel)\n",
    "\n",
    "We now implement a kernel function in the SVM. In particular we use the Gaussian kernel (_rbf_) and try to approximate the level-$\\infty$ predictions using these hyperparameters:\n",
    "\n",
    "- `gamma` $\\in \\left[ 10^{-1}, 10^1 \\right]$,\n",
    "- `C` $\\in \\left[ 10^{-1}, 5 \\times 10^2 \\right]$,\n",
    "- `epsilon` $\\in \\left[ 10^{-4}, 10^{-1} \\right]$,\n",
    "- `shrinking` $\\in \\lbrace 0, 1 \\rbrace$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:41:26,344: INFO ==> SVR\n",
      "2020-05-11 15:41:26,350: INFO ==> Fitting the estimator...\n",
      "2020-05-11 15:44:39,686: INFO ==> Evaluating the estimator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "\n",
      "    C = 22.524558668596875\n",
      "    epsilon = 0.0001\n",
      "    gamma = 0.1\n",
      "    shrinking = 1\n",
      "\n",
      "RMSE on the validation set: 0.443 ± 0.182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm      import SVR\n",
    "from skopt            import BayesSearchCV\n",
    "from skopt.space      import Real, Integer, Categorical\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from mltools.libscore import ViewCV\n",
    "from mltools.libplot  import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params  = {'epsilon':   Real(1.0e-4, 1.0e-1, prior='log-uniform'),\n",
    "                  'C':         Real(1.0e-1, 5.0e2,  prior='log-uniform'),\n",
    "                  'gamma':     Real(1.0e-1, 1.0e1,  prior='log-uniform'),\n",
    "                  'shrinking': Integer(0, 1)\n",
    "                 } #----------------------------------------------------------------------------------- define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "log.info('SVR')\n",
    "svr = BayesSearchCV(SVR(kernel='rbf'),\n",
    "                    search_spaces=search_params,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    n_jobs=-1,\n",
    "                    refit=True,\n",
    "                    n_iter=50,\n",
    "                    random_state=RAND,\n",
    "                    cv=cv_lv1\n",
    "                   ) #-------------------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the estimator...')\n",
    "svr.fit(features_train_lv1, labels_train_lv1)\n",
    "\n",
    "# evaluate the estimator\n",
    "log.info('Evaluating the estimator...')\n",
    "cv_results = ViewCV(svr)\n",
    "\n",
    "print('\\nBest parameters:\\n')\n",
    "pretty(cv_results.best_parameters)\n",
    "\n",
    "print('\\nRMSE on the validation set: {:.3f} ± {:.3f}'.format(np.sqrt(-cv_results.test_mean()),\n",
    "                                                             np.sqrt(cv_results.test_std())\n",
    "                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Histogram Based Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)\n",
    "\n",
    "We consider the new (still _experimental_) version of _Scikit_ **gradient boosting** algorithm, which is **histogram bases**. This should automatically handle _NaN_ gradients and be quite faster than the usual implementation. We will consider the followin hyperparameters:\n",
    "\n",
    "- `loss` $\\in \\lbrace least\\_squares, least\\_absolute\\_deviation \\rbrace$,\n",
    "- `learning_rate` $\\in \\left[ 10^{-4}, 10^{-1} \\right]$,\n",
    "- `max_iter` $\\in \\left[ 10, 300 \\right]$,\n",
    "- `max_depth` $\\in \\left[ 2, 100 \\right]$,\n",
    "- `min_samples_leaf` $\\in \\left[ 10, 100 \\right]$,\n",
    "- `l2_regularization` $\\in \\left[ 10^{-6}, 10^2 \\right]$,\n",
    "- `max_leaf_nodes` $\\in \\left[ 2, 50 \\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:44:39,735: INFO ==> BOOSTED TREES\n",
      "2020-05-11 15:44:39,736: INFO ==> Fitting the estimator...\n",
      "2020-05-11 15:54:16,696: INFO ==> Evaluating the estimator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "\n",
      "    l2_regularization = 0.01753175923543476\n",
      "    learning_rate = 0.1\n",
      "    loss = least_squares\n",
      "    max_depth = 100\n",
      "    max_iter = 76\n",
      "    max_leaf_nodes = 22\n",
      "    min_samples_leaf = 10\n",
      "\n",
      "RMSE on the validation set: 0.158 ± 0.110\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble     import HistGradientBoostingRegressor\n",
    "from skopt                import BayesSearchCV\n",
    "from skopt.space          import Real, Integer, Categorical\n",
    "from sklearn.metrics      import mean_squared_error\n",
    "from mltools.libscore     import ViewCV\n",
    "from mltools.libplot      import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params = {'max_iter':          Integer(1.0e1, 3.0e2, prior='log-uniform'),\n",
    "                 'max_depth':         Integer(2, 100,  prior='uniform'),\n",
    "                 'max_leaf_nodes':    Integer(2, 50,   prior='uniform'),\n",
    "                 'min_samples_leaf':  Integer(10, 100, prior='uniform'),\n",
    "                 'learning_rate':     Real(1.0e-4, 1.0e-1, prior='log-uniform'),\n",
    "                 'l2_regularization': Real(1.0e-6, 1.0e2,  prior='log-uniform'),\n",
    "                 'loss':              Categorical(['least_squares', 'least_absolute_deviation'])\n",
    "                } #------------------------------------------------------------------------------------- define the hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "log.info('BOOSTED TREES')\n",
    "grd_boost = BayesSearchCV(HistGradientBoostingRegressor(scoring='loss', validation_fraction=None, n_iter_no_change=10, random_state=RAND),\n",
    "                          search_spaces=search_params,\n",
    "                          scoring='neg_mean_squared_error',\n",
    "                          n_jobs=-1,\n",
    "                          refit=True,\n",
    "                          n_iter=25,\n",
    "                          random_state=RAND,\n",
    "                          cv=cv_lv1\n",
    "                         ) #--------------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the estimator...')\n",
    "grd_boost.fit(features_train_lv1, labels_train_lv1)\n",
    "\n",
    "# evaluate the estimator\n",
    "log.info('Evaluating the estimator...')\n",
    "cv_results = ViewCV(grd_boost)\n",
    "\n",
    "print('\\nBest parameters:\\n')\n",
    "pretty(cv_results.best_parameters)\n",
    "\n",
    "print('\\nRMSE on the validation set: {:.3f} ± {:.3f}'.format(np.sqrt(-cv_results.test_mean()),\n",
    "                                                             np.sqrt(cv_results.test_std())\n",
    "                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions Stacking\n",
    "\n",
    "We then stack the predictions for the second level training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the predictions for the 2nd level set\n",
    "predictions_lv2 = np.c_[lin_reg.best_estimator_.predict(features_train_lv2),\n",
    "                        svr.best_estimator_.predict(features_train_lv2),\n",
    "                        grd_boost.best_estimator_.predict(features_train_lv2)\n",
    "                       ]\n",
    "\n",
    "# stack the predictions for the test set\n",
    "predictions_test = np.c_[lin_reg.best_estimator_.predict(features_test),\n",
    "                         svr.best_estimator_.predict(features_test),\n",
    "                         grd_boost.best_estimator_.predict(features_test)\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Learning\n",
    "\n",
    "We then train the meta learner. We compare the performance of three different algorithms:\n",
    "\n",
    "- Linear Regression,\n",
    "- Boosted Trees,\n",
    "- Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:54:23,052: INFO ==> LINEAR REGRESSION\n",
      "2020-05-11 15:54:23,053: INFO ==> Fitting the estimator...\n",
      "2020-05-11 15:54:25,383: INFO ==> Evaluating the estimator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "\n",
      "    fit_intercept = 0\n",
      "    normalize = 0\n",
      "\n",
      "RMSE on the validation set: 0.161 ± 0.010\n",
      "\n",
      "RMSE of the test predictions: 0.087\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "from mltools.libscore        import ViewCV\n",
    "from mltools.libplot         import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params  = {'fit_intercept': [0, 1],\n",
    "                  'normalize':     [0, 1]\n",
    "                 } #----------------------------------------------------------------------------- define hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "log.info('LINEAR REGRESSION')\n",
    "lin_reg_meta = GridSearchCV(estimator=LinearRegression(),\n",
    "                            param_grid=search_params,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            n_jobs=-1,\n",
    "                            refit=True,\n",
    "                            cv=cv_lv2\n",
    "                           ) #----------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the estimator...')\n",
    "lin_reg_meta.fit(predictions_lv2, labels_train_lv2)\n",
    "\n",
    "# evaluate the estimator\n",
    "log.info('Evaluating the estimator...')\n",
    "cv_results = ViewCV(lin_reg_meta)\n",
    "\n",
    "print('\\nBest parameters:\\n')\n",
    "pretty(cv_results.best_parameters)\n",
    "\n",
    "print('\\nRMSE on the validation set: {:.3f} ± {:.3f}'.format(np.sqrt(-cv_results.test_mean()),\n",
    "                                                             np.sqrt(cv_results.test_std())\n",
    "                                                            )\n",
    "     )\n",
    "\n",
    "print('\\nRMSE of the test predictions: {:.3f}'.format(np.sqrt(mean_squared_error(y_true=labels_test, y_pred=lin_reg_meta.predict(predictions_test)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:54:25,452: INFO ==> BOOSTED TREES\n",
      "2020-05-11 15:54:25,466: INFO ==> Fitting the estimator...\n",
      "2020-05-11 16:10:03,506: INFO ==> Evaluating the estimator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "\n",
      "    l2_regularization = 0.010732312983479157\n",
      "    learning_rate = 0.1\n",
      "    loss = least_squares\n",
      "    max_depth = 33\n",
      "    max_iter = 228\n",
      "    max_leaf_nodes = 30\n",
      "    min_samples_leaf = 10\n",
      "\n",
      "RMSE on the validation set: 0.156 ± 0.089\n",
      "\n",
      "RMSE of the test predictions: 0.033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble     import HistGradientBoostingRegressor\n",
    "from skopt                import BayesSearchCV\n",
    "from skopt.space          import Real, Integer, Categorical\n",
    "from sklearn.metrics      import mean_squared_error\n",
    "from mltools.libscore     import ViewCV\n",
    "from mltools.libplot      import Plot\n",
    "\n",
    "import joblib\n",
    "\n",
    "# parameter dictionary\n",
    "search_params = {'max_iter':          Integer(1.0e1, 3.0e2, prior='log-uniform'),\n",
    "                 'max_depth':         Integer(2, 100,  prior='uniform'),\n",
    "                 'max_leaf_nodes':    Integer(2, 50,   prior='uniform'),\n",
    "                 'min_samples_leaf':  Integer(10, 100, prior='uniform'),\n",
    "                 'learning_rate':     Real(1.0e-4, 1.0e-1, prior='log-uniform'),\n",
    "                 'l2_regularization': Real(1.0e-6, 1.0e2,  prior='log-uniform'),\n",
    "                 'loss':              Categorical(['least_squares', 'least_absolute_deviation'])\n",
    "                } #------------------------------------------------------------------------------------- define the hyperparameter grid\n",
    "\n",
    "# optimization search\n",
    "log.info('BOOSTED TREES')\n",
    "grd_boost_meta = BayesSearchCV(HistGradientBoostingRegressor(scoring='loss', validation_fraction=None, n_iter_no_change=10, random_state=RAND),\n",
    "                               search_spaces=search_params,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               n_jobs=-1,\n",
    "                               refit=True,\n",
    "                               n_iter=25,\n",
    "                               random_state=RAND,\n",
    "                               cv=cv_lv2\n",
    "                              ) #--------------------------------------------------------------------------- define the optimization estimator\n",
    "\n",
    "# fit the estimator\n",
    "log.info('Fitting the estimator...')\n",
    "grd_boost_meta.fit(predictions_lv2, labels_train_lv2)\n",
    "\n",
    "# evaluate the estimator\n",
    "log.info('Evaluating the estimator...')\n",
    "cv_results = ViewCV(grd_boost_meta)\n",
    "\n",
    "print('\\nBest parameters:\\n')\n",
    "pretty(cv_results.best_parameters)\n",
    "\n",
    "print('\\nRMSE on the validation set: {:.3f} ± {:.3f}'.format(np.sqrt(-cv_results.test_mean()),\n",
    "                                                             np.sqrt(cv_results.test_std())\n",
    "                                                            )\n",
    "     )\n",
    "\n",
    "print('\\nRMSE of the test predictions: {:.3f}'.format(np.sqrt(mean_squared_error(y_true=labels_test, y_pred=grd_boost_meta.predict(predictions_test)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lv2_training\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer_1 (Dense)        (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dense_layer_1_activation (Le (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "exp (Dense)                  (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 16\n",
      "Trainable params: 16\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "RMSE of the validation set predictions: 0.142\n",
      "RMSE of the test set predictions:       0.106\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(RAND)\n",
    "\n",
    "from tensorflow.keras            import Sequential\n",
    "from tensorflow.keras.layers     import Dense, BatchNormalization, Dropout, LeakyReLU, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks  import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils      import model_to_dot\n",
    "\n",
    "# create validation set\n",
    "predictions_lv2_train, predictions_lv2_val, \\\n",
    "labels_train_lv2_train, labels_train_lv2_val = train_test_split(predictions_lv2, labels_train_lv2,\n",
    "                                                                test_size=0.5,\n",
    "                                                                shuffle=True,\n",
    "                                                                random_state=RAND)\n",
    "\n",
    "# create the model\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential(name='lv2_training')\n",
    "\n",
    "model.add(InputLayer(input_shape=(predictions_lv2_train.shape[1],), name='lv1_predictions'))\n",
    "\n",
    "model.add(Dense(3, name='dense_layer_1'))\n",
    "model.add(LeakyReLU(alpha=0.1, name='dense_layer_1_activation'))\n",
    "\n",
    "model.add(Dense(1, name='exp'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "model.summary()\n",
    "model_dot = model_to_dot(model=model, show_shapes=True)\n",
    "model_dot.write_pdf(path.join(IMG_PATH, 'neural_network_stacking.pdf'))\n",
    "\n",
    "# fit the model\n",
    "callbacks = [EarlyStopping(monitor='val_mean_squared_error', patience=1000, verbose=0),\n",
    "             ReduceLROnPlateau(monitor='val_mean_squared_error', factor=0.33, patience=750, verbose=0),\n",
    "             ModelCheckpoint(filepath=path.join(MOD_PATH, 'neural_network_stacking.h5'), monitor='val_mean_squared_error', save_best_only=True, verbose=0)\n",
    "            ]\n",
    "model_history = model.fit(x=predictions_lv2_train,\n",
    "                          y=labels_train_lv2_train.values,\n",
    "                          batch_size=predictions_lv2_train.shape[0],\n",
    "                          epochs=10000,\n",
    "                          callbacks=callbacks,\n",
    "                          validation_data=(predictions_lv2_val, labels_train_lv2_val.values),\n",
    "                          verbose=0\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "val_evaluation  = model.evaluate(x=predictions_lv2_val, y=labels_train_lv2_val.values, verbose=0)\n",
    "print('\\nRMSE of the validation set predictions: {:.3f}'.format(np.sqrt(val_evaluation[1])))\n",
    "test_evaluation = model.evaluate(x=predictions_test, y=labels_test.values, verbose=0)\n",
    "print('RMSE of the test set predictions:       {:.3f}'.format(np.sqrt(test_evaluation[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and Data Saving\n",
    "\n",
    "After training the algorithms, it seems that simply using a linear regression model as meta learner improves the predictive abilities of the model. The neural networks improves the model with very few parameters. Finally the gradient boosting seems to provide the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "predictions = {'exp_true':       labels_test.tolist(),\n",
    "               'lin_reg':        lin_reg_meta.best_estimator_.predict(predictions_test).tolist(),\n",
    "               'grd_boost':      grd_boost_meta.best_estimator_.predict(predictions_test).tolist(),\n",
    "               'neural_network': model.predict(predictions_test).tolist()\n",
    "              }\n",
    "\n",
    "with open(path.join(OUT_PATH, 'stacking_results.json'), 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
