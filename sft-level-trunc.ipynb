{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "In the framework of bosonic Open String Field Theory (OSFT), we consider several observables characterised by conformal weight and type, and the position of vacua in the potential for various values of truncated mass level. We focus on the prediction of the extrapolated value for the level-$\\infty$ truncation using Machine Learning (ML) techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "In this section we prepare the analysis: we take a look at the present configuration, set the number of threads used in the analysis, prepare log files and create a structure for the directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules when running the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications\n",
    "\n",
    "The current analysis runs on a machine with the following specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"CPU: $(head /proc/cpuinfo | awk -F': ' '/^model name/ {print $2}')\"\n",
    "!echo \"GPU: $(lspci | awk -F': ' '/3D controller/ {print $2}')\"\n",
    "!echo \"RAM: $(free --giga| awk '/^Mem/ {print $7}')GB available\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set the number of cores used in this analysis for parallel computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import cpu_count\n",
    "\n",
    "# set no. of threads\n",
    "THREADS = 8\n",
    "MAX_THREADS = cpu_count()\n",
    "\n",
    "# sanity check: cannot use more than available\n",
    "if THREADS > MAX_THREADS:\n",
    "    THREADS = MAX_THREADS\n",
    "\n",
    "# print summary\n",
    "print('Using {:d} of {:d} threads available.'.format(THREADS, MAX_THREADS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the directory structure for output, logs, code, data, and images. Everything will be considered with respect to the root directory containing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "\n",
    "# define directory names\n",
    "ROOT_DIR = '.'\n",
    "IMG_DIR  = 'img'\n",
    "MOD_DIR  = 'mod'\n",
    "LOG_DIR  = 'log'\n",
    "DAT_DIR  = 'dat'\n",
    "OUT_DIR  = 'out'\n",
    "\n",
    "# get the name of the dataset\n",
    "DB_NAME = 'data_sft_dict'\n",
    "DB_FILE = DB_NAME + '.json'\n",
    "DB_PATH = path.join(ROOT_DIR, DB_FILE)\n",
    "\n",
    "# define full paths\n",
    "IMG_PATH = path.join(ROOT_DIR, IMG_DIR)\n",
    "print('Images: {}'.format(IMG_PATH))\n",
    "MOD_PATH = path.join(ROOT_DIR, MOD_DIR)\n",
    "print('Models: {}'.format(MOD_PATH))\n",
    "LOG_PATH = path.join(ROOT_DIR, LOG_DIR)\n",
    "print('Logs:   {}'.format(LOG_PATH))\n",
    "DAT_PATH = path.join(ROOT_DIR, DAT_DIR)\n",
    "print('Data:   {}'.format(DAT_PATH))\n",
    "OUT_PATH = path.join(ROOT_DIR, OUT_DIR)\n",
    "print('Other:  {}'.format(OUT_PATH))\n",
    "\n",
    "# define shortcuts for path of images, models, logs, data, etc.\n",
    "imgpath = lambda s: path.join(IMG_PATH, s)\n",
    "modpath = lambda s: path.join(MOD_PATH, s)\n",
    "logpath = lambda s: path.join(LOG_PATH, s)\n",
    "datpath = lambda s: path.join(DAT_PATH, s)\n",
    "outpath = lambda s: path.join(OUT_PATH, s)\n",
    "\n",
    "# create directories if non existent\n",
    "if not path.isdir(IMG_PATH):\n",
    "    makedirs(IMG_PATH, exist_ok = True)\n",
    "if not path.isdir(MOD_PATH):\n",
    "    makedirs(MOD_PATH, exist_ok = True)\n",
    "if not path.isdir(LOG_PATH):\n",
    "    makedirs(LOG_PATH, exist_ok = True)\n",
    "if not path.isdir(DAT_PATH):\n",
    "    makedirs(DAT_PATH, exist_ok = True)\n",
    "if not path.isdir(OUT_PATH):\n",
    "    makedirs(OUT_PATH, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save relevant debugging output to a log file containing dates, times and information on the operations to be able to easily find issues in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from os   import path, rename\n",
    "from time import strftime, gmtime\n",
    "\n",
    "# call the log with the same name as the dataset we use\n",
    "logname  = DB_NAME \n",
    "filename = path.join(LOG_PATH, DB_NAME + '.log')\n",
    "ctime    = strftime('_%Y%m%d.%H%M%S', gmtime())\n",
    "level    = logging.DEBUG\n",
    "\n",
    "# rotate the log if one is already present\n",
    "if path.isfile(filename):\n",
    "    rename(filename, filename + ctime)\n",
    "\n",
    "# print the log to file\n",
    "log = logging.getLogger(logname + ctime)\n",
    "fmt = logging.Formatter('%(asctime)s: %(levelname)s ==> %(message)s')\n",
    "han = logging.FileHandler(filename = filename)\n",
    "log.setLevel(level)\n",
    "han.setLevel(level)\n",
    "han.setFormatter(fmt)\n",
    "log.addHandler(han)\n",
    "\n",
    "# write introduction to log\n",
    "log.info('\\n\\n'\n",
    "         '--------------------------------------------\\n'\n",
    "         '  MACHINE LEARNING FOR LEVEL TRUNCATION IN\\n'\n",
    "         '  BOSONIC OPEN STRING FIELD THEORY\\n\\n'\n",
    "         '--------------------------------------------\\n'\n",
    "         '  Authors: Harold Erbin, Riccardo Finotello\\n'\n",
    "         '--------------------------------------------\\n'\n",
    "         '  Abstract:\\n\\n'\n",
    "         '  We consider the position of the lumps of\\n'\n",
    "         '  the tachyon potential in bosonic open\\n'\n",
    "         '  string field theory at a finite mass level\\n'\n",
    "         '  truncation. We then extrapolate the\\n'\n",
    "         '  predictions for level-$\\infty$ using\\n'\n",
    "         '  machine learning techniques.\\n\\n'\n",
    "        )\n",
    "\n",
    "print('Current log: {}'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally import the modules we use in the analysis and check their version for issues tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Python version\n",
    "import sys\n",
    "assert sys.version_info.major > 2 and sys.version_info.minor > 5\n",
    "print('Python version: {}.{}.{}'.format(sys.version_info.major,\n",
    "                                        sys.version_info.minor,\n",
    "                                        sys.version_info.micro\n",
    "                                       )\n",
    "     )\n",
    "\n",
    "# import Numpy\n",
    "import numpy as np\n",
    "assert np.__version__ >= '1.18'\n",
    "print('Numpy version:           {}'.format(np.__version__))\n",
    "\n",
    "# import Pandas\n",
    "import pandas as pd\n",
    "assert pd.__version__  >= '1.0'\n",
    "print('Pandas version:          {}'.format(pd.__version__))\n",
    "\n",
    "# import Seaborn and Matplotlib\n",
    "import seaborn           as sns\n",
    "import matplotlib        as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "assert mpl.__version__ >= '3.1' and sns.__version__ >= '0.10'\n",
    "print('Matplotlib version:      {}'.format(mpl.__version__))\n",
    "print('Seaborn version:         {}'.format(sns.__version__))\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "subplots = lambda x, y: plt.subplots(x, y, figsize=(y*6, x*5))\n",
    "\n",
    "# import Scikit-learn and Scikit-optimize\n",
    "import sklearn\n",
    "import skopt\n",
    "assert sklearn.__version__ >= '0.22' and skopt.__version__ >= '0.7'\n",
    "print('Scikit-learn version:    {}'.format(sklearn.__version__))\n",
    "print('Scikit-optimize version: {}'.format(skopt.__version__))\n",
    "\n",
    "# import Tensorflow (check that Keras is included: v2.0+)\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ > '2.0'\n",
    "print('Tensorflow version:      {}'.format(tf.__version__))\n",
    "\n",
    "# check the warnings (I can't really handle UserWarnings unfortunately...)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category = UserWarning)\n",
    "\n",
    "# set the random state for reproducibility\n",
    "RAND = 121\n",
    "np.random.seed(RAND)\n",
    "np.random.RandomState(RAND)\n",
    "tf.random.set_seed(RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some computations we will be using GPU acceleration: we fix the amount of memory accessible by the GPU in order to limit its growth and to allocate only the necessary amount at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU') # physical GPUs\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "log.debug('Set GPU memeory growth.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset\n",
    "\n",
    "The environment being ready for the analysis, we now import the dataset and take a look at its characteristics and properties. We then proceed to tidy up its content for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the extraction of the dataset we use `pandas` to read from the original format (JSON):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n",
    "\n",
    "# check if file exists\n",
    "if path.isfile(DB_PATH):\n",
    "    df = pd.read_json(DB_PATH)\n",
    "\n",
    "# check if object is in the environment\n",
    "if df is None:\n",
    "    log.error('Cannot load database from JSON file!')\n",
    "else:\n",
    "    log.debug('Database correctly imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reference, we show the composition of the dataset highlighting its composition and data types stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database stores its data in a non tidy format (`object` _dtype_ is generic). Its dimensions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nrow, df_ncol = df.shape\n",
    "print(\"Rows:    {:d}\".format(df_nrow))\n",
    "print(\"Columns: {:d}\".format(df_ncol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns are already named but the truncation levels have numeric names which might lead to confusion. We rename them to avoid issues with other numeric entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# rename columns with numbers in their name\n",
    "colrename = lambda s: re.sub(r'^([0-9]*)$', r'level_\\1', s)\n",
    "df        = df.rename(columns=colrename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new columns are therefore named:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)\n",
    "log.info('Columns in the database: {}'.format(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements of the dataset are vectors containing **solutions for different radii** in the rows. For each row various observables have been computed: `type` refers to the **oscillations** in the level dependence, while `weight` is the conformal property identifying each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of observables in each row is not unique across the datasets. In fact we can check that they are unique only in the same row, but they increase in the column direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the length of each list and find the unique values\n",
    "shapes = df.applymap(len)\n",
    "shapes = shapes.apply(lambda x: np.unique(x).squeeze(), axis=1)\n",
    "\n",
    "# plot the counts for each shape (treat it as categorical)\n",
    "fig, ax = subplots(1, 1)\n",
    "\n",
    "sns.countplot(x=shapes.values,\n",
    "              palette='viridis',\n",
    "              ax=ax\n",
    "             )\n",
    "ax.set(xlabel='length',\n",
    "       ylabel='counts',\n",
    "       title='Length of the Solutions'\n",
    "      )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('length-solutions.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying Up the Dataset\n",
    "\n",
    "We then proceed to tidy the dataset in such a way that each column stores only one \"observation\" of the solutions. We first insert a column labelling the solutions **for each** entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the ID of the solution\n",
    "solutions = [] #------------------------------------ create a list of vectors\n",
    "for n in range(shapes.shape[0]):\n",
    "    solutions.append(np.full(shapes.iloc[n], n)) #-- insert vec. of the IDs\n",
    "\n",
    "# add the column to the dataset and reorder\n",
    "df['solutions'] = solutions\n",
    "df = df[['solutions',\n",
    "         'init',\n",
    "         'weight',\n",
    "         'type',\n",
    "         'exp',\n",
    "         'level_2',\n",
    "         'level_3',\n",
    "         'level_4',\n",
    "         'level_5',\n",
    "         'level_6',\n",
    "         'level_7',\n",
    "         'level_8',\n",
    "         'level_9',\n",
    "         'level_10',\n",
    "         'level_11',\n",
    "         'level_12',\n",
    "         'level_13',\n",
    "         'level_14',\n",
    "         'level_15',\n",
    "         'level_16',\n",
    "         'level_17',\n",
    "         'level_18'\n",
    "        ]\n",
    "       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then stack each column on top of the other by creating a `pandas` dataframe for each row and then putting them one on top of the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.DataFrame({f: df[f].iloc[n] for f in df})\n",
    "                for n in range(df.shape[0]) \n",
    "               ],\n",
    "               axis=0,\n",
    "               ignore_index=True\n",
    "              ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset now holds only numeric types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore have a description of the variables. In particular we first take a look at the initial points, weights, types and the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['init', 'weight', 'type', 'exp']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move to the truncation levels from 2 to 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(regex='^level_[2-9]|level_10').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we consider the levels from 11 to 18:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(regex='^level_1[1-8]').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then identify and drop any duplicates which may have been created: we consider duplicates rows which are identical throughout the columns. We keep only the first entry of each of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future referencing, we sort the dataset by `solutions`, `weight` and `type` (in this order). This way the output of any preprocessing or analysis can be saved in a comparable format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering_columns = ['solutions', 'weight', 'type']\n",
    "df = df.sort_values(ordering_columns, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the dataset for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(datpath('sft-tidy-data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset has a reduced shape with respect to the initial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow, ncol = df.shape\n",
    "print('Rows:    {:d}'.format(nrow))\n",
    "print('Columns: {:d}'.format(ncol))\n",
    "\n",
    "log.info('Tidy dataset: {:d} rows x {:d} columns'.format(nrow, ncol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "In the previous section we loaded and prepared the dataset for the analysis. In this section we focus on Exploratory Data Analysis (EDA), revealing outliers and underlying distribution, and the preprocessing of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the tables summarising the variability of the features and the labels, the dataset may contain a certain number of outlying observations. In fact we want to quantify the population of outliers by computing the _interquartile_ range of each feature (only those necessary for the analysis: we can drop `solutions` and `init` which are disposable, intermediate properties of the solutions). We compute the fraction of outliers for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quartile(series: pd.Series) -> pd.Series:\n",
    "    '''\n",
    "    Return the logical array of samples outside the interquartile range.\n",
    "    \n",
    "    Required arguments:\n",
    "        series: the Pandas series to consider.\n",
    "        \n",
    "    Returns:\n",
    "        the logical array of outliers\n",
    "    '''\n",
    "    # compute the 25th and 75th percentile\n",
    "    q1, q3 = series.quantile([0.25, 0.75])\n",
    "    iqr    = q3 - q1\n",
    "    \n",
    "    # compute the lower and upper bounds of the interval\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    \n",
    "    return (series < lower) | (series > upper)\n",
    "\n",
    "# apply to dataset\n",
    "df_quart = df.drop(columns=['solutions', 'init', 'exp'])\n",
    "df_quart = df_quart.apply(quartile, axis=0)\n",
    "df_quart = df_quart.apply(lambda x: round(np.mean(x), 2), axis=0)\n",
    "df_quart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be visualised in plots of the features: we build the histograms of the **digitized** distribution of the values. In other words, we first construct reasonable bins of the data in each column and then plot the distributions counting the number of entries inside each interval designated by the bins of the histograms (N.B.: they are **left-inclusive** intervals, namely between two labels $a$ and $b$ we count the number of occurrencies in the interval $[a, b)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_bins(series: pd.Series,\n",
    "                  n_bins: int,\n",
    "                  ax: None\n",
    "                 ) -> None:\n",
    "    '''\n",
    "    Digitise the series into a given number of discrete bins.\n",
    "    \n",
    "    Required arguments:\n",
    "        series: the Pandas series to manipulate,\n",
    "        n_bins: the number of bins,\n",
    "        ax:     the subplot axis.\n",
    "        \n",
    "    Returns:\n",
    "        the subplot axis\n",
    "    '''\n",
    "    # first check the number of unique values\n",
    "    n_unique = pd.unique(series).shape[0]\n",
    "    if n_bins > n_unique:\n",
    "        n_bins = n_unique\n",
    "        \n",
    "    # compute the discretisation\n",
    "    cuts, bins = pd.cut(series,\n",
    "                        right=False,\n",
    "                        bins=n_bins,\n",
    "                        labels=range(n_bins),\n",
    "                        retbins=True,\n",
    "                        precision=1\n",
    "                       )\n",
    "    \n",
    "    # plot the discretization\n",
    "    sns.distplot(cuts,\n",
    "                 bins=range(n_bins + 1),\n",
    "                 kde=False,\n",
    "                 axlabel='intervals',\n",
    "                 ax=ax\n",
    "                )\n",
    "    ax.set(ylabel='#',\n",
    "           title='Distribution of \\\"{}\\\"'.format(series.name),\n",
    "           xticks=range(n_bins + 1),\n",
    "           yscale='log'\n",
    "          )\n",
    "    ax.set_xticklabels(np.round(bins, 1), rotation=90, ha='right')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# plot all features\n",
    "nrows, ncols = (5,4)\n",
    "fig, ax = subplots(nrows, ncols)\n",
    "\n",
    "df_plot = df.drop(columns=['solutions', 'init'])\n",
    "df_cols = np.array(df_plot.columns).reshape(nrows, ncols)\n",
    "\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        discrete_bins(df_plot[df_cols[i,j]], n_bins=10, ax=ax[i,j])\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('dataset-distribution_full.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the distributions are not ideally of the same order, but they have a decent amount of outlying samples outside the expected interval. In fact this seems to be a problem when `weight` is larger than $1.5$. We can in fact try to apply the same procedure in the case of low `weight` and high `weight`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider for example the case with `weight` $< 1.5$ and plot the distribution of the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all features\n",
    "nrows, ncols = (5,4)\n",
    "fig, ax = subplots(nrows, ncols)\n",
    "\n",
    "df_low  = df.loc[df['weight'] < 1.5].drop(columns=['solutions', 'init'])\n",
    "df_cols = np.array(df_low.columns).reshape(nrows, ncols)\n",
    "\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        discrete_bins(df_low[df_cols[i,j]], n_bins=10, ax=ax[i,j])\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('dataset-distribution_low.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in this case the variables are in general $\\mathrm{O}(1)$ and their variability is well distributed as we can also appreciate by computing the **sample variance**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low.apply([np.mean, np.var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can also try to visualise the boxplot of the variables since their distribution is more restricted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(1,1)\n",
    "\n",
    "# drop the column type since it's categorical\n",
    "plot_data = df_low.drop(columns='type')\n",
    "sns.boxplot(data=plot_data,\n",
    "            order=plot_data.columns,\n",
    "            palette=sns.color_palette('bright', plot_data.shape[1]),\n",
    "            ax=ax\n",
    "           )\n",
    "ax.set(title='Distribution of the Variables (weight < 1.5)')\n",
    "ax.set_xticklabels(df_low.columns, rotation=90)\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('dataset-distribution_box_low.pdf'),\n",
    "            dpi=150,\n",
    "            format='pdf'\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all numerical variables (`type` is categorical and has been excluded from the plot) are all $\\mathrm{O}(1)$, the interquartile ranges (the continuous vertical lines) are comparable, the medians (the short continuous horizontal lines) roughly lie in the same range and the number of outliers (separate points) is restricted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally deal with the same considerations for `weight` $> 1.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all features\n",
    "nrows, ncols = (5,4)\n",
    "fig, ax = subplots(nrows, ncols)\n",
    "\n",
    "df_high = df.loc[df['weight'] >= 1.5].drop(columns=['solutions', 'init'])\n",
    "df_cols = np.array(df_high.columns).reshape(nrows, ncols)\n",
    "\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        discrete_bins(df_high[df_cols[i,j]], n_bins=10, ax=ax[i,j])\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('dataset-distribution_high.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the distributions are far wider and less contained: variables are no longer $\\mathrm{O}(1)$. In fact the same computation of mean and variance leads to a totally different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high.apply(['mean', 'var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we immediately notice is that when `weight` $> 1.5$ then the `type` of the observable is strictly $4.0$ (we may have to exclude it when dealing with the regression analysis). In fact, we can try to study the properties of the observables (`weight` and `type`) in the two cases side-by-side after binarising the conformal weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a categorical column with high and low weights\n",
    "df_bin = df[['weight', 'type']].copy()\n",
    "df_bin['weight_bin'] = \\\n",
    "    pd.Categorical((df_bin['weight'] < 1.5).astype(int)).\\\n",
    "    rename_categories(['> 1.5', '< 1.5'])\n",
    "\n",
    "df_bin.groupby(['weight_bin', 'type']).agg({'weight': ['mean', 'var']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore learn that for higher weights the `type` of the observable is strictly $4$, while for lower weights `type` $= 2$ implies `weight` $= 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step in the exploratory data analysis we show the correlation matrix of the variables (only those used in the regression analysis):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation matrix of the variables\n",
    "df_corr = df.drop(columns=['solutions', 'init'])\n",
    "df_corr = df_corr.corr()\n",
    "\n",
    "fig, ax = subplots(1,1)\n",
    "sns.heatmap(df_corr,\n",
    "            vmin=-1.0,\n",
    "            vmax=1.0,\n",
    "            cmap='RdBu_r',\n",
    "            ax=ax\n",
    "           )\n",
    "ax.set(title='Correlation Matrix')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('corr-mat.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the divergent palette we can see that the `type` of the observable is mostly unrelated to the other variables, while the truncation levels are strongly correlated among themselves (especially higher levels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Analysis and PCA\n",
    "\n",
    "Before moving to the regression analysis and still in the **EDA** section of the pre-analysis, we consider the clustering labelling of the truncation levels: we would like to investigate the possibility of finding an underlying structure in the truncation levels pointing towards the extrapolated labels, `exp`. In the ideal scenario the labels of the clusters should be in 1:1 correspondence with the predictions labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the no. of clusters = no. of labels\n",
    "n_cls = pd.unique(df['exp']).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis we focus separately on clustering `weight` $< 1.5$ and `weight` $\\ge 1.5$ since the variability in the data may force some labels to take the same value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low  = df.loc[df['weight'] < 1.5].drop(columns=['solutions',\n",
    "                                                   'init',\n",
    "                                                   'type',\n",
    "                                                   'weight']\n",
    "                                         )\n",
    "df_high = df.loc[df['weight'] >= 1.5].drop(columns=['solutions',\n",
    "                                                    'init',\n",
    "                                                    'type',\n",
    "                                                    'weight']\n",
    "                                          )\n",
    "\n",
    "# shuffle the datasets\n",
    "df_low  = df_low.sample(frac=1, random_state=RAND)\n",
    "df_high = df_high.sample(frac=1, random_state=RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is mostly a qualitative analysis, we do not worry to split the dataset into training and test sets and keep a single training split. We first preprocess the features with a `StandardScaler` in the case of low `weight` and a `RobustScaler` for high `weight`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster       import KMeans\n",
    "\n",
    "# preprocess the data\n",
    "data_low  = StandardScaler().fit_transform(df_low.iloc[:,1:])\n",
    "data_high = RobustScaler().fit_transform(df_high.iloc[:,1:])\n",
    "\n",
    "# apply clustering\n",
    "log.info('Start of the clustering analysis.')\n",
    "lab_low  = KMeans(n_clusters=3, random_state=RAND).fit_predict(data_low)\n",
    "lab_high = KMeans(n_clusters=3, random_state=RAND).fit_predict(data_high)\n",
    "log.info('End of the clustering analysis')\n",
    "\n",
    "# save the labels\n",
    "df_low['kmeans']  = lab_low\n",
    "df_high['kmeans'] = lab_high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualise a 2D scatter plot of the labels as compared to the extrapolation results, we first perform a dimensionality reduction on the truncation levels (using the `PCA` and retaining only the first two components). This is also a good chance to provide some insights on the principal components of the truncation levels and whether they can be of use in the analysis. We first compute the principal components of the **scaled** dataset (divided into high `weight` and low `weight` parts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SVD to the truncation levels\n",
    "_, s_low, _  = np.linalg.svd(data_low)\n",
    "_, s_high, _ = np.linalg.svd(data_high)\n",
    "\n",
    "# square and normalise\n",
    "s2_low  = s_low**2 / sum(s_low**2)\n",
    "s2_high = s_high**2 / sum(s_high**2)\n",
    "\n",
    "# create a dataframe holding components and distinguish weight <> 1.5\n",
    "svd_dat = pd.DataFrame(\\\n",
    "            {'comp': np.hstack((np.arange(1, data_low.shape[1]+1),\n",
    "                                np.arange(1, data_low.shape[1]+1)\n",
    "                               )\n",
    "                              ),\n",
    "             'var': np.hstack((s2_low, s2_high)),\n",
    "             'weight': ['< 1.5'] * s2_low.shape[0] +\n",
    "                       ['≥ 1.5'] * s2_high.shape[0]\n",
    "            }\n",
    "                      )\n",
    "\n",
    "# plot the retained variance of each component\n",
    "fig, ax = subplots(1,1)\n",
    "sns.scatterplot(data=svd_dat,\n",
    "                x='comp',\n",
    "                y='var',\n",
    "                hue='weight',\n",
    "                style='weight',\n",
    "                palette=sns.color_palette('bright', 2),\n",
    "                ax=ax\n",
    "               )\n",
    "ax.set(title='Variance Explained by Principal Component',\n",
    "       xlabel='component',\n",
    "       ylabel='variance explained',\n",
    "       yscale='log'\n",
    "      )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('pca-variance-explained.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that the `PCA` can be performed on the first few components for the two datasets since most of the variance is already explained by the first component when `weight` $\\ge 1.5$ or by the first two components when `weight` $< 1.5$ (we drop the components which explain less than 10% of the variability):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_comp = {'comp': np.arange(1, data_low.shape[1]+1),\n",
    "            'low_weight': np.round(svd_dat.loc[svd_dat['weight'] == '< 1.5']['var'].\\\n",
    "                            values, 3),\n",
    "            'high_weight': np.round(svd_dat.loc[svd_dat['weight'] == '≥ 1.5']['var'].\\\n",
    "                            values, 3)\n",
    "           }\n",
    "svd_comp = pd.DataFrame(svd_comp)\n",
    "svd_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might sound redundant since we only wanted to plot the two principal components, but this may come in handy when performing the regression analysis. In the meantime we can reduce the number of components using the `PCA` and plot the cluster labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# perform PCA\n",
    "log.info('Start of PCA computation.')\n",
    "princ_low  = PCA(n_components=2, random_state=RAND).fit_transform(data_low)\n",
    "princ_high = PCA(n_components=2, random_state=RAND).fit_transform(data_high)\n",
    "log.info('End of PCA computation.')\n",
    "\n",
    "# create dataframe\n",
    "dat_low = {'comp_1': princ_low[:,0],\n",
    "           'comp_2': princ_low[:,1],\n",
    "           'clusters': df_low['kmeans'].values,\n",
    "           'extrapolations': df_low['exp'].values\n",
    "          }\n",
    "dat_high = {'comp_1': princ_high[:,0],\n",
    "            'comp_2': princ_high[:,1],\n",
    "            'clusters': df_high['kmeans'].values,\n",
    "            'extrapolations': df_high['exp'].values\n",
    "           }\n",
    "\n",
    "dat_low  = pd.DataFrame(dat_low)\n",
    "dat_high = pd.DataFrame(dat_high)\n",
    "\n",
    "# plot the principal components, clusters and extrapolations\n",
    "fig, ax = subplots(1,2)\n",
    "\n",
    "sns.scatterplot(data=dat_low,\n",
    "                x='comp_1',\n",
    "                y='comp_2',\n",
    "                hue='clusters',\n",
    "                style='extrapolations',\n",
    "                alpha=0.6,\n",
    "                palette=sns.color_palette('bright', n_cls),\n",
    "                ax=ax[0]\n",
    "               )\n",
    "ax[0].set(title='KMeans Clustering (weight < 1.5)',\n",
    "          xlabel='princ. comp. #1',\n",
    "          ylabel='princ. comp. #2'\n",
    "         )\n",
    "\n",
    "sns.scatterplot(data=dat_high,\n",
    "                x='comp_1',\n",
    "                y='comp_2',\n",
    "                hue='clusters',\n",
    "                style='extrapolations',\n",
    "                alpha=0.6,\n",
    "                palette=sns.color_palette('bright', n_cls),\n",
    "                ax=ax[1]\n",
    "               )\n",
    "ax[1].set(title='KMeans Clustering (weight ≥ 1.5)',\n",
    "          xlabel='princ. comp. #1',\n",
    "          ylabel='princ. comp. #2'\n",
    "         )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('kmeans-clusters.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data it seems that when `weight` $< 1.5$, the data present a good clustered structure which can well represent the distribution of the data. For example we can compute the mean of the labels of the clusters for each prediction label to see where their distribution is peaked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_low.groupby(['extrapolations']).agg({'clusters': 'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case it seems that by simply rounding to the next integer the mean of the cluster labels we can assign a cluster centroid to each extrapolated label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot however say the same in the case of high weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_high.groupby(['extrapolations']).agg({'clusters': 'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, in this case the data is so scattered that no clusters are visibly recognisable. We also notice from the plots that when `weight` $\\ge 1.5$ the variance of the data is almost completely captured by the first principal component since the range of variability is more than six times larger than the range of the second principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis\n",
    "\n",
    "We can now move to the regression analysis. We shall consider several different approaches to the regression task, starting from a linear regression for simplicity and analysis of the coefficients, moving to Support Vector Machines (SVM) and decision tree algorithms (random forest, RF, and gradient boosted decision trees, GBDT, to be specific\n",
    "\n",
    "As a first attempt, we will **keep the full dataset** without distinction on `weight` to assess the ability of the ML algorithm to adapt to the situation. We will however remove the first values of the `solutions` since they are artificially \"too perfect\" for the analysis and may spoil the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df[df['solutions'] != 0]\n",
    "df_analysis = df_analysis.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variability of the dataset is however mostly unaffected by the transformation as the summary of the truncation levels can confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.filter(regex='^level.*$').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Validation Sets\n",
    "\n",
    "The first necessary step for the analysis is the selection of test and validations sets. The selection is performed on the basis of the `solutions` column in order to keep equal values of `solutions` in the same set as to prevent the mixing of different distributions: we first split the unique values of `solutions` into train/validation/test sets and then assign the corresponding samples to the splits.\n",
    "\n",
    "We will keep around 10% of the samples in the test set to start with and we will then perform a quick regression analysis to choose the size of the validation set in order to keep the Mean Squared Error (MSE) as contained as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# define the unique values of solutions\n",
    "solutions = pd.unique(df_analysis['solutions'])\n",
    "\n",
    "# split the test set\n",
    "log.debug('Splitting test set.')\n",
    "train_sol, test_sol = train_test_split(solutions,\n",
    "                                       test_size=0.10,\n",
    "                                       shuffle=True,\n",
    "                                       random_state=RAND\n",
    "                                      )\n",
    "\n",
    "# assign the corresponding samples\n",
    "df_train = df_analysis[df_analysis['solutions'].isin(train_sol)]\n",
    "df_test  = df_analysis[df_analysis['solutions'].isin(test_sol)]\n",
    "\n",
    "print('Train set: {:d} samples({:.2f}% of the total set).'.\\\n",
    "      format(df_train.shape[0], 100*df_train.shape[0]/df_analysis.shape[0])\n",
    "     )\n",
    "print('Test set:  {:d} samples({:.2f}% of the total set).'.\\\n",
    "      format(df_test.shape[0], 100*df_test.shape[0]/df_analysis.shape[0])\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select a range of sizes of the validation set for which we test a simple linear regression model. We then plot the MSE as a function of the size of the validation set (taken with respect to the training set) and choose a suitable split for the analysis. Since this is still an exploratory computation, we will not pre-process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics      import mean_squared_error\n",
    "\n",
    "# choose a range of validation set sizes\n",
    "dev_sizes = np.linspace(0.10, 0.75, num=14)\n",
    "\n",
    "# prepare lists of errors (training and validation)\n",
    "errors = {'val_size': [], 'error': [], 'type': []}\n",
    "\n",
    "# select training and validation solutions\n",
    "for n in range(dev_sizes.shape[0]):\n",
    "    train_tmp_sol, val_tmp_sol = train_test_split(train_sol,\n",
    "                                                  test_size=dev_sizes[n],\n",
    "                                                  shuffle=False\n",
    "                                                 )\n",
    "\n",
    "    # remember to shuffle the dataset\n",
    "    df_train_tmp = df_train[df_train['solutions'].isin(train_tmp_sol)].\\\n",
    "                    sample(frac=1, random_state=RAND)\n",
    "    df_val_tmp  = df_train[df_train['solutions'].isin(val_tmp_sol)].\\\n",
    "                    sample(frac=1, random_state=RAND)\n",
    "\n",
    "    df_train_tmp_feat = df_train_tmp.drop(columns=['solutions', 'init', 'exp'])\n",
    "    y_tmp_train_true  = df_train_tmp['exp'].values.reshape(-1,)\n",
    "    \n",
    "    df_val_tmp_feat   = df_val_tmp.drop(columns=['solutions', 'init', 'exp'])\n",
    "    y_tmp_val_true    = df_val_tmp['exp'].values.reshape(-1,)\n",
    "\n",
    "    # fit a linear regression model (don't fit the intercept: no meaning!)\n",
    "    model = LinearRegression(fit_intercept=False,\n",
    "                             n_jobs=THREADS).fit(df_train_tmp_feat,\n",
    "                                                 y_tmp_train_true\n",
    "                                                )\n",
    "\n",
    "    # make predictions on train and test sets\n",
    "    y_tmp_train_pred = model.predict(df_train_tmp_feat).reshape(-1,)\n",
    "    y_tmp_val_pred   = model.predict(df_val_tmp_feat).reshape(-1,)\n",
    "\n",
    "    # compute the MSE\n",
    "    errors['error'].append(mean_squared_error(y_tmp_train_true,\n",
    "                                              y_tmp_train_pred\n",
    "                                             )\n",
    "                          )\n",
    "    errors['type'].append('training')\n",
    "    errors['val_size'].append(dev_sizes[n])\n",
    "    \n",
    "    errors['error'].append(mean_squared_error(y_tmp_val_true,\n",
    "                                             y_tmp_val_pred\n",
    "                                            )\n",
    "                         )\n",
    "    errors['type'].append('validation')\n",
    "    errors['val_size'].append(dev_sizes[n])\n",
    "    \n",
    "# save the errors as dataframe\n",
    "errors = pd.DataFrame(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot the results of training and validation errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(1,1)\n",
    "\n",
    "sns.scatterplot(data=errors,\n",
    "                x='val_size',\n",
    "                y='error',\n",
    "                hue='type',\n",
    "                palette=sns.color_palette('bright', 2),\n",
    "                ax=ax\n",
    "               )\n",
    "ax.set(title='MSE as a Function of the Size of the Validation Set',\n",
    "       xlabel='size (w.r.t. training set)',\n",
    "       ylabel='MSE',\n",
    "       yscale='log'\n",
    "      )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('training-validation-errors.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the previous result, we choose to keep around 10% of the training data in the validation set. We select the samples as we did for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training set\n",
    "train_sol, val_sol = train_test_split(train_sol,\n",
    "                                      test_size=0.10,\n",
    "                                      shuffle=False\n",
    "                                     )\n",
    "\n",
    "# assign the corresponding samples (remember to shuffle everything!)\n",
    "train = df_train[df_train['solutions'].isin(train_sol)].\\\n",
    "            sample(frac=1, random_state=RAND)\n",
    "valid = df_train[df_train['solutions'].isin(val_sol)].\\\n",
    "            sample(frac=1, random_state=RAND)\n",
    "test  = df_test.sample(frac=1, random_state=RAND)\n",
    "\n",
    "print('Train set:      {:d} samples({:.2f}% of the total set).'.\\\n",
    "      format(train.shape[0], 100*train.shape[0]/df_analysis.shape[0])\n",
    "     )\n",
    "print('Validation set: {:d} samples({:.2f}% of the total set).'.\\\n",
    "      format(valid.shape[0], 100*valid.shape[0]/df_analysis.shape[0])\n",
    "     )\n",
    "print('Test set:       {:d} samples({:.2f}% of the total set).'.\\\n",
    "      format(test.shape[0], 100*test.shape[0]/df_analysis.shape[0])\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the dataset we use in the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sort_values(ordering_columns, ignore_index=True).\\\n",
    "    to_csv(datpath('sft-train-set.csv'), index=False)\n",
    "valid.sort_values(ordering_columns, ignore_index=True).\\\n",
    "    to_csv(datpath('sft-val-set.csv'), index=False)\n",
    "test.sort_values(ordering_columns, ignore_index=True).\\\n",
    "    to_csv(datpath('sft-test-set.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Before computing the algorithms we pre-process the data using previous results to improve the possible outcome. We use a `RobustScaler` to scale the input accounting for the outliers. We scale only the truncation levels since `weight` is already $\\mathrm{O}(1)$ and `type` is a \"categorical\" (ordered, since clearly $4 > 2$) variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# select only the features for training\n",
    "X_train = train.drop(columns=['solutions', 'init', 'exp']).\\\n",
    "            reset_index(drop=True)\n",
    "X_val   = valid.drop(columns=['solutions', 'init', 'exp']).\\\n",
    "            reset_index(drop=True)\n",
    "X_test  = test.drop(columns=['solutions', 'init', 'exp']).\\\n",
    "            reset_index(drop=True)\n",
    "\n",
    "# take the labels\n",
    "y_train = train['exp'].values.reshape(-1,)\n",
    "y_val   = valid['exp'].values.reshape(-1,)\n",
    "y_test  = test['exp'].values.reshape(-1,)\n",
    "\n",
    "# scale the samples\n",
    "rob_scl = RobustScaler()\n",
    "X_train_transf = pd.DataFrame(rob_scl.\\\n",
    "                    fit_transform(X_train.filter(regex='^level.*$')),\n",
    "                    columns=X_train.filter(regex='^level.*$').columns\n",
    "                             )\n",
    "X_val_transf   = pd.DataFrame(rob_scl.\\\n",
    "                    transform(X_val.filter(regex='^level.*$')),\n",
    "                    columns=X_val.filter(regex='^level.*$').columns\n",
    "                             )\n",
    "X_test_transf  = pd.DataFrame(rob_scl.\\\n",
    "                    transform(X_test.filter(regex='^level.*$')),\n",
    "                    columns=X_test.filter(regex='^level.*$').columns\n",
    "                             )\n",
    "\n",
    "# concatenate the results\n",
    "X_train = pd.concat([X_train[['weight', 'type']], X_train_transf], axis=1)\n",
    "X_val   = pd.concat([X_val[['weight', 'type']], X_val_transf], axis=1)\n",
    "X_test  = pd.concat([X_test[['weight', 'type']], X_test_transf], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the analysis we will print several metrics to evaluate the fit. In particular we will use the MSE, the Mean Absolute Error (MAE) and the $R^2$ score (R2). For the errors (_residuals_) we will also include the computation of the confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy  import stats\n",
    "from typing import Tuple\n",
    "\n",
    "def mse_ci(y_true: float,\n",
    "           y_pred: float,\n",
    "           dof: float,\n",
    "           confidence: float = 0.95) -> Tuple[float, float]:\n",
    "    '''\n",
    "    Compute the confidence interval of the variance.\n",
    "    \n",
    "    Required arguments:\n",
    "        y_true: true values,\n",
    "        y_pred: predictions,\n",
    "        dof:    the no. of degrees of freedom.\n",
    "        \n",
    "    Returns:\n",
    "        the array of lower and upper bounds of the confidence interval.\n",
    "    '''\n",
    "    \n",
    "    # compute the deviation of the data and the squared errors\n",
    "    deviation = y_pred - y_true\n",
    "    sq_errors = deviation ** 2\n",
    "\n",
    "    # compute the confidence intervals\n",
    "    conf_interval = stats.t.interval(confidence,\n",
    "                                     dof,\n",
    "                                     loc   = sq_errors.mean(),\n",
    "                                     scale = stats.sem(sq_errors)\n",
    "                                    )\n",
    "    \n",
    "    return conf_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (Preliminary Study)\n",
    "\n",
    "The first approach to the regression models is the **linear regression**: it can be used to set a baseline for further improvement and can be a practice ground to study the correlations between the coefficients and their impact on the results. In this case the features have been scaled and centred (`RobustScaler` centres the data before transforming): we do not need to account for fitting the intercept since it would have no meaning in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics      import mean_squared_error, \\\n",
    "                                 mean_absolute_error, \\\n",
    "                                 r2_score\n",
    "\n",
    "log.info('Fitting linear regression.')\n",
    "\n",
    "# define the model\n",
    "lin_reg = LinearRegression(fit_intercept=False,\n",
    "                           normalize=False,\n",
    "                           n_jobs=THREADS\n",
    "                          )\n",
    "\n",
    "# fit the model\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# generate predictions\n",
    "y_train_pred = lin_reg.predict(X_train)\n",
    "y_val_pred   = lin_reg.predict(X_val)\n",
    "y_test_pred  = lin_reg.predict(X_test)\n",
    "\n",
    "# define the no. of dof\n",
    "dof = X_train.shape[0] - X_train.shape[1] # i.e. samples - linear coeff.\n",
    "\n",
    "# compute metrics\n",
    "lin_reg_train = {'MSE':    mean_squared_error(y_train, y_train_pred),\n",
    "                 'MSE_CI': mse_ci(y_train, y_train_pred, dof),\n",
    "                 'MAE':    mean_absolute_error(y_train, y_train_pred),\n",
    "                 'R2':     r2_score(y_train, y_train_pred)\n",
    "                }\n",
    "lin_reg_val = {'MSE':    mean_squared_error(y_val, y_val_pred),\n",
    "               'MSE_CI': mse_ci(y_val, y_val_pred, dof),\n",
    "               'MAE':    mean_absolute_error(y_val, y_val_pred),\n",
    "               'R2':     r2_score(y_val, y_val_pred)\n",
    "              }\n",
    "lin_reg_test = {'MSE':    mean_squared_error(y_test, y_test_pred),\n",
    "                'MSE_CI': mse_ci(y_test, y_test_pred, dof),\n",
    "                'MAE':    mean_absolute_error(y_test, y_test_pred),\n",
    "                'R2':     r2_score(y_test, y_test_pred)\n",
    "               }\n",
    "\n",
    "# print the metrics\n",
    "print('LINEAR REGRESSION:\\n\\n',\n",
    "      '  Training set:\\n\\n',\n",
    "      '    MSE: {:.3f}'.format(lin_reg_train['MSE']),\n",
    "      '    95% CI: [{:.3f}, {:.3f}]\\n'.format(lin_reg_train['MSE_CI'][0],\n",
    "                                              lin_reg_train['MSE_CI'][1]\n",
    "                                             ),\n",
    "      '    MAE: {:.3f}\\n'.format(lin_reg_train['MAE']),\n",
    "      '    R2:  {:.3f}\\n\\n'.format(lin_reg_train['R2']),\n",
    "      '  Validation set:\\n\\n',\n",
    "      '    MSE: {:.3f}'.format(lin_reg_val['MSE']),\n",
    "      '    95% CI: [{:.3f}, {:.3f}]\\n'.format(lin_reg_val['MSE_CI'][0],\n",
    "                                              lin_reg_val['MSE_CI'][1]\n",
    "                                             ),\n",
    "      '    MAE: {:.3f}\\n'.format(lin_reg_val['MAE']),\n",
    "      '    R2:  {:.3f}\\n\\n'.format(lin_reg_val['R2']),\n",
    "      '  Test set:\\n\\n',\n",
    "      '    MSE: {:.3f}'.format(lin_reg_test['MSE']),\n",
    "      '    95% CI: [{:.3f}, {:.3f}]\\n'.format(lin_reg_test['MSE_CI'][0],\n",
    "                                              lin_reg_test['MSE_CI'][1]\n",
    "                                             ),\n",
    "      '    MAE: {:.3f}\\n'.format(lin_reg_test['MAE']),\n",
    "      '    R2:  {:.3f}\\n\\n'.format(lin_reg_test['R2'])\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linear model we can try and be more specific as to what coefficient is contributing to the final prediction computing statistics for each coefficient of the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def lin_summary(estimator: sklearn.base.BaseEstimator,\n",
    "                X: pd.Series,\n",
    "                y: np.ndarray) -> pd.DataFrame:\n",
    "    '''\n",
    "    Build a summary of the linear regression.\n",
    "    \n",
    "    Required arguments:\n",
    "        estimator: the linear fit model,\n",
    "        X:         the predictor,\n",
    "        y:         the estimand.\n",
    "        \n",
    "    Returns:\n",
    "        the statistics on the coefficients.\n",
    "    '''\n",
    "    # compute the no. of dof and the coefficients\n",
    "    dof  = X.shape[0] - X.shape[1]\n",
    "    coef = estimator.coef_\n",
    "    \n",
    "    # compute residuals, residual variance and the square of deviations\n",
    "    res = y - estimator.predict(X)\n",
    "    var = np.sum(res**2) / dof\n",
    "    ssx = np.sum(np.square(X.values - np.mean(X.values, axis=0)), axis=0)\n",
    "    \n",
    "    # compute standard error, t coeffcient and p-value that t_obs > |t|\n",
    "    se  = np.sqrt(var / (ssx + 1.0e-6)) #---------- avoid division by zero\n",
    "    t   = coef / (se + 1.0e-6) #------------------- avoid division by zero\n",
    "    p   = 2 * (1.0 - stats.t.cdf(abs(t), dof))\n",
    "    \n",
    "    # compute confidence intervals (two sided)\n",
    "    intervals = stats.t.interval(0.975,\n",
    "                                 dof,\n",
    "                                 loc=coef,\n",
    "                                 scale=se\n",
    "                                )\n",
    "    \n",
    "    # create the dataframe\n",
    "    summary = {'coefficients':          coef,\n",
    "               'standard error':        se,\n",
    "               't statistic':           np.round(t, 3),\n",
    "               'p value (t_obs > |t|)': np.round(p, 3),\n",
    "               '95% CI (lower)':        intervals[0],\n",
    "               '95% CI (upper)':        intervals[1]\n",
    "              }\n",
    "    return pd.DataFrame(summary, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then give a summary of the values of the coefficients given the number of degrees of freedom of the fit (i.e. the difference between the no. of samples and the no. of coefficients, since we are not fitting the intercept). We start from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sum_train = lin_summary(lin_reg, X_train, y_train)\n",
    "lin_reg_sum_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the huge variation of the samples in the set leads to a very precise (possibly **too precise**) determination of the coefficients of the fit. The same happens for the development set (i.e. the **validation set**) where only `type` cannot be assumed to be different from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sum_val = lin_summary(lin_reg, X_val, y_val)\n",
    "lin_reg_sum_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally present the summary of the test set: we use it to plot the contribution of the coefficients, residuals and predictions and histogram of the residuals as opposed to the same statistics for the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sum_test  = lin_summary(lin_reg, X_test, y_test)\n",
    "lin_reg_val_pred  = pd.DataFrame({'real value': y_val.reshape(-1,),\n",
    "                                  'prediction': y_val_pred.reshape(-1,),\n",
    "                                  'residuals':  y_val.reshape(-1,) -\n",
    "                                                y_val_pred.reshape(-1,),\n",
    "                                  'id':         np.arange(y_val.shape[0])\n",
    "                                 }\n",
    "                                )\n",
    "lin_reg_test_pred = pd.DataFrame({'real value': y_test.reshape(-1,),\n",
    "                                  'prediction': y_test_pred.reshape(-1,),\n",
    "                                  'residuals':  y_test.reshape(-1,) -\n",
    "                                                y_test_pred.reshape(-1,),\n",
    "                                  'id':         np.arange(y_test.shape[0])\n",
    "                                 }\n",
    "                                )\n",
    "\n",
    "fig, ax = subplots(2,3)\n",
    "\n",
    "# plot the predictions and the real values\n",
    "sns.lineplot(data=lin_reg_val_pred,\n",
    "             x='id',\n",
    "             y='real value',\n",
    "             label='real value',\n",
    "             color=sns.color_palette('muted', 2)[0],\n",
    "             alpha=0.9,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "sns.lineplot(data=lin_reg_val_pred,\n",
    "             x='id',\n",
    "             y='prediction',\n",
    "             label='prediction',\n",
    "             color=sns.color_palette('muted', 2)[1],\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "ax[0,0].set(title='Real Value and Predictions (LR, Validation Set)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of the errors\n",
    "sns.distplot(lin_reg_val_pred['residuals'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             ax=ax[0,1]\n",
    "            )\n",
    "ax[0,1].set(title='Histogram of Residuals (LR, Validation Set)',\n",
    "            xlabel='residual',\n",
    "            ylabel='#',\n",
    "            xticks=np.round(\\\n",
    "                   np.linspace(np.min(lin_reg_val_pred['residuals']),\n",
    "                               np.max(lin_reg_val_pred['residuals']),\n",
    "                               10\n",
    "                              ),\n",
    "                          2\n",
    "                         )\n",
    "         )\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=lin_reg_test_pred,\n",
    "                x='prediction',\n",
    "                y='residuals',\n",
    "                ax=ax[0,2]\n",
    "               )\n",
    "ax[0,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[0,2].set(title='Distribution of the Residuals (LR, Validation Set)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "\n",
    "# plot the predictions and the real values\n",
    "sns.lineplot(data=lin_reg_test_pred,\n",
    "             x='id',\n",
    "             y='real value',\n",
    "             label='real value',\n",
    "             color=sns.color_palette('muted', 2)[0],\n",
    "             alpha=0.9,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "sns.lineplot(data=lin_reg_test_pred,\n",
    "             x='id',\n",
    "             y='prediction',\n",
    "             label='prediction',\n",
    "             color=sns.color_palette('muted', 2)[1],\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "ax[1,0].set(title='Real Value and Predictions (LR, Test Set)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of the errors\n",
    "sns.distplot(lin_reg_test_pred['residuals'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             ax=ax[1,1]\n",
    "            )\n",
    "ax[1,1].set(title='Histogram of Residuals (LR, Test Set)',\n",
    "            xlabel='residual',\n",
    "            ylabel='#',\n",
    "            xticks=np.round(\\\n",
    "                   np.linspace(np.min(lin_reg_test_pred['residuals']),\n",
    "                               np.max(lin_reg_test_pred['residuals']),\n",
    "                               10\n",
    "                              ),\n",
    "                          2\n",
    "                         )\n",
    "         )\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=lin_reg_test_pred,\n",
    "                x='prediction',\n",
    "                y='residuals',\n",
    "                ax=ax[1,2]\n",
    "               )\n",
    "ax[1,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[1,2].set(title='Distribution of the Residuals (LR, Test Set)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('lin_reg_preliminary.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to a different analysis of the linear regression algorithm. Specifically we investigate the possibility of dividing the `weight` variable in high and low weights and look at the summary of the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Weight Discrimination (Preliminary)\n",
    "\n",
    "In this section we consider the linear regression analysis dividing the `weight` variable in two parts according to its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_low  = X_train[X_train['weight'] < 1.5]\n",
    "X_train_high = X_train[X_train['weight'] >= 1.5]\n",
    "X_val_low    = X_val[X_val['weight'] < 1.5]\n",
    "X_val_high   = X_val[X_val['weight'] >= 1.5]\n",
    "X_test_low   = X_test[X_test['weight'] < 1.5]\n",
    "X_test_high  = X_test[X_test['weight'] >= 1.5]\n",
    "\n",
    "y_train_low  = y_train[X_train_low.index]\n",
    "y_train_high = y_train[X_train_high.index]\n",
    "y_val_low    = y_val[X_val_low.index]\n",
    "y_val_high   = y_val[X_val_high.index]\n",
    "y_test_low   = y_test[X_test_low.index]\n",
    "y_test_high  = y_test[X_test_high.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed as before to fit the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics      import mean_squared_error, \\\n",
    "                                 mean_absolute_error, \\\n",
    "                                 r2_score\n",
    "\n",
    "log.info('Fitting linear regression with weight discrimination.')\n",
    "\n",
    "# define the model\n",
    "lin_reg_low  = LinearRegression(fit_intercept=False,\n",
    "                                normalize=False,\n",
    "                                n_jobs=THREADS\n",
    "                               )\n",
    "lin_reg_high = LinearRegression(fit_intercept=False,\n",
    "                                normalize=False,\n",
    "                                n_jobs=THREADS\n",
    "                               )\n",
    "\n",
    "# fit the model\n",
    "lin_reg_low.fit(X_train_low, y_train_low)\n",
    "lin_reg_high.fit(X_train_high, y_train_high)\n",
    "\n",
    "# generate predictions\n",
    "y_train_pred_low  = lin_reg_low.predict(X_train_low)\n",
    "y_val_pred_low    = lin_reg_low.predict(X_val_low)\n",
    "y_test_pred_low   = lin_reg_low.predict(X_test_low)\n",
    "\n",
    "y_train_pred_high = lin_reg_high.predict(X_train_high)\n",
    "y_val_pred_high   = lin_reg_high.predict(X_val_high)\n",
    "y_test_pred_high  = lin_reg_high.predict(X_test_high)\n",
    "\n",
    "# define the no. of dof\n",
    "dof_low  = X_train_low.shape[0] - X_train_low.shape[1]\n",
    "dof_high = X_train_high.shape[0] - X_train_high.shape[1]\n",
    "\n",
    "# compute metrics\n",
    "lin_reg_low_train = {'MSE':    mean_squared_error(y_train_low,\n",
    "                                                  y_train_pred_low\n",
    "                                                 ),\n",
    "                     'MSE_CI': mse_ci(y_train_low,\n",
    "                                      y_train_pred_low,\n",
    "                                      dof_low\n",
    "                                     ),\n",
    "                     'MAE':    mean_absolute_error(y_train_low,\n",
    "                                                   y_train_pred_low\n",
    "                                                  ),\n",
    "                     'R2':     r2_score(y_train_low,\n",
    "                                        y_train_pred_low\n",
    "                                       )\n",
    "                    }\n",
    "lin_reg_low_val = {'MSE':    mean_squared_error(y_val_low,\n",
    "                                                y_val_pred_low\n",
    "                                               ),\n",
    "                   'MSE_CI': mse_ci(y_val_low,\n",
    "                                    y_val_pred_low,\n",
    "                                    dof_low\n",
    "                                   ),\n",
    "                   'MAE':    mean_absolute_error(y_val_low,\n",
    "                                                 y_val_pred_low\n",
    "                                                ),\n",
    "                   'R2':     r2_score(y_val_low,\n",
    "                                      y_val_pred_low\n",
    "                                     )\n",
    "                  }\n",
    "lin_reg_low_test = {'MSE':    mean_squared_error(y_test_low,\n",
    "                                                 y_test_pred_low\n",
    "                                                ),\n",
    "                    'MSE_CI': mse_ci(y_test_low,\n",
    "                                     y_test_pred_low,\n",
    "                                     dof_low\n",
    "                                    ),\n",
    "                    'MAE':    mean_absolute_error(y_test_low,\n",
    "                                                  y_test_pred_low\n",
    "                                                 ),\n",
    "                    'R2':     r2_score(y_test_low,\n",
    "                                       y_test_pred_low\n",
    "                                      )\n",
    "                   }\n",
    "\n",
    "lin_reg_high_train = {'MSE':    mean_squared_error(y_train_high,\n",
    "                                                   y_train_pred_high\n",
    "                                                  ),\n",
    "                      'MSE_CI': mse_ci(y_train_high,\n",
    "                                       y_train_pred_high,\n",
    "                                       dof_high\n",
    "                                      ),\n",
    "                      'MAE':    mean_absolute_error(y_train_high,\n",
    "                                                    y_train_pred_high\n",
    "                                                   ),\n",
    "                      'R2':     r2_score(y_train_high,\n",
    "                                         y_train_pred_high\n",
    "                                        )\n",
    "                     }\n",
    "lin_reg_high_val = {'MSE':    mean_squared_error(y_val_high,\n",
    "                                                 y_val_pred_high\n",
    "                                                ),\n",
    "                    'MSE_CI': mse_ci(y_val_high,\n",
    "                                     y_val_pred_high,\n",
    "                                     dof_high\n",
    "                                    ),\n",
    "                    'MAE':    mean_absolute_error(y_val_high,\n",
    "                                                  y_val_pred_high\n",
    "                                                 ),\n",
    "                    'R2':     r2_score(y_val_high,\n",
    "                                       y_val_pred_high\n",
    "                                      )\n",
    "                   }\n",
    "lin_reg_high_test = {'MSE':    mean_squared_error(y_test_high,\n",
    "                                                  y_test_pred_high\n",
    "                                                 ),\n",
    "                     'MSE_CI': mse_ci(y_test_high,\n",
    "                                      y_test_pred_high,\n",
    "                                      dof_high\n",
    "                                     ),\n",
    "                     'MAE':    mean_absolute_error(y_test_high,\n",
    "                                                   y_test_pred_high\n",
    "                                                  ),\n",
    "                     'R2':     r2_score(y_test_high,\n",
    "                                        y_test_pred_high\n",
    "                                       )\n",
    "                    }\n",
    "\n",
    "# print the metrics\n",
    "print('LINEAR REGRESSION (WEIGHT < 1.5):\\n\\n',\n",
    "      '  Training set:\\n\\n',\n",
    "      '    MSE: {:.3f}'.format(lin_reg_low_train['MSE']),\n",
    "      '    95% CI: [{:.3f}, {:.3f}]\\n'.format(lin_reg_low_train['MSE_CI'][0],\n",
    "                                              lin_reg_low_train['MSE_CI'][1]\n",
    "                                             ),\n",
    "      '    MAE: {:.3f}\\n'.format(lin_reg_low_train['MAE']),\n",
    "      '    R2:  {:.3f}\\n\\n'.format(lin_reg_low_train['R2']),\n",
    "      '  Validation set:\\n\\n',\n",
    "      '    MSE: {:.3f}'.format(lin_reg_low_val['MSE']),\n",
    "      '    95% CI: [{:.3f}, {:.3f}]\\n'.format(lin_reg_low_val['MSE_CI'][0],\n",
    "                                              lin_reg_low_val['MSE_CI'][1]\n",
    "                                             ),\n",
    "      '    MAE: {:.3f}\\n'.format(lin_reg_low_val['MAE']),\n",
    "      '    R2:  {:.3f}\\n\\n'.format(lin_reg_low_val['R2']),\n",
    "      '  Test set:\\n\\n',\n",
    "      '    MSE: {:.3f}'.format(lin_reg_low_test['MSE']),\n",
    "      '    95% CI: [{:.3f}, {:.3f}]\\n'.format(lin_reg_low_test['MSE_CI'][0],\n",
    "                                              lin_reg_low_test['MSE_CI'][1]\n",
    "                                             ),\n",
    "      '    MAE: {:.3f}\\n'.format(lin_reg_low_test['MAE']),\n",
    "      '    R2:  {:.3f}\\n\\n'.format(lin_reg_low_test['R2'])\n",
    "     )\n",
    "\n",
    "print('LINEAR REGRESSION (WEIGHT ≥ 1.5):\\n\\n',\n",
    "      '  Training set:\\n\\n',\n",
    "      '    MSE: {:.3f}'.format(lin_reg_high_train['MSE']),\n",
    "      '    95% CI: [{:.3f}, {:.3f}]\\n'.format(lin_reg_high_train['MSE_CI'][0],\n",
    "                                              lin_reg_high_train['MSE_CI'][1]\n",
    "                                             ),\n",
    "      '    MAE: {:.3f}\\n'.format(lin_reg_high_train['MAE']),\n",
    "      '    R2:  {:.3f}\\n\\n'.format(lin_reg_high_train['R2']),\n",
    "      '  Validation set:\\n\\n',\n",
    "      '    MSE: {:.3f}'.format(lin_reg_high_val['MSE']),\n",
    "      '    95% CI: [{:.3f}, {:.3f}]\\n'.format(lin_reg_high_val['MSE_CI'][0],\n",
    "                                              lin_reg_high_val['MSE_CI'][1]\n",
    "                                             ),\n",
    "      '    MAE: {:.3f}\\n'.format(lin_reg_high_val['MAE']),\n",
    "      '    R2:  {:.3f}\\n\\n'.format(lin_reg_high_val['R2']),\n",
    "      '  Test set:\\n\\n',\n",
    "      '    MSE: {:.3f}'.format(lin_reg_high_test['MSE']),\n",
    "      '    95% CI: [{:.3f}, {:.3f}]\\n'.format(lin_reg_high_test['MSE_CI'][0],\n",
    "                                              lin_reg_high_test['MSE_CI'][1]\n",
    "                                             ),\n",
    "      '    MAE: {:.3f}\\n'.format(lin_reg_high_test['MAE']),\n",
    "      '    R2:  {:.3f}\\n\\n'.format(lin_reg_high_test['R2'])\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately notice an improvement in the predictions when splitting the dataset. In fact both MSE and R2 scores are definitely better than before. As a summary we can then take a look at the summary of the regression analysis starting from the training set for `weight` $< 1.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sum_train_low = lin_summary(lin_reg_low, X_train_low, y_train_low)\n",
    "lin_reg_sum_train_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from before, all coefficients seem to be relevant for the fit. In the development set however we see that there are still two contributions which may reasonably vanish (`type` in particular seem to have the same behaviour as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sum_val_low = lin_summary(lin_reg_low, X_val_low, y_val_low)\n",
    "lin_reg_sum_val_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then look at the summary of the test set to plot prediction and residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sum_test_low  = lin_summary(lin_reg_low, X_test_low, y_test_low)\n",
    "lin_reg_val_pred_low  = pd.DataFrame(\\\n",
    "                                {'real value': y_val_low.reshape(-1,),\n",
    "                                 'prediction': y_val_pred_low.reshape(-1,),\n",
    "                                 'residuals':  y_val_low.reshape(-1,) -\n",
    "                                               y_val_pred_low.reshape(-1,),\n",
    "                                 'id':         np.arange(y_val_low.shape[0])\n",
    "                                }\n",
    "                                    )\n",
    "lin_reg_test_pred_low = pd.DataFrame(\\\n",
    "                                {'real value': y_test_low.reshape(-1,),\n",
    "                                 'prediction': y_test_pred_low.reshape(-1,),\n",
    "                                 'residuals':  y_test_low.reshape(-1,) -\n",
    "                                               y_test_pred_low.reshape(-1,),\n",
    "                                 'id':         np.arange(y_test_low.shape[0])\n",
    "                                }\n",
    "                                    )\n",
    "\n",
    "fig, ax = subplots(2,3)\n",
    "\n",
    "# plot the predictions and the real values\n",
    "sns.lineplot(data=lin_reg_val_pred_low,\n",
    "             x='id',\n",
    "             y='real value',\n",
    "             label='real value',\n",
    "             color=sns.color_palette('muted', 2)[0],\n",
    "             alpha=0.9,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "sns.lineplot(data=lin_reg_val_pred_low,\n",
    "             x='id',\n",
    "             y='prediction',\n",
    "             label='prediction',\n",
    "             color=sns.color_palette('muted', 2)[1],\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "ax[0,0].set(title='Real Value and Predictions (LR, Weight < 1.5, Val. Set)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of the errors\n",
    "sns.distplot(lin_reg_val_pred_low['residuals'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             ax=ax[0,1]\n",
    "            )\n",
    "ax[0,1].set(title='Histogram of Residuals (LR, Weight < 1.5, Val. Set)',\n",
    "            xlabel='residual',\n",
    "            ylabel='#',\n",
    "            xticks=np.round(\\\n",
    "                   np.linspace(np.min(lin_reg_val_pred_low['residuals']),\n",
    "                               np.max(lin_reg_val_pred_low['residuals']),\n",
    "                               10\n",
    "                              ),\n",
    "                          2\n",
    "                         )\n",
    "         )\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=lin_reg_test_pred_low,\n",
    "                x='prediction',\n",
    "                y='residuals',\n",
    "                ax=ax[0,2]\n",
    "               )\n",
    "ax[0,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[0,2].set(title='Distrib. of the Residuals (LR, Weight < 1.5, Val. Set)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "\n",
    "# plot the predictions and the real values\n",
    "sns.lineplot(data=lin_reg_test_pred_low,\n",
    "             x='id',\n",
    "             y='real value',\n",
    "             label='real value',\n",
    "             color=sns.color_palette('muted', 2)[0],\n",
    "             alpha=0.9,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "sns.lineplot(data=lin_reg_test_pred_low,\n",
    "             x='id',\n",
    "             y='prediction',\n",
    "             label='prediction',\n",
    "             color=sns.color_palette('muted', 2)[1],\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "ax[1,0].set(title='Real Value and Predictions (LR, Weight < 1.5, Test Set)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of the errors\n",
    "sns.distplot(lin_reg_test_pred_low['residuals'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             ax=ax[1,1]\n",
    "            )\n",
    "ax[1,1].set(title='Histogram of Residuals (LR, Weight < 1.5, Test Set)',\n",
    "            xlabel='residual',\n",
    "            ylabel='#',\n",
    "            xticks=np.round(\\\n",
    "                   np.linspace(np.min(lin_reg_test_pred_low['residuals']),\n",
    "                               np.max(lin_reg_test_pred_low['residuals']),\n",
    "                               10\n",
    "                              ),\n",
    "                          2\n",
    "                         )\n",
    "         )\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=lin_reg_test_pred_low,\n",
    "                x='prediction',\n",
    "                y='residuals',\n",
    "                ax=ax[1,2]\n",
    "               )\n",
    "ax[1,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[1,2].set(title='Distrib. of the Residuals (LR, Weight < 1.5, Test Set)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('lin_reg_low_preliminary.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we look at the summaries when `weight` $\\ge 1.5$, starting from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sum_train_high = lin_summary(lin_reg_high, X_train_high, y_train_high)\n",
    "lin_reg_sum_train_high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately we also take a look at the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sum_val_high = lin_summary(lin_reg_high, X_val_high, y_val_high)\n",
    "lin_reg_sum_val_high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually we use the test set to plot errors, predictions and distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sum_test_high  = lin_summary(lin_reg_high, X_test_high, y_test_high)\n",
    "lin_reg_val_pred_high  = pd.DataFrame(\\\n",
    "                                {'real value': y_val_high.reshape(-1,),\n",
    "                                 'prediction': y_val_pred_high.reshape(-1,),\n",
    "                                 'residuals':  y_val_high.reshape(-1,) -\n",
    "                                               y_val_pred_high.reshape(-1,),\n",
    "                                 'id':         np.arange(y_val_high.shape[0])\n",
    "                                }\n",
    "                                    )\n",
    "lin_reg_test_pred_high = pd.DataFrame(\\\n",
    "                                {'real value': y_test_high.reshape(-1,),\n",
    "                                 'prediction': y_test_pred_high.reshape(-1,),\n",
    "                                 'residuals':  y_test_high.reshape(-1,) -\n",
    "                                               y_test_pred_high.reshape(-1,),\n",
    "                                 'id':         np.arange(y_test_high.shape[0])\n",
    "                                }\n",
    "                                    )\n",
    "\n",
    "fig, ax = subplots(2,3)\n",
    "\n",
    "# plot the predictions and the real values\n",
    "sns.lineplot(data=lin_reg_val_pred_high,\n",
    "             x='id',\n",
    "             y='real value',\n",
    "             label='real value',\n",
    "             color=sns.color_palette('muted', 2)[0],\n",
    "             alpha=0.9,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "sns.lineplot(data=lin_reg_val_pred_high,\n",
    "             x='id',\n",
    "             y='prediction',\n",
    "             label='prediction',\n",
    "             color=sns.color_palette('muted', 2)[1],\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "ax[0,0].set(title='Real Value and Predictions (LR, Weight ≥ 1.5, Val. Set)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of the errors\n",
    "sns.distplot(lin_reg_val_pred_high['residuals'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             ax=ax[0,1]\n",
    "            )\n",
    "ax[0,1].set(title='Histogram of Residuals (LR, Weight ≥ 1.5, Val. Set)',\n",
    "            xlabel='residual',\n",
    "            ylabel='#',\n",
    "            xticks=np.round(\\\n",
    "                   np.linspace(np.min(lin_reg_val_pred_high['residuals']),\n",
    "                               np.max(lin_reg_val_pred_high['residuals']),\n",
    "                               10\n",
    "                              ),\n",
    "                          2\n",
    "                         )\n",
    "         )\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=lin_reg_test_pred_high,\n",
    "                x='prediction',\n",
    "                y='residuals',\n",
    "                ax=ax[0,2]\n",
    "               )\n",
    "ax[0,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[0,2].set(title='Distrib. of the Residuals (LR, Weight ≥ 1.5, Val. Set)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "\n",
    "# plot the predictions and the real values\n",
    "sns.lineplot(data=lin_reg_test_pred_high,\n",
    "             x='id',\n",
    "             y='real value',\n",
    "             label='real value',\n",
    "             color=sns.color_palette('muted', 2)[0],\n",
    "             alpha=0.9,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "sns.lineplot(data=lin_reg_test_pred_high,\n",
    "             x='id',\n",
    "             y='prediction',\n",
    "             label='prediction',\n",
    "             color=sns.color_palette('muted', 2)[1],\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "ax[1,0].set(title='Real Value and Predictions (LR, Weight ≥ 1.5, Test Set)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of the errors\n",
    "sns.distplot(lin_reg_test_pred_high['residuals'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             ax=ax[1,1]\n",
    "            )\n",
    "ax[1,1].set(title='Histogram of Residuals (LR, Weight ≥ 1.5, Test Set)',\n",
    "            xlabel='residual',\n",
    "            ylabel='#',\n",
    "            xticks=np.round(\\\n",
    "                   np.linspace(np.min(lin_reg_test_pred_high['residuals']),\n",
    "                               np.max(lin_reg_test_pred_high['residuals']),\n",
    "                               10\n",
    "                              ),\n",
    "                          2\n",
    "                         )\n",
    "         )\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=lin_reg_test_pred_high,\n",
    "                x='prediction',\n",
    "                y='residuals',\n",
    "                ax=ax[1,2]\n",
    "               )\n",
    "ax[1,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[1,2].set(title='Distrib. of the Residuals (LR, Weight ≥ 1.5, Test Set)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('lin_reg_high_preliminary.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Removing Categorical Variables (Preliminary)\n",
    "\n",
    "We finally study the possibility to make accurate predictions removing the categorical variable which proved to be quite problematic in most of the previous analysis. We therefore remove the column `type` from the train/validation/test splits and train a linear model. We then analyse the situation as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX_train = X_train.drop(columns='type')\n",
    "newX_val   = X_val.drop(columns='type')\n",
    "newX_test  = X_test.drop(columns='type')\n",
    "\n",
    "log.info('Fitting linear regression w/o categorical variables.')\n",
    "\n",
    "# define the model\n",
    "lin_reg_nocat = LinearRegression(fit_intercept=False,\n",
    "                                 normalize=False,\n",
    "                                 n_jobs=THREADS\n",
    "                                )\n",
    "\n",
    "# fit the model\n",
    "lin_reg_nocat.fit(newX_train, y_train)\n",
    "\n",
    "# generate predictions\n",
    "y_train_pred_nocat = lin_reg_nocat.predict(newX_train)\n",
    "y_val_pred_nocat   = lin_reg_nocat.predict(newX_val)\n",
    "y_test_pred_nocat  = lin_reg_nocat.predict(newX_test)\n",
    "\n",
    "# define the no. of dof\n",
    "dof_nocat = newX_train.shape[0] - newX_train.shape[1] # i.e. samples - coeff.\n",
    "\n",
    "# compute metrics\n",
    "lin_reg_nocat_train = {'MSE':    mean_squared_error(y_train,\n",
    "                                                    y_train_pred_nocat\n",
    "                                                   ),\n",
    "                       'MSE_CI': mse_ci(y_train,\n",
    "                                        y_train_pred_nocat,\n",
    "                                        dof_nocat\n",
    "                                       ),\n",
    "                       'MAE':    mean_absolute_error(y_train,\n",
    "                                                     y_train_pred_nocat\n",
    "                                                    ),\n",
    "                       'R2':     r2_score(y_train,\n",
    "                                          y_train_pred_nocat\n",
    "                                         )\n",
    "                      }\n",
    "lin_reg_nocat_val = {'MSE':    mean_squared_error(y_val,\n",
    "                                                  \n",
    "                                                  y_val_pred_nocat\n",
    "                                                 \n",
    "                                                 ),\n",
    "                     'MSE_CI': mse_ci(y_val,\n",
    "                                      \n",
    "                                      y_val_pred_nocat,\n",
    "                                      \n",
    "                                      dof_nocat\n",
    "                                     \n",
    "                                     ),\n",
    "                     'MAE':    mean_absolute_error(y_val,\n",
    "                                                   \n",
    "                                                   y_val_pred_nocat\n",
    "                                                  \n",
    "                                                  ),\n",
    "                     'R2':     r2_score(y_val,\n",
    "                                        \n",
    "                                        y_val_pred_nocat\n",
    "                                       \n",
    "                                       )\n",
    "                    }\n",
    "lin_reg_nocat_test = {'MSE':    mean_squared_error(y_test,\n",
    "                                                   y_test_pred_nocat\n",
    "                                                  ),\n",
    "                      'MSE_CI': mse_ci(y_test,\n",
    "                                       y_test_pred_nocat,\n",
    "                                       dof_nocat\n",
    "                                      ),\n",
    "                      'MAE':    mean_absolute_error(y_test,\n",
    "                                                    y_test_pred_nocat\n",
    "                                                   ),\n",
    "                      'R2':     r2_score(y_test,\n",
    "                                         y_test_pred_nocat\n",
    "                                        )\n",
    "                     }\n",
    "\n",
    "# print the metrics\n",
    "print('LINEAR REGRESSION (w/o CAT. VARIABLES):\\n\\n',\n",
    "      '  Training set:\\n\\n',\n",
    "      '    MSE: {:.3f}'.format(lin_reg_nocat_train['MSE']),\n",
    "      '    95% CI: [{:.3f}, {:.3f}]\\n'.\\\n",
    "                                    format(lin_reg_nocat_train['MSE_CI'][0],\n",
    "                                           lin_reg_nocat_train['MSE_CI'][1]\n",
    "                                          ),\n",
    "      '    MAE: {:.3f}\\n'.format(lin_reg_nocat_train['MAE']),\n",
    "      '    R2:  {:.3f}\\n\\n'.format(lin_reg_nocat_train['R2']),\n",
    "      '  Validation set:\\n\\n',\n",
    "      '    MSE: {:.3f}'.format(lin_reg_nocat_val['MSE']),\n",
    "      '    95% CI: [{:.3f}, {:.3f}]\\n'.format(lin_reg_nocat_val['MSE_CI'][0],\n",
    "                                              lin_reg_nocat_val['MSE_CI'][1]\n",
    "                                             ),\n",
    "      '    MAE: {:.3f}\\n'.format(lin_reg_nocat_val['MAE']),\n",
    "      '    R2:  {:.3f}\\n\\n'.format(lin_reg_nocat_val['R2']),\n",
    "      '  Test set:\\n\\n',\n",
    "      '    MSE: {:.3f}'.format(lin_reg_nocat_test['MSE']),\n",
    "      '    95% CI: [{:.3f}, {:.3f}]\\n'.format(lin_reg_nocat_test['MSE_CI'][0],\n",
    "                                              lin_reg_nocat_test['MSE_CI'][1]\n",
    "                                             ),\n",
    "      '    MAE: {:.3f}\\n'.format(lin_reg_nocat_test['MAE']),\n",
    "      '    R2:  {:.3f}\\n\\n'.format(lin_reg_nocat_test['R2'])\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then first show the summary of the fit on the training set to better understand the impact of the categorical variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sum_nocat_train = lin_summary(lin_reg_nocat, newX_train, y_train)\n",
    "lin_reg_sum_nocat_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the coefficients in this case are all clearly different from zero with a very large confidence. We then move to the validation set for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sum_nocat_val = lin_summary(lin_reg_nocat, newX_val, y_val)\n",
    "lin_reg_sum_nocat_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we find that there is at least another coefficient which cannot be assumed to differ from 0 with confidence. In the end it seems that the categorical variable may actually help or, at worst, be completely irrelevant for the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we consider the test set to plot residuals and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sum_nocat_test  = lin_summary(lin_reg_nocat, newX_test, y_test)\n",
    "lin_reg_val_pred_nocat  = pd.DataFrame({'real value': y_val.\\\n",
    "                                                        reshape(-1,),\n",
    "                                        'prediction': y_val_pred_nocat.\\\n",
    "                                                        reshape(-1,),\n",
    "                                        'residuals':  y_val.\\\n",
    "                                                        reshape(-1,) -\n",
    "                                                      y_val_pred_nocat.\\\n",
    "                                                        reshape(-1,),\n",
    "                                        'id':         np.arange(y_val.\\\n",
    "                                                        shape[0])\n",
    "                                 }\n",
    "                                )\n",
    "lin_reg_test_pred_nocat = pd.DataFrame({'real value': y_test.\\\n",
    "                                                        reshape(-1,),\n",
    "                                        'prediction': y_test_pred_nocat.\\\n",
    "                                                        reshape(-1,),\n",
    "                                        'residuals':  y_test.\\\n",
    "                                                        reshape(-1,) -\n",
    "                                                      y_test_pred_nocat.\\\n",
    "                                                        reshape(-1,),\n",
    "                                        'id':         np.arange(y_test.\\\n",
    "                                                        shape[0])\n",
    "                                 }\n",
    "                                )\n",
    "\n",
    "fig, ax = subplots(2,3)\n",
    "\n",
    "# plot the predictions and the real values\n",
    "sns.lineplot(data=lin_reg_val_pred_nocat,\n",
    "             x='id',\n",
    "             y='real value',\n",
    "             label='real value',\n",
    "             color=sns.color_palette('muted', 2)[0],\n",
    "             alpha=0.9,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "sns.lineplot(data=lin_reg_val_pred_nocat,\n",
    "             x='id',\n",
    "             y='prediction',\n",
    "             label='prediction',\n",
    "             color=sns.color_palette('muted', 2)[1],\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "ax[0,0].set(title='Real Value and Predictions (LR, Validation Set)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of the errors\n",
    "sns.distplot(lin_reg_val_pred_nocat['residuals'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             ax=ax[0,1]\n",
    "            )\n",
    "ax[0,1].set(title='Histogram of Residuals (LR, Validation Set)',\n",
    "            xlabel='residual',\n",
    "            ylabel='#',\n",
    "            xticks=np.round(\\\n",
    "                   np.linspace(np.min(lin_reg_val_pred_nocat['residuals']),\n",
    "                               np.max(lin_reg_val_pred_nocat['residuals']),\n",
    "                               10\n",
    "                              ),\n",
    "                          2\n",
    "                         )\n",
    "         )\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=lin_reg_test_pred_nocat,\n",
    "                x='prediction',\n",
    "                y='residuals',\n",
    "                ax=ax[0,2]\n",
    "               )\n",
    "ax[0,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[0,2].set(title='Distribution of the Residuals (LR, Validation Set)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "\n",
    "# plot the predictions and the real values\n",
    "sns.lineplot(data=lin_reg_test_pred_nocat,\n",
    "             x='id',\n",
    "             y='real value',\n",
    "             label='real value',\n",
    "             color=sns.color_palette('muted', 2)[0],\n",
    "             alpha=0.9,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "sns.lineplot(data=lin_reg_test_pred_nocat,\n",
    "             x='id',\n",
    "             y='prediction',\n",
    "             label='prediction',\n",
    "             color=sns.color_palette('muted', 2)[1],\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "ax[1,0].set(title='Real Value and Predictions (LR, Test Set)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of the errors\n",
    "sns.distplot(lin_reg_test_pred_nocat['residuals'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             ax=ax[1,1]\n",
    "            )\n",
    "ax[1,1].set(title='Histogram of Residuals (LR, Test Set)',\n",
    "            xlabel='residual',\n",
    "            ylabel='#',\n",
    "            xticks=np.round(\\\n",
    "                   np.linspace(np.min(lin_reg_test_pred_nocat['residuals']),\n",
    "                               np.max(lin_reg_test_pred_nocat['residuals']),\n",
    "                               10\n",
    "                              ),\n",
    "                          2\n",
    "                         )\n",
    "         )\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=lin_reg_test_pred_nocat,\n",
    "                x='prediction',\n",
    "                y='residuals',\n",
    "                ax=ax[1,2]\n",
    "               )\n",
    "ax[1,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[1,2].set(title='Distribution of the Residuals (LR, Test Set)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('lin_reg_nocat_preliminary.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Analysis\n",
    "\n",
    "We can now finally move to the ML analysis. From the previous study we learnt that we may be able to draw significant predictions from the dataset: the best strategy for the moment might be to take the full dataset and try to adjust the algorithm to improve the predictions (we may think about split the algorithm into higher and lower `weight` in a different analysis.\n",
    "\n",
    "We will investigate several possible algorithms:\n",
    "\n",
    "- **linear models** such as (simple) linear regression (LR), _elastic net_ (EN) to add L1 and L2 regularisation, _lasso_ for L1 regularisation, _ridge_ for L2 regularisation,\n",
    "- **SVM** algorithms such as _linear SVR_ (l-SVR), SVR with _Gaussian kernel_ (r-SVR, from _rbf_, _radial basis function_),\n",
    "- **decision trees** based algorithms such as _random forests_ (RF) and _gradient boosted decision trees_ (GBDT),\n",
    "- **artificial neural networks** (ANN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most algorithms we perform hyperparameter optimisation using Bayesan optimisation of MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "In this first scenario, we do not have any hyperparameter to tune and can therefore proceed with simple training and inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics      import mean_squared_error, \\\n",
    "                                 mean_absolute_error, \\\n",
    "                                 r2_score\n",
    "\n",
    "log.info('Fitting linear regression.')\n",
    "\n",
    "# define the model\n",
    "lin_reg = LinearRegression(fit_intercept=False,\n",
    "                           normalize=False,\n",
    "                           n_jobs=THREADS\n",
    "                          )\n",
    "\n",
    "# fit the model\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# compute predictions\n",
    "lin_reg_train_pred = lin_reg.predict(X_train).reshape(-1,)\n",
    "lin_reg_val_pred   = lin_reg.predict(X_val).reshape(-1,)\n",
    "lin_reg_test_pred  = lin_reg.predict(X_test).reshape(-1,)\n",
    "\n",
    "# compute the no. of dofs\n",
    "dof_train = X_train.shape[0] - X_train.shape[1]\n",
    "dof_val   = X_val.shape[0] - X_val.shape[1]\n",
    "dof_test  = X_test.shape[0] - X_test.shape[1]\n",
    "\n",
    "# store metrics\n",
    "lin_reg_metrics = {'tr_mse':    mean_squared_error(y_train,\n",
    "                                                   lin_reg_train_pred\n",
    "                                                  ),\n",
    "                   'tr_ci_low': mse_ci(y_train,\n",
    "                                       lin_reg_train_pred,\n",
    "                                       dof_train\n",
    "                                      )[0],\n",
    "                   'tr_ci_upp': mse_ci(y_train,\n",
    "                                       lin_reg_train_pred,\n",
    "                                       dof_train\n",
    "                                      )[1],\n",
    "                   'tr_mae':    mean_absolute_error(y_train,\n",
    "                                                    lin_reg_train_pred\n",
    "                                                   ),\n",
    "                   'tr_rsq':    r2_score(y_train,\n",
    "                                         lin_reg_train_pred\n",
    "                                        ),\n",
    "                   'v_mse':     mean_squared_error(y_val,\n",
    "                                                   lin_reg_val_pred\n",
    "                                                  ),\n",
    "                   'v_ci_low':  mse_ci(y_val,\n",
    "                                       lin_reg_val_pred,\n",
    "                                       dof_val\n",
    "                                      )[0],\n",
    "                   'v_ci_upp':  mse_ci(y_val,\n",
    "                                       lin_reg_val_pred,\n",
    "                                       dof_val\n",
    "                                      )[1],\n",
    "                   'v_mae':     mean_absolute_error(y_val,\n",
    "                                                    lin_reg_val_pred\n",
    "                                                   ),\n",
    "                   'v_rsq':     r2_score(y_val,\n",
    "                                         lin_reg_val_pred\n",
    "                                        ),\n",
    "                   'te_mse':    mean_squared_error(y_test,\n",
    "                                                   lin_reg_test_pred\n",
    "                                                  ),\n",
    "                   'te_ci_low': mse_ci(y_test,\n",
    "                                       lin_reg_test_pred,\n",
    "                                       dof_test\n",
    "                                      )[0],\n",
    "                   'te_ci_upp': mse_ci(y_test,\n",
    "                                       lin_reg_test_pred,\n",
    "                                       dof_test\n",
    "                                      )[1],\n",
    "                   'te_mae':    mean_absolute_error(y_test,\n",
    "                                                    lin_reg_test_pred\n",
    "                                                   ),\n",
    "                   'te_rsq':    r2_score(y_test,\n",
    "                                         lin_reg_test_pred\n",
    "                                        ),\n",
    "                  }\n",
    "lin_reg_metrics = pd.DataFrame(lin_reg_metrics, index=['lin_reg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then print the metrics associated to the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\\\n",
    "'LINEAR REGRESSION:\\n\\n',\n",
    "'  Training set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(lin_reg_metrics['tr_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(lin_reg_metrics['tr_ci_low'].squeeze(),\n",
    "                                       lin_reg_metrics['tr_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(lin_reg_metrics['tr_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(lin_reg_metrics['tr_rsq'].squeeze()),\n",
    "'  Validation set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(lin_reg_metrics['v_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(lin_reg_metrics['v_ci_low'].squeeze(),\n",
    "                                       lin_reg_metrics['v_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(lin_reg_metrics['v_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(lin_reg_metrics['v_rsq'].squeeze()),\n",
    "'  Test set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(lin_reg_metrics['te_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(lin_reg_metrics['te_ci_low'].squeeze(),\n",
    "                                       lin_reg_metrics['te_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(lin_reg_metrics['te_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(lin_reg_metrics['te_rsq'].squeeze())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then prepare lists of the predictions made using the estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a database for training predictions\n",
    "ml_train_predictions = train[['solutions',\n",
    "                              'init',\n",
    "                              'weight',\n",
    "                              'type',\n",
    "                              'exp']].copy()\n",
    "ml_train_predictions = ml_train_predictions.reset_index(drop=True)\n",
    "ml_train_predictions['lr']     = lin_reg_train_pred\n",
    "ml_train_predictions['lr_res'] = y_train - lin_reg_train_pred\n",
    "\n",
    "# prepare a database for validation predictions\n",
    "ml_val_predictions = valid[['solutions',\n",
    "                            'init',\n",
    "                            'weight',\n",
    "                            'type',\n",
    "                            'exp']].copy()\n",
    "ml_val_predictions = ml_val_predictions.reset_index(drop=True)\n",
    "ml_val_predictions['lr']     = lin_reg_val_pred\n",
    "ml_val_predictions['lr_res'] = y_val - lin_reg_val_pred\n",
    "\n",
    "# prepare a database for test predictions\n",
    "ml_test_predictions = test[['solutions',\n",
    "                            'init',\n",
    "                            'weight',\n",
    "                            'type',\n",
    "                            'exp']].copy()\n",
    "ml_test_predictions = ml_test_predictions.reset_index(drop=True)\n",
    "ml_test_predictions['lr']     = lin_reg_test_pred\n",
    "ml_test_predictions['lr_res'] = y_test - lin_reg_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally plot the histogram of errors, the predictions and the residual distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(2,3)\n",
    "\n",
    "# plot the predictions\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='lr',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "ax[0,0].set(title='Predictions and Real Values (Validation)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='lr',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "ax[1,0].set(title='Predictions and Real Values (Test)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of residuals\n",
    "sns.distplot(ml_val_predictions['lr_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[0,1]\n",
    "            )\n",
    "ax[0,1].set(title='Histogram of Residuals (Validation)')\n",
    "\n",
    "sns.distplot(ml_test_predictions['lr_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[1,1]\n",
    "            )\n",
    "ax[1,1].set(title='Histogram of Residuals (Test)')\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=ml_val_predictions,\n",
    "                x='lr',\n",
    "                y='lr_res',\n",
    "                ax=ax[0,2]\n",
    "               )\n",
    "ax[0,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[0,2].set(title='Distribution of the Residuals (Validation)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "\n",
    "sns.scatterplot(data=ml_test_predictions,\n",
    "                x='lr',\n",
    "                y='lr_res',\n",
    "                ax=ax[1,2]\n",
    "               )\n",
    "ax[1,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[1,2].set(title='Distribution of the Residuals (Test)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('lin_reg.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net\n",
    "\n",
    "We then introduce both L1 and L2 regularisation using the _elastic net_: we look for the contributions of L1-norm and L2-norm to the fit in order to improve the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics      import mean_squared_error, \\\n",
    "                                 mean_absolute_error, \\\n",
    "                                 r2_score\n",
    "from skopt                import gp_minimize\n",
    "from skopt.space          import Categorical, Integer, Real\n",
    "from skopt.utils          import use_named_args\n",
    "\n",
    "log.info('Fitting the elastic net.')\n",
    "\n",
    "# define the model\n",
    "els_net = ElasticNet(fit_intercept=False,\n",
    "                     normalize=False,\n",
    "                     max_iter=1e5,\n",
    "                     random_state=RAND\n",
    "                    )\n",
    "\n",
    "# define the hyperparameters\n",
    "hyperparameters = [Real(1.0e-3, 1.0e-1, prior='log-uniform', name='alpha'),\n",
    "                   Real(0.0, 1.0, name='l1_ratio')\n",
    "                  ]\n",
    "hyper_names     = [s.name for s in hyperparameters]\n",
    "\n",
    "# minimise the objective\n",
    "@use_named_args(hyperparameters)\n",
    "def objective(**args):\n",
    "    '''\n",
    "    Compute the objective function.\n",
    "    \n",
    "    Arguments:\n",
    "        **args: arguments to pass to the estimator\n",
    "    '''\n",
    "    log.debug('Exploring {}'.format(args)) # log the exploration point\n",
    "\n",
    "    # fit the estimator\n",
    "    els_net.set_params(**args)\n",
    "    els_net.fit(X_train, y_train)\n",
    "\n",
    "    # compute predictions on the validation set\n",
    "    y_val_pred = els_net.predict(X_val).reshape(-1,)\n",
    "    \n",
    "    return mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "# compute the minimisation\n",
    "els_net_res = gp_minimize(objective,\n",
    "                          hyperparameters,\n",
    "                          n_calls=30,\n",
    "                          random_state=RAND,\n",
    "                          n_jobs=THREADS\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous computation we extract the values of the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hyperparameters\n",
    "els_net_hyper = dict(zip(hyper_names, els_net_res.x))\n",
    "print('Best hyperparameters:\\n')\n",
    "for key, value in els_net_hyper.items():\n",
    "    print('  {}: {:.3f}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the algorithm again on the training set and compute metrics and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the new algorithm\n",
    "els_net = els_net.set_params(**els_net_hyper)\n",
    "els_net = els_net.fit(X_train, y_train)\n",
    "\n",
    "# compute predictions\n",
    "els_net_train_pred = els_net.predict(X_train).reshape(-1,)\n",
    "els_net_val_pred   = els_net.predict(X_val).reshape(-1,)\n",
    "els_net_test_pred  = els_net.predict(X_test).reshape(-1,)\n",
    "\n",
    "# store metrics\n",
    "els_net_metrics = {'tr_mse':    mean_squared_error(y_train,\n",
    "                                                   els_net_train_pred\n",
    "                                                  ),\n",
    "                   'tr_ci_low': mse_ci(y_train,\n",
    "                                       els_net_train_pred,\n",
    "                                       dof_train\n",
    "                                      )[0],\n",
    "                   'tr_ci_upp': mse_ci(y_train,\n",
    "                                       els_net_train_pred,\n",
    "                                       dof_train\n",
    "                                      )[1],\n",
    "                   'tr_mae':    mean_absolute_error(y_train,\n",
    "                                                    els_net_train_pred\n",
    "                                                   ),\n",
    "                   'tr_rsq':    r2_score(y_train,\n",
    "                                         els_net_train_pred\n",
    "                                        ),\n",
    "                   'v_mse':     mean_squared_error(y_val,\n",
    "                                                   els_net_val_pred\n",
    "                                                  ),\n",
    "                   'v_ci_low':  mse_ci(y_val,\n",
    "                                       els_net_val_pred,\n",
    "                                       dof_val\n",
    "                                      )[0],\n",
    "                   'v_ci_upp':  mse_ci(y_val,\n",
    "                                       els_net_val_pred,\n",
    "                                       dof_val\n",
    "                                      )[1],\n",
    "                   'v_mae':     mean_absolute_error(y_val,\n",
    "                                                    els_net_val_pred\n",
    "                                                   ),\n",
    "                   'v_rsq':     r2_score(y_val,\n",
    "                                         els_net_val_pred\n",
    "                                        ),\n",
    "                   'te_mse':    mean_squared_error(y_test,\n",
    "                                                   els_net_test_pred\n",
    "                                                  ),\n",
    "                   'te_ci_low': mse_ci(y_test,\n",
    "                                       els_net_test_pred,\n",
    "                                       dof_test\n",
    "                                      )[0],\n",
    "                   'te_ci_upp': mse_ci(y_test,\n",
    "                                       els_net_test_pred,\n",
    "                                       dof_test\n",
    "                                      )[1],\n",
    "                   'te_mae':    mean_absolute_error(y_test,\n",
    "                                                    els_net_test_pred\n",
    "                                                   ),\n",
    "                   'te_rsq':    r2_score(y_test,\n",
    "                                         els_net_test_pred\n",
    "                                        ),\n",
    "                  }\n",
    "els_net_metrics = pd.DataFrame(els_net_metrics, index=['els_net'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then print the metrics for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\\\n",
    "'ELASTIC NET:\\n\\n',\n",
    "'  Training set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(els_net_metrics['tr_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(els_net_metrics['tr_ci_low'].squeeze(),\n",
    "                                       els_net_metrics['tr_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(els_net_metrics['tr_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(els_net_metrics['tr_rsq'].squeeze()),\n",
    "'  Validation set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(els_net_metrics['v_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(els_net_metrics['v_ci_low'].squeeze(),\n",
    "                                       els_net_metrics['v_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(els_net_metrics['v_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(els_net_metrics['v_rsq'].squeeze()),\n",
    "'  Test set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(els_net_metrics['te_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(els_net_metrics['te_ci_low'].squeeze(),\n",
    "                                       els_net_metrics['te_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(els_net_metrics['te_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(els_net_metrics['te_rsq'].squeeze())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the predictions in the database of predictions we prepared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training predictions\n",
    "ml_train_predictions['en']     = els_net_train_pred\n",
    "ml_train_predictions['en_res'] = y_train - lin_reg_train_pred\n",
    "\n",
    "# store validation predictions\n",
    "ml_val_predictions['en']     = els_net_val_pred\n",
    "ml_val_predictions['en_res'] = y_val - lin_reg_val_pred\n",
    "\n",
    "# store test predictions\n",
    "ml_test_predictions['en']     = els_net_test_pred\n",
    "ml_test_predictions['en_res'] = y_test - lin_reg_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally plot the histogram of errors, the predictions and the residual distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(2,3)\n",
    "\n",
    "# plot the predictions\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='en',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "ax[0,0].set(title='Predictions and Real Values (Validation)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='en',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "ax[1,0].set(title='Predictions and Real Values (Test)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of residuals\n",
    "sns.distplot(ml_val_predictions['en_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[0,1]\n",
    "            )\n",
    "ax[0,1].set(title='Histogram of Residuals (Validation)')\n",
    "\n",
    "sns.distplot(ml_test_predictions['en_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[1,1]\n",
    "            )\n",
    "ax[1,1].set(title='Histogram of Residuals (Test)')\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=ml_val_predictions,\n",
    "                x='en',\n",
    "                y='en_res',\n",
    "                ax=ax[0,2]\n",
    "               )\n",
    "ax[0,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[0,2].set(title='Distribution of the Residuals (Validation)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "\n",
    "sns.scatterplot(data=ml_test_predictions,\n",
    "                x='en',\n",
    "                y='en_res',\n",
    "                ax=ax[1,2]\n",
    "               )\n",
    "ax[1,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[1,2].set(title='Distribution of the Residuals (Test)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('els_net.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "Using the _lasso_ regression we investigate the impact of L1 regularisation on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics      import mean_squared_error, \\\n",
    "                                 mean_absolute_error, \\\n",
    "                                 r2_score\n",
    "from skopt                import gp_minimize\n",
    "from skopt.space          import Categorical, Integer, Real\n",
    "from skopt.utils          import use_named_args\n",
    "\n",
    "log.info('Fitting the lasso.')\n",
    "\n",
    "# define the model\n",
    "lss_reg = Lasso(fit_intercept=False,\n",
    "                normalize=False,\n",
    "                max_iter=1e5,\n",
    "                random_state=RAND\n",
    "               )\n",
    "\n",
    "# define the hyperparameters\n",
    "hyperparameters = [Real(1.0e-2, 1.0e1, prior='log-uniform', name='alpha')]\n",
    "hyper_names     = [s.name for s in hyperparameters]\n",
    "\n",
    "# minimise the objective\n",
    "@use_named_args(hyperparameters)\n",
    "def objective(**args):\n",
    "    '''\n",
    "    Compute the objective function.\n",
    "    \n",
    "    Arguments:\n",
    "        **args: arguments to pass to the estimator\n",
    "    '''\n",
    "    log.debug('Exploring {}'.format(args)) # log the exploration point\n",
    "\n",
    "    # fit the estimator\n",
    "    lss_reg.set_params(**args)\n",
    "    lss_reg.fit(X_train, y_train)\n",
    "\n",
    "    # compute predictions on the validation set\n",
    "    y_val_pred = lss_reg.predict(X_val).reshape(-1,)\n",
    "    \n",
    "    return mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "# compute the minimisation\n",
    "lss_reg_res = gp_minimize(objective,\n",
    "                          hyperparameters,\n",
    "                          n_calls=30,\n",
    "                          random_state=RAND,\n",
    "                          n_jobs=THREADS\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous computation we extract the values of the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hyperparameters\n",
    "lss_reg_hyper = dict(zip(hyper_names, lss_reg_res.x))\n",
    "print('Best hyperparameters:\\n')\n",
    "for key, value in lss_reg_hyper.items():\n",
    "    print('  {}: {:.3f}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the algorithm again on the training set and compute metrics and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the new algorithm\n",
    "lss_reg = lss_reg.set_params(**lss_reg_hyper)\n",
    "lss_reg = lss_reg.fit(X_train, y_train)\n",
    "\n",
    "# compute predictions\n",
    "lss_reg_train_pred = lss_reg.predict(X_train).reshape(-1,)\n",
    "lss_reg_val_pred   = lss_reg.predict(X_val).reshape(-1,)\n",
    "lss_reg_test_pred  = lss_reg.predict(X_test).reshape(-1,)\n",
    "\n",
    "# store metrics\n",
    "lss_reg_metrics = {'tr_mse':    mean_squared_error(y_train,\n",
    "                                                   lss_reg_train_pred\n",
    "                                                  ),\n",
    "                   'tr_ci_low': mse_ci(y_train,\n",
    "                                       lss_reg_train_pred,\n",
    "                                       dof_train\n",
    "                                      )[0],\n",
    "                   'tr_ci_upp': mse_ci(y_train,\n",
    "                                       lss_reg_train_pred,\n",
    "                                       dof_train\n",
    "                                      )[1],\n",
    "                   'tr_mae':    mean_absolute_error(y_train,\n",
    "                                                    lss_reg_train_pred\n",
    "                                                   ),\n",
    "                   'tr_rsq':    r2_score(y_train,\n",
    "                                         lss_reg_train_pred\n",
    "                                        ),\n",
    "                   'v_mse':     mean_squared_error(y_val,\n",
    "                                                   lss_reg_val_pred\n",
    "                                                  ),\n",
    "                   'v_ci_low':  mse_ci(y_val,\n",
    "                                       lss_reg_val_pred,\n",
    "                                       dof_val\n",
    "                                      )[0],\n",
    "                   'v_ci_upp':  mse_ci(y_val,\n",
    "                                       lss_reg_val_pred,\n",
    "                                       dof_val\n",
    "                                      )[1],\n",
    "                   'v_mae':     mean_absolute_error(y_val,\n",
    "                                                    lss_reg_val_pred\n",
    "                                                   ),\n",
    "                   'v_rsq':     r2_score(y_val,\n",
    "                                         lss_reg_val_pred\n",
    "                                        ),\n",
    "                   'te_mse':    mean_squared_error(y_test,\n",
    "                                                   lss_reg_test_pred\n",
    "                                                  ),\n",
    "                   'te_ci_low': mse_ci(y_test,\n",
    "                                       lss_reg_test_pred,\n",
    "                                       dof_test\n",
    "                                      )[0],\n",
    "                   'te_ci_upp': mse_ci(y_test,\n",
    "                                       lss_reg_test_pred,\n",
    "                                       dof_test\n",
    "                                      )[1],\n",
    "                   'te_mae':    mean_absolute_error(y_test,\n",
    "                                                    lss_reg_test_pred\n",
    "                                                   ),\n",
    "                   'te_rsq':    r2_score(y_test,\n",
    "                                         lss_reg_test_pred\n",
    "                                        ),\n",
    "                  }\n",
    "lss_reg_metrics = pd.DataFrame(lss_reg_metrics, index=['lss_reg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then print the metrics for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\\\n",
    "'LASSO:\\n\\n',\n",
    "'  Training set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(lss_reg_metrics['tr_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(lss_reg_metrics['tr_ci_low'].squeeze(),\n",
    "                                       lss_reg_metrics['tr_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(lss_reg_metrics['tr_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(lss_reg_metrics['tr_rsq'].squeeze()),\n",
    "'  Validation set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(lss_reg_metrics['v_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(lss_reg_metrics['v_ci_low'].squeeze(),\n",
    "                                       lss_reg_metrics['v_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(lss_reg_metrics['v_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(lss_reg_metrics['v_rsq'].squeeze()),\n",
    "'  Test set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(lss_reg_metrics['te_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(lss_reg_metrics['te_ci_low'].squeeze(),\n",
    "                                       lss_reg_metrics['te_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(lss_reg_metrics['te_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(lss_reg_metrics['te_rsq'].squeeze())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the predictions in the database of predictions we prepared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training predictions\n",
    "ml_train_predictions['ls']     = lss_reg_train_pred\n",
    "ml_train_predictions['ls_res'] = y_train - lss_reg_train_pred\n",
    "\n",
    "# store validation predictions\n",
    "ml_val_predictions['ls']     = lss_reg_val_pred\n",
    "ml_val_predictions['ls_res'] = y_val - lss_reg_val_pred\n",
    "\n",
    "# store test predictions\n",
    "ml_test_predictions['ls']     = lss_reg_test_pred\n",
    "ml_test_predictions['ls_res'] = y_test - lss_reg_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally plot the histogram of errors, the predictions and the residual distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(2,3)\n",
    "\n",
    "# plot the predictions\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='ls',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "ax[0,0].set(title='Predictions and Real Values (Validation)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='ls',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "ax[1,0].set(title='Predictions and Real Values (Test)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of residuals\n",
    "sns.distplot(ml_val_predictions['ls_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[0,1]\n",
    "            )\n",
    "ax[0,1].set(title='Histogram of Residuals (Validation)')\n",
    "\n",
    "sns.distplot(ml_test_predictions['ls_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[1,1]\n",
    "            )\n",
    "ax[1,1].set(title='Histogram of Residuals (Test)')\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=ml_val_predictions,\n",
    "                x='ls',\n",
    "                y='ls_res',\n",
    "                ax=ax[0,2]\n",
    "               )\n",
    "ax[0,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[0,2].set(title='Distribution of the Residuals (Validation)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "\n",
    "sns.scatterplot(data=ml_test_predictions,\n",
    "                x='ls',\n",
    "                y='ls_res',\n",
    "                ax=ax[1,2]\n",
    "               )\n",
    "ax[1,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[1,2].set(title='Distribution of the Residuals (Test)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('lss_reg.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge\n",
    "\n",
    "L2 regularisation can then be probed using the _ridge_ regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics      import mean_squared_error, \\\n",
    "                                 mean_absolute_error, \\\n",
    "                                 r2_score\n",
    "from skopt                import gp_minimize\n",
    "from skopt.space          import Categorical, Integer, Real\n",
    "from skopt.utils          import use_named_args\n",
    "\n",
    "log.info('Fitting the ridge.')\n",
    "\n",
    "# define the model\n",
    "rdg_reg = Ridge(fit_intercept=False,\n",
    "                normalize=False,\n",
    "                random_state=RAND\n",
    "               )\n",
    "\n",
    "# define the hyperparameters\n",
    "hyperparameters = [Real(1.0e-3, 1.0e-1, prior='log-uniform', name='alpha')]\n",
    "hyper_names     = [s.name for s in hyperparameters]\n",
    "\n",
    "# minimise the objective\n",
    "@use_named_args(hyperparameters)\n",
    "def objective(**args):\n",
    "    '''\n",
    "    Compute the objective function.\n",
    "    \n",
    "    Arguments:\n",
    "        **args: arguments to pass to the estimator\n",
    "    '''\n",
    "    log.debug('Exploring {}'.format(args)) # log the exploration point\n",
    "\n",
    "    # fit the estimator\n",
    "    rdg_reg.set_params(**args)\n",
    "    rdg_reg.fit(X_train, y_train)\n",
    "\n",
    "    # compute predictions on the validation set\n",
    "    y_val_pred = rdg_reg.predict(X_val).reshape(-1,)\n",
    "    \n",
    "    return mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "# compute the minimisation\n",
    "rdg_reg_res = gp_minimize(objective,\n",
    "                          hyperparameters,\n",
    "                          n_calls=50,\n",
    "                          random_state=RAND,\n",
    "                          n_jobs=THREADS\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous computation we extract the values of the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hyperparameters\n",
    "rdg_reg_hyper = dict(zip(hyper_names, rdg_reg_res.x))\n",
    "print('Best hyperparameters:\\n')\n",
    "for key, value in rdg_reg_hyper.items():\n",
    "    print('  {}: {:.3f}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the algorithm again on the training set and compute metrics and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the new algorithm\n",
    "rdg_reg = rdg_reg.set_params(**rdg_reg_hyper)\n",
    "rdg_reg = rdg_reg.fit(X_train, y_train)\n",
    "\n",
    "# compute predictions\n",
    "rdg_reg_train_pred = rdg_reg.predict(X_train).reshape(-1,)\n",
    "rdg_reg_val_pred   = rdg_reg.predict(X_val).reshape(-1,)\n",
    "rdg_reg_test_pred  = rdg_reg.predict(X_test).reshape(-1,)\n",
    "\n",
    "# store metrics\n",
    "rdg_reg_metrics = {'tr_mse':    mean_squared_error(y_train,\n",
    "                                                   rdg_reg_train_pred\n",
    "                                                  ),\n",
    "                   'tr_ci_low': mse_ci(y_train,\n",
    "                                       rdg_reg_train_pred,\n",
    "                                       dof_train\n",
    "                                      )[0],\n",
    "                   'tr_ci_upp': mse_ci(y_train,\n",
    "                                       rdg_reg_train_pred,\n",
    "                                       dof_train\n",
    "                                      )[1],\n",
    "                   'tr_mae':    mean_absolute_error(y_train,\n",
    "                                                    rdg_reg_train_pred\n",
    "                                                   ),\n",
    "                   'tr_rsq':    r2_score(y_train,\n",
    "                                         rdg_reg_train_pred\n",
    "                                        ),\n",
    "                   'v_mse':     mean_squared_error(y_val,\n",
    "                                                   rdg_reg_val_pred\n",
    "                                                  ),\n",
    "                   'v_ci_low':  mse_ci(y_val,\n",
    "                                       rdg_reg_val_pred,\n",
    "                                       dof_val\n",
    "                                      )[0],\n",
    "                   'v_ci_upp':  mse_ci(y_val,\n",
    "                                       rdg_reg_val_pred,\n",
    "                                       dof_val\n",
    "                                      )[1],\n",
    "                   'v_mae':     mean_absolute_error(y_val,\n",
    "                                                    rdg_reg_val_pred\n",
    "                                                   ),\n",
    "                   'v_rsq':     r2_score(y_val,\n",
    "                                         rdg_reg_val_pred\n",
    "                                        ),\n",
    "                   'te_mse':    mean_squared_error(y_test,\n",
    "                                                   rdg_reg_test_pred\n",
    "                                                  ),\n",
    "                   'te_ci_low': mse_ci(y_test,\n",
    "                                       rdg_reg_test_pred,\n",
    "                                       dof_test\n",
    "                                      )[0],\n",
    "                   'te_ci_upp': mse_ci(y_test,\n",
    "                                       rdg_reg_test_pred,\n",
    "                                       dof_test\n",
    "                                      )[1],\n",
    "                   'te_mae':    mean_absolute_error(y_test,\n",
    "                                                    rdg_reg_test_pred\n",
    "                                                   ),\n",
    "                   'te_rsq':    r2_score(y_test,\n",
    "                                         rdg_reg_test_pred\n",
    "                                        ),\n",
    "                  }\n",
    "rdg_reg_metrics = pd.DataFrame(rdg_reg_metrics, index=['rdg_reg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then print the metrics for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\\\n",
    "'RIDGE:\\n\\n',\n",
    "'  Training set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(rdg_reg_metrics['tr_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(rdg_reg_metrics['tr_ci_low'].squeeze(),\n",
    "                                       rdg_reg_metrics['tr_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(rdg_reg_metrics['tr_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(rdg_reg_metrics['tr_rsq'].squeeze()),\n",
    "'  Validation set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(rdg_reg_metrics['v_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(rdg_reg_metrics['v_ci_low'].squeeze(),\n",
    "                                       rdg_reg_metrics['v_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(rdg_reg_metrics['v_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(rdg_reg_metrics['v_rsq'].squeeze()),\n",
    "'  Test set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(rdg_reg_metrics['te_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(rdg_reg_metrics['te_ci_low'].squeeze(),\n",
    "                                       rdg_reg_metrics['te_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(rdg_reg_metrics['te_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(rdg_reg_metrics['te_rsq'].squeeze())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the predictions in the database of predictions we prepared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training predictions\n",
    "ml_train_predictions['rd']     = rdg_reg_train_pred\n",
    "ml_train_predictions['rd_res'] = y_train - rdg_reg_train_pred\n",
    "\n",
    "# store validation predictions\n",
    "ml_val_predictions['rd']     = rdg_reg_val_pred\n",
    "ml_val_predictions['rd_res'] = y_val - rdg_reg_val_pred\n",
    "\n",
    "# store test predictions\n",
    "ml_test_predictions['rd']     = rdg_reg_test_pred\n",
    "ml_test_predictions['rd_res'] = y_test - rdg_reg_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally plot the histogram of errors, the predictions and the residual distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(2,3)\n",
    "\n",
    "# plot the predictions\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='rd',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "ax[0,0].set(title='Predictions and Real Values (Validation)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='rd',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "ax[1,0].set(title='Predictions and Real Values (Test)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of residuals\n",
    "sns.distplot(ml_val_predictions['rd_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[0,1]\n",
    "            )\n",
    "ax[0,1].set(title='Histogram of Residuals (Validation)')\n",
    "\n",
    "sns.distplot(ml_test_predictions['rd_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[1,1]\n",
    "            )\n",
    "ax[1,1].set(title='Histogram of Residuals (Test)')\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=ml_val_predictions,\n",
    "                x='rd',\n",
    "                y='rd_res',\n",
    "                ax=ax[0,2]\n",
    "               )\n",
    "ax[0,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[0,2].set(title='Distribution of the Residuals (Validation)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "\n",
    "sns.scatterplot(data=ml_test_predictions,\n",
    "                x='rd',\n",
    "                y='rd_res',\n",
    "                ax=ax[1,2]\n",
    "               )\n",
    "ax[1,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[1,2].set(title='Distribution of the Residuals (Test)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('rdg_reg.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVR\n",
    "\n",
    "Moving to SVM algorithms, we first investigate a linear model (l-SVR) as a baseline computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm     import LinearSVR\n",
    "from sklearn.metrics import mean_squared_error, \\\n",
    "                            mean_absolute_error, \\\n",
    "                            r2_score\n",
    "from skopt           import gp_minimize\n",
    "from skopt.space     import Categorical, Integer, Real\n",
    "from skopt.utils     import use_named_args\n",
    "\n",
    "log.info('Fitting the linear SVR.')\n",
    "\n",
    "# define the model\n",
    "lin_svr = LinearSVR(fit_intercept=False,\n",
    "                    max_iter=1e4,\n",
    "                    random_state=RAND\n",
    "                   )\n",
    "\n",
    "# define the hyperparameters\n",
    "hyperparameters = [Real(1.0e-6, 1.0e-1, prior='log-uniform', name='epsilon'),\n",
    "                   Real(1.0e1, 1.0e3, prior='log-uniform', name='C'),\n",
    "                   Categorical(['epsilon_insensitive',\n",
    "                                'squared_epsilon_insensitive'\n",
    "                               ],\n",
    "                               name='loss'\n",
    "                              ),\n",
    "                   \n",
    "                  ]\n",
    "hyper_names     = [s.name for s in hyperparameters]\n",
    "\n",
    "# minimise the objective\n",
    "@use_named_args(hyperparameters)\n",
    "def objective(**args):\n",
    "    '''\n",
    "    Compute the objective function.\n",
    "    \n",
    "    Arguments:\n",
    "        **args: arguments to pass to the estimator\n",
    "    '''\n",
    "    log.debug('Exploring {}'.format(args)) # log the exploration point\n",
    "\n",
    "    # fit the estimator\n",
    "    lin_svr.set_params(**args)\n",
    "    lin_svr.fit(X_train, y_train)\n",
    "\n",
    "    # compute predictions on the validation set\n",
    "    y_val_pred = lin_svr.predict(X_val).reshape(-1,)\n",
    "    \n",
    "    return mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "# compute the minimisation\n",
    "lin_svr_res = gp_minimize(objective,\n",
    "                          hyperparameters,\n",
    "                          n_calls=30,\n",
    "                          random_state=RAND,\n",
    "                          n_jobs=THREADS\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous computation we extract the values of the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hyperparameters\n",
    "lin_svr_hyper = dict(zip(hyper_names, lin_svr_res.x))\n",
    "print('Best hyperparameters:\\n')\n",
    "for key, value in lin_svr_hyper.items():\n",
    "    if isinstance(value, float):\n",
    "        print('  {}: {:.9f}'.format(key, value))\n",
    "    else:\n",
    "        print('  {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the algorithm again on the training set and compute metrics and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the new algorithm\n",
    "lin_svr = lin_svr.set_params(**lin_svr_hyper)\n",
    "lin_svr = lin_svr.fit(X_train, y_train)\n",
    "\n",
    "# compute predictions\n",
    "lin_svr_train_pred = lin_svr.predict(X_train).reshape(-1,)\n",
    "lin_svr_val_pred   = lin_svr.predict(X_val).reshape(-1,)\n",
    "lin_svr_test_pred  = lin_svr.predict(X_test).reshape(-1,)\n",
    "\n",
    "# store metrics\n",
    "lin_svr_metrics = {'tr_mse':    mean_squared_error(y_train,\n",
    "                                                   lin_svr_train_pred\n",
    "                                                  ),\n",
    "                   'tr_ci_low': mse_ci(y_train,\n",
    "                                       lin_svr_train_pred,\n",
    "                                       dof_train\n",
    "                                      )[0],\n",
    "                   'tr_ci_upp': mse_ci(y_train,\n",
    "                                       lin_svr_train_pred,\n",
    "                                       dof_train\n",
    "                                      )[1],\n",
    "                   'tr_mae':    mean_absolute_error(y_train,\n",
    "                                                    lin_svr_train_pred\n",
    "                                                   ),\n",
    "                   'tr_rsq':    r2_score(y_train,\n",
    "                                         lin_svr_train_pred\n",
    "                                        ),\n",
    "                   'v_mse':     mean_squared_error(y_val,\n",
    "                                                   lin_svr_val_pred\n",
    "                                                  ),\n",
    "                   'v_ci_low':  mse_ci(y_val,\n",
    "                                       lin_svr_val_pred,\n",
    "                                       dof_val\n",
    "                                      )[0],\n",
    "                   'v_ci_upp':  mse_ci(y_val,\n",
    "                                       lin_svr_val_pred,\n",
    "                                       dof_val\n",
    "                                      )[1],\n",
    "                   'v_mae':     mean_absolute_error(y_val,\n",
    "                                                    lin_svr_val_pred\n",
    "                                                   ),\n",
    "                   'v_rsq':     r2_score(y_val,\n",
    "                                         lin_svr_val_pred\n",
    "                                        ),\n",
    "                   'te_mse':    mean_squared_error(y_test,\n",
    "                                                   lin_svr_test_pred\n",
    "                                                  ),\n",
    "                   'te_ci_low': mse_ci(y_test,\n",
    "                                       lin_svr_test_pred,\n",
    "                                       dof_test\n",
    "                                      )[0],\n",
    "                   'te_ci_upp': mse_ci(y_test,\n",
    "                                       lin_svr_test_pred,\n",
    "                                       dof_test\n",
    "                                      )[1],\n",
    "                   'te_mae':    mean_absolute_error(y_test,\n",
    "                                                    lin_svr_test_pred\n",
    "                                                   ),\n",
    "                   'te_rsq':    r2_score(y_test,\n",
    "                                         lin_svr_test_pred\n",
    "                                        ),\n",
    "                  }\n",
    "lin_svr_metrics = pd.DataFrame(lin_svr_metrics, index=['lin_svr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then print the metrics for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\\\n",
    "'LINEAR SVR:\\n\\n',\n",
    "'  Training set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(lin_svr_metrics['tr_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(lin_svr_metrics['tr_ci_low'].squeeze(),\n",
    "                                       lin_svr_metrics['tr_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(lin_svr_metrics['tr_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(lin_svr_metrics['tr_rsq'].squeeze()),\n",
    "'  Validation set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(lin_svr_metrics['v_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(lin_svr_metrics['v_ci_low'].squeeze(),\n",
    "                                       lin_svr_metrics['v_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(lin_svr_metrics['v_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(lin_svr_metrics['v_rsq'].squeeze()),\n",
    "'  Test set:\\n\\n',\n",
    "'    MSE: {:.3f}'.format(lin_svr_metrics['te_mse'].squeeze()),\n",
    "'    95% CI [{:.3f}, {:.3f}]\\n'.format(lin_svr_metrics['te_ci_low'].squeeze(),\n",
    "                                       lin_svr_metrics['te_ci_upp'].squeeze()\n",
    "                                      ),\n",
    "'    MAE: {:.3f}\\n'.format(lin_svr_metrics['te_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(lin_svr_metrics['te_rsq'].squeeze())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the predictions in the database of predictions we prepared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training predictions\n",
    "ml_train_predictions['lv']     = lin_svr_train_pred\n",
    "ml_train_predictions['lv_res'] = y_train - lin_svr_train_pred\n",
    "\n",
    "# store validation predictions\n",
    "ml_val_predictions['lv']     = lin_svr_val_pred\n",
    "ml_val_predictions['lv_res'] = y_val - lin_svr_val_pred\n",
    "\n",
    "# store test predictions\n",
    "ml_test_predictions['lv']     = lin_svr_test_pred\n",
    "ml_test_predictions['lv_res'] = y_test - lin_svr_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally plot the histogram of errors, the predictions and the residual distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(2,3)\n",
    "\n",
    "# plot the predictions\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='lv',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "ax[0,0].set(title='Predictions and Real Values (Validation)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='lv',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "ax[1,0].set(title='Predictions and Real Values (Test)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of residuals\n",
    "sns.distplot(ml_val_predictions['lv_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[0,1]\n",
    "            )\n",
    "ax[0,1].set(title='Histogram of Residuals (Validation)')\n",
    "\n",
    "sns.distplot(ml_test_predictions['lv_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[1,1]\n",
    "            )\n",
    "ax[1,1].set(title='Histogram of Residuals (Test)')\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=ml_val_predictions,\n",
    "                x='lv',\n",
    "                y='lv_res',\n",
    "                ax=ax[0,2]\n",
    "               )\n",
    "ax[0,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[0,2].set(title='Distribution of the Residuals (Validation)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "\n",
    "sns.scatterplot(data=ml_test_predictions,\n",
    "                x='lv',\n",
    "                y='lv_res',\n",
    "                ax=ax[1,2]\n",
    "               )\n",
    "ax[1,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[1,2].set(title='Distribution of the Residuals (Test)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('lin_svr.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian SVR\n",
    "\n",
    "We then apply the _kernel trick_ to the SVM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm     import SVR\n",
    "from sklearn.metrics import mean_squared_error, \\\n",
    "                            mean_absolute_error, \\\n",
    "                            r2_score\n",
    "from skopt           import gp_minimize\n",
    "from skopt.space     import Categorical, Integer, Real\n",
    "from skopt.utils     import use_named_args\n",
    "\n",
    "log.info('Fitting the Gaussian SVR.')\n",
    "\n",
    "# define the model\n",
    "svr_rbf = SVR(kernel='rbf')\n",
    "\n",
    "# define the hyperparameters\n",
    "hyperparameters = [Real(1e-5, 1e-1, prior='log-uniform', name='gamma'),\n",
    "                   Real(1.0e-3, 1.0e-1, prior='log-uniform', name='epsilon'),\n",
    "                   Real(1.0e1, 1.0e3, prior='log-uniform', name='C')\n",
    "                  ]\n",
    "hyper_names     = [s.name for s in hyperparameters]\n",
    "\n",
    "# minimise the objective\n",
    "@use_named_args(hyperparameters)\n",
    "def objective(**args):\n",
    "    '''\n",
    "    Compute the objective function.\n",
    "    \n",
    "    Arguments:\n",
    "        **args: arguments to pass to the estimator\n",
    "    '''\n",
    "    log.debug('Exploring {}'.format(args)) # log the exploration point\n",
    "\n",
    "    # fit the estimator\n",
    "    svr_rbf.set_params(**args)\n",
    "    svr_rbf.fit(X_train, y_train)\n",
    "\n",
    "    # compute predictions on the validation set\n",
    "    y_val_pred = svr_rbf.predict(X_val).reshape(-1,)\n",
    "    \n",
    "    return mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "# compute the minimisation\n",
    "svr_rbf_res = gp_minimize(objective,\n",
    "                          hyperparameters,\n",
    "                          n_calls=75,\n",
    "                          random_state=RAND,\n",
    "                          n_jobs=THREADS\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous computation we extract the values of the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hyperparameters\n",
    "svr_rbf_hyper = dict(zip(hyper_names, svr_rbf_res.x))\n",
    "print('Best hyperparameters:\\n')\n",
    "for key, value in svr_rbf_hyper.items():\n",
    "    if isinstance(value, float):\n",
    "        print('  {}: {:.9f}'.format(key, value))\n",
    "    else:\n",
    "        print('  {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the algorithm again on the training set and compute metrics and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the new algorithm\n",
    "svr_rbf = svr_rbf.set_params(**svr_rbf_hyper)\n",
    "svr_rbf = svr_rbf.fit(X_train, y_train)\n",
    "\n",
    "# compute predictions\n",
    "svr_rbf_train_pred = svr_rbf.predict(X_train).reshape(-1,)\n",
    "svr_rbf_val_pred   = svr_rbf.predict(X_val).reshape(-1,)\n",
    "svr_rbf_test_pred  = svr_rbf.predict(X_test).reshape(-1,)\n",
    "\n",
    "# store metrics\n",
    "svr_rbf_metrics = {'tr_mse':    mean_squared_error(y_train,\n",
    "                                                   svr_rbf_train_pred\n",
    "                                                  ),\n",
    "                   'tr_mae':    mean_absolute_error(y_train,\n",
    "                                                    svr_rbf_train_pred\n",
    "                                                   ),\n",
    "                   'tr_rsq':    r2_score(y_train,\n",
    "                                         svr_rbf_train_pred\n",
    "                                        ),\n",
    "                   'v_mse':     mean_squared_error(y_val,\n",
    "                                                   svr_rbf_val_pred\n",
    "                                                  ),\n",
    "                   'v_mae':     mean_absolute_error(y_val,\n",
    "                                                    svr_rbf_val_pred\n",
    "                                                   ),\n",
    "                   'v_rsq':     r2_score(y_val,\n",
    "                                         svr_rbf_val_pred\n",
    "                                        ),\n",
    "                   'te_mse':    mean_squared_error(y_test,\n",
    "                                                   svr_rbf_test_pred\n",
    "                                                  ),\n",
    "                   'te_mae':    mean_absolute_error(y_test,\n",
    "                                                    svr_rbf_test_pred\n",
    "                                                   ),\n",
    "                   'te_rsq':    r2_score(y_test,\n",
    "                                         svr_rbf_test_pred\n",
    "                                        ),\n",
    "                  }\n",
    "svr_rbf_metrics = pd.DataFrame(svr_rbf_metrics, index=['svr_rbf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then print the metrics for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\\\n",
    "'GAUSSIAN SVR:\\n\\n',\n",
    "'  Training set:\\n\\n',\n",
    "'    MSE: {:.3f}\\n'.format(svr_rbf_metrics['tr_mse'].squeeze()),\n",
    "'    MAE: {:.3f}\\n'.format(svr_rbf_metrics['tr_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(svr_rbf_metrics['tr_rsq'].squeeze()),\n",
    "'  Validation set:\\n\\n',\n",
    "'    MSE: {:.3f}\\n'.format(svr_rbf_metrics['v_mse'].squeeze()),\n",
    "'    MAE: {:.3f}\\n'.format(svr_rbf_metrics['v_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(svr_rbf_metrics['v_rsq'].squeeze()),\n",
    "'  Test set:\\n\\n',\n",
    "'    MSE: {:.3f}\\n'.format(svr_rbf_metrics['te_mse'].squeeze()),\n",
    "'    MAE: {:.3f}\\n'.format(svr_rbf_metrics['te_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(svr_rbf_metrics['te_rsq'].squeeze())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the predictions in the database of predictions we prepared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training predictions\n",
    "ml_train_predictions['sv']     = svr_rbf_train_pred\n",
    "ml_train_predictions['sv_res'] = y_train - svr_rbf_train_pred\n",
    "\n",
    "# store validation predictions\n",
    "ml_val_predictions['sv']     = svr_rbf_val_pred\n",
    "ml_val_predictions['sv_res'] = y_val - svr_rbf_val_pred\n",
    "\n",
    "# store test predictions\n",
    "ml_test_predictions['sv']     = svr_rbf_test_pred\n",
    "ml_test_predictions['sv_res'] = y_test - svr_rbf_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally plot the histogram of errors, the predictions and the residual distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(2,3)\n",
    "\n",
    "# plot the predictions\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='sv',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "ax[0,0].set(title='Predictions and Real Values (Validation)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='sv',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "ax[1,0].set(title='Predictions and Real Values (Test)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of residuals\n",
    "sns.distplot(ml_val_predictions['sv_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[0,1]\n",
    "            )\n",
    "ax[0,1].set(title='Histogram of Residuals (Validation)')\n",
    "\n",
    "sns.distplot(ml_test_predictions['sv_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[1,1]\n",
    "            )\n",
    "ax[1,1].set(title='Histogram of Residuals (Test)')\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=ml_val_predictions,\n",
    "                x='sv',\n",
    "                y='sv_res',\n",
    "                ax=ax[0,2]\n",
    "               )\n",
    "ax[0,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[0,2].set(title='Distribution of the Residuals (Validation)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "\n",
    "sns.scatterplot(data=ml_test_predictions,\n",
    "                x='sv',\n",
    "                y='sv_res',\n",
    "                ax=ax[1,2]\n",
    "               )\n",
    "ax[1,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[1,2].set(title='Distribution of the Residuals (Test)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('svr_rbf.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "We now consider the case of random forests of decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm        import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, \\\n",
    "                            mean_absolute_error, \\\n",
    "                            r2_score\n",
    "from skopt           import gp_minimize\n",
    "from skopt.space     import Categorical, Integer, Real\n",
    "from skopt.utils     import use_named_args\n",
    "\n",
    "log.info('Fitting the random forests.')\n",
    "\n",
    "# define the model\n",
    "rnd_for = LGBMRegressor(boosting_type = 'rf',\n",
    "                        objective = 'regression',\n",
    "                        subsample_freq = 1,\n",
    "                        n_jobs = 1)\n",
    "\n",
    "# define the hyperparameters\n",
    "hyperparameters = [Integer(2, 100, name='num_leaves'),\n",
    "                   Integer(50, 300, name='max_depth'),\n",
    "                   Integer(2, 50, name='n_estimators'),\n",
    "                   Real(0.1, 0.99, name='subsample'),\n",
    "                   Real(0.7, 1.0, name='colsample_bytree'),\n",
    "                   Real(1.0e-6, 1.0e-2,\n",
    "                        prior='log-uniform',\n",
    "                        name='min_child_weight'\n",
    "                       ),\n",
    "                   Real(1.0e-1, 1.0e2, prior='log-uniform', name='reg_alpha'),\n",
    "                   Real(1.0e-1, 1.0e2, prior='log-uniform', name='reg_lambda')\n",
    "                  ]\n",
    "hyper_names     = [s.name for s in hyperparameters]\n",
    "\n",
    "# minimise the objective\n",
    "@use_named_args(hyperparameters)\n",
    "def objective(**args):\n",
    "    '''\n",
    "    Compute the objective function.\n",
    "    \n",
    "    Arguments:\n",
    "        **args: arguments to pass to the estimator\n",
    "    '''\n",
    "    log.debug('Exploring {}'.format(args)) # log the exploration point\n",
    "\n",
    "    # fit the estimator\n",
    "    rnd_for.set_params(**args)\n",
    "    rnd_for.fit(X_train, y_train)\n",
    "\n",
    "    # compute predictions on the validation set\n",
    "    y_val_pred = rnd_for.predict(X_val).reshape(-1,)\n",
    "    \n",
    "    return mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "# compute the minimisation\n",
    "rnd_for_res = gp_minimize(objective,\n",
    "                          hyperparameters,\n",
    "                          n_calls=125,\n",
    "                          random_state=RAND,\n",
    "                          n_jobs=THREADS\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous computation we extract the values of the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hyperparameters\n",
    "rnd_for_hyper = dict(zip(hyper_names, rnd_for_res.x))\n",
    "print('Best hyperparameters:\\n')\n",
    "for key, value in rnd_for_hyper.items():\n",
    "    if isinstance(value, float):\n",
    "        print('  {}: {:.9f}'.format(key, value))\n",
    "    else:\n",
    "        print('  {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the algorithm again on the training set and compute metrics and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the new algorithm\n",
    "rnd_for = rnd_for.set_params(**rnd_for_hyper)\n",
    "rnd_for = rnd_for.fit(X_train, y_train)\n",
    "\n",
    "# compute predictions\n",
    "rnd_for_train_pred = rnd_for.predict(X_train).reshape(-1,)\n",
    "rnd_for_val_pred   = rnd_for.predict(X_val).reshape(-1,)\n",
    "rnd_for_test_pred  = rnd_for.predict(X_test).reshape(-1,)\n",
    "\n",
    "# store metrics\n",
    "rnd_for_metrics = {'tr_mse':    mean_squared_error(y_train,\n",
    "                                                   rnd_for_train_pred\n",
    "                                                  ),\n",
    "                   'tr_mae':    mean_absolute_error(y_train,\n",
    "                                                    rnd_for_train_pred\n",
    "                                                   ),\n",
    "                   'tr_rsq':    r2_score(y_train,\n",
    "                                         rnd_for_train_pred\n",
    "                                        ),\n",
    "                   'v_mse':     mean_squared_error(y_val,\n",
    "                                                   rnd_for_val_pred\n",
    "                                                  ),\n",
    "                   'v_mae':     mean_absolute_error(y_val,\n",
    "                                                    rnd_for_val_pred\n",
    "                                                   ),\n",
    "                   'v_rsq':     r2_score(y_val,\n",
    "                                         rnd_for_val_pred\n",
    "                                        ),\n",
    "                   'te_mse':    mean_squared_error(y_test,\n",
    "                                                   rnd_for_test_pred\n",
    "                                                  ),\n",
    "                   'te_mae':    mean_absolute_error(y_test,\n",
    "                                                    rnd_for_test_pred\n",
    "                                                   ),\n",
    "                   'te_rsq':    r2_score(y_test,\n",
    "                                         rnd_for_test_pred\n",
    "                                        ),\n",
    "                  }\n",
    "rnd_for_metrics = pd.DataFrame(rnd_for_metrics, index=['rnd_for'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then print the metrics for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\\\n",
    "'RANDOM FORESTS:\\n\\n',\n",
    "'  Training set:\\n\\n',\n",
    "'    MSE: {:.3f}\\n'.format(rnd_for_metrics['tr_mse'].squeeze()),\n",
    "'    MAE: {:.3f}\\n'.format(rnd_for_metrics['tr_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(rnd_for_metrics['tr_rsq'].squeeze()),\n",
    "'  Validation set:\\n\\n',\n",
    "'    MSE: {:.3f}\\n'.format(rnd_for_metrics['v_mse'].squeeze()),\n",
    "'    MAE: {:.3f}\\n'.format(rnd_for_metrics['v_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(rnd_for_metrics['v_rsq'].squeeze()),\n",
    "'  Test set:\\n\\n',\n",
    "'    MSE: {:.3f}\\n'.format(rnd_for_metrics['te_mse'].squeeze()),\n",
    "'    MAE: {:.3f}\\n'.format(rnd_for_metrics['te_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(rnd_for_metrics['te_rsq'].squeeze())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the predictions in the database of predictions we prepared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training predictions\n",
    "ml_train_predictions['rf']     = rnd_for_train_pred\n",
    "ml_train_predictions['rf_res'] = y_train - rnd_for_train_pred\n",
    "\n",
    "# store validation predictions\n",
    "ml_val_predictions['rf']     = rnd_for_val_pred\n",
    "ml_val_predictions['rf_res'] = y_val - rnd_for_val_pred\n",
    "\n",
    "# store test predictions\n",
    "ml_test_predictions['rf']     = rnd_for_test_pred\n",
    "ml_test_predictions['rf_res'] = y_test - rnd_for_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally plot the histogram of errors, the predictions and the residual distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(2,3)\n",
    "\n",
    "# plot the predictions\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='rf',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "ax[0,0].set(title='Predictions and Real Values (Validation)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='rf',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "ax[1,0].set(title='Predictions and Real Values (Test)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of residuals\n",
    "sns.distplot(ml_val_predictions['rf_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[0,1]\n",
    "            )\n",
    "ax[0,1].set(title='Histogram of Residuals (Validation)')\n",
    "\n",
    "sns.distplot(ml_test_predictions['rf_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[1,1]\n",
    "            )\n",
    "ax[1,1].set(title='Histogram of Residuals (Test)')\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=ml_val_predictions,\n",
    "                x='rf',\n",
    "                y='rf_res',\n",
    "                ax=ax[0,2]\n",
    "               )\n",
    "ax[0,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[0,2].set(title='Distribution of the Residuals (Validation)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "\n",
    "sns.scatterplot(data=ml_test_predictions,\n",
    "                x='rf',\n",
    "                y='rf_res',\n",
    "                ax=ax[1,2]\n",
    "               )\n",
    "ax[1,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[1,2].set(title='Distribution of the Residuals (Test)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('rnd_for.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Decision Trees\n",
    "\n",
    "As a final experiment with decision trees, we consider the case of _gradient boosting_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm        import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, \\\n",
    "                            mean_absolute_error, \\\n",
    "                            r2_score\n",
    "from skopt           import gp_minimize\n",
    "from skopt.space     import Categorical, Integer, Real\n",
    "from skopt.utils     import use_named_args\n",
    "\n",
    "log.info('Fitting the gradient boosted decision trees.')\n",
    "\n",
    "# define the model\n",
    "grd_bst = LGBMRegressor(boosting_type = 'gbdt',\n",
    "                        objective = 'regression',\n",
    "                        subsample_freq = 1,\n",
    "                        n_jobs = 1)\n",
    "\n",
    "# define the hyperparameters\n",
    "hyperparameters = [Integer(2, 25, name='num_leaves'),\n",
    "                   Integer(2, 25, name='max_depth'),\n",
    "                   Integer(1e1, 4e3,\n",
    "                           prior='log-uniform',\n",
    "                           name='n_estimators'\n",
    "                          ),\n",
    "                   Real(0.1, 0.99, name='subsample'),\n",
    "                   Real(0.7, 1.0, name='colsample_bytree'),\n",
    "                   Real(1.0e-3, 1.0e-1,\n",
    "                        prior='log-uniform',\n",
    "                        name='min_child_weight'\n",
    "                       ),\n",
    "                   Real(1.0e0, 1.0e2, prior='log-uniform', name='reg_alpha'),\n",
    "                   Real(1.0e0, 1.0e3, prior='log-uniform', name='reg_lambda')\n",
    "                  ]\n",
    "hyper_names     = [s.name for s in hyperparameters]\n",
    "\n",
    "# minimise the objective\n",
    "@use_named_args(hyperparameters)\n",
    "def objective(**args):\n",
    "    '''\n",
    "    Compute the objective function.\n",
    "    \n",
    "    Arguments:\n",
    "        **args: arguments to pass to the estimator\n",
    "    '''\n",
    "    log.debug('Exploring {}'.format(args)) # log the exploration point\n",
    "\n",
    "    # fit the estimator\n",
    "    grd_bst.set_params(**args)\n",
    "    grd_bst.fit(X_train, y_train)\n",
    "\n",
    "    # compute predictions on the validation set\n",
    "    y_val_pred = grd_bst.predict(X_val).reshape(-1,)\n",
    "    \n",
    "    return mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "# compute the minimisation\n",
    "grd_bst_res = gp_minimize(objective,\n",
    "                          hyperparameters,\n",
    "                          n_calls=125,\n",
    "                          random_state=RAND,\n",
    "                          n_jobs=THREADS\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous computation we extract the values of the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hyperparameters\n",
    "grd_bst_hyper = dict(zip(hyper_names, grd_bst_res.x))\n",
    "print('Best hyperparameters:\\n')\n",
    "for key, value in grd_bst_hyper.items():\n",
    "    if isinstance(value, float):\n",
    "        print('  {}: {:.3f}'.format(key, value))\n",
    "    else:\n",
    "        print('  {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the algorithm again on the training set and compute metrics and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the new algorithm\n",
    "grd_bst = grd_bst.set_params(**grd_bst_hyper)\n",
    "grd_bst = grd_bst.fit(X_train, y_train)\n",
    "\n",
    "# compute predictions\n",
    "grd_bst_train_pred = grd_bst.predict(X_train).reshape(-1,)\n",
    "grd_bst_val_pred   = grd_bst.predict(X_val).reshape(-1,)\n",
    "grd_bst_test_pred  = grd_bst.predict(X_test).reshape(-1,)\n",
    "\n",
    "# store metrics\n",
    "grd_bst_metrics = {'tr_mse':    mean_squared_error(y_train,\n",
    "                                                   grd_bst_train_pred\n",
    "                                                  ),\n",
    "                   'tr_mae':    mean_absolute_error(y_train,\n",
    "                                                    grd_bst_train_pred\n",
    "                                                   ),\n",
    "                   'tr_rsq':    r2_score(y_train,\n",
    "                                         grd_bst_train_pred\n",
    "                                        ),\n",
    "                   'v_mse':     mean_squared_error(y_val,\n",
    "                                                   grd_bst_val_pred\n",
    "                                                  ),\n",
    "                   'v_mae':     mean_absolute_error(y_val,\n",
    "                                                    grd_bst_val_pred\n",
    "                                                   ),\n",
    "                   'v_rsq':     r2_score(y_val,\n",
    "                                         grd_bst_val_pred\n",
    "                                        ),\n",
    "                   'te_mse':    mean_squared_error(y_test,\n",
    "                                                   grd_bst_test_pred\n",
    "                                                  ),\n",
    "                   'te_mae':    mean_absolute_error(y_test,\n",
    "                                                    grd_bst_test_pred\n",
    "                                                   ),\n",
    "                   'te_rsq':    r2_score(y_test,\n",
    "                                         grd_bst_test_pred\n",
    "                                        ),\n",
    "                  }\n",
    "grd_bst_metrics = pd.DataFrame(grd_bst_metrics, index=['grd_bst'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then print the metrics for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\\\n",
    "'GRADIENT BOOSTED DECISION TREES:\\n\\n',\n",
    "'  Training set:\\n\\n',\n",
    "'    MSE: {:.4f}\\n'.format(grd_bst_metrics['tr_mse'].squeeze()),\n",
    "'    MAE: {:.4f}\\n'.format(grd_bst_metrics['tr_mae'].squeeze()),\n",
    "'    R2:  {:.4f}\\n\\n'.format(grd_bst_metrics['tr_rsq'].squeeze()),\n",
    "'  Validation set:\\n\\n',\n",
    "'    MSE: {:.4f}\\n'.format(grd_bst_metrics['v_mse'].squeeze()),\n",
    "'    MAE: {:.4f}\\n'.format(grd_bst_metrics['v_mae'].squeeze()),\n",
    "'    R2:  {:.4f}\\n\\n'.format(grd_bst_metrics['v_rsq'].squeeze()),\n",
    "'  Test set:\\n\\n',\n",
    "'    MSE: {:.4f}\\n'.format(grd_bst_metrics['te_mse'].squeeze()),\n",
    "'    MAE: {:.4f}\\n'.format(grd_bst_metrics['te_mae'].squeeze()),\n",
    "'    R2:  {:.4f}\\n\\n'.format(grd_bst_metrics['te_rsq'].squeeze())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the predictions in the database of predictions we prepared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training predictions\n",
    "ml_train_predictions['bt']     = grd_bst_train_pred\n",
    "ml_train_predictions['bt_res'] = y_train - grd_bst_train_pred\n",
    "\n",
    "# store validation predictions\n",
    "ml_val_predictions['bt']     = grd_bst_val_pred\n",
    "ml_val_predictions['bt_res'] = y_val - grd_bst_val_pred\n",
    "\n",
    "# store test predictions\n",
    "ml_test_predictions['bt']     = grd_bst_test_pred\n",
    "ml_test_predictions['bt_res'] = y_test - grd_bst_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally plot the histogram of errors, the predictions and the residual distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(2,3)\n",
    "\n",
    "# plot the predictions\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='bt',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "ax[0,0].set(title='Predictions and Real Values (Validation)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='bt',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "ax[1,0].set(title='Predictions and Real Values (Test)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of residuals\n",
    "sns.distplot(ml_val_predictions['bt_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[0,1]\n",
    "            )\n",
    "ax[0,1].set(title='Histogram of Residuals (Validation)')\n",
    "\n",
    "sns.distplot(ml_test_predictions['bt_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[1,1]\n",
    "            )\n",
    "ax[1,1].set(title='Histogram of Residuals (Test)')\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=ml_val_predictions,\n",
    "                x='bt',\n",
    "                y='bt_res',\n",
    "                ax=ax[0,2]\n",
    "               )\n",
    "ax[0,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[0,2].set(title='Distribution of the Residuals (Validation)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "\n",
    "sns.scatterplot(data=ml_test_predictions,\n",
    "                x='bt',\n",
    "                y='bt_res',\n",
    "                ax=ax[1,2]\n",
    "               )\n",
    "ax[1,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[1,2].set(title='Distribution of the Residuals (Test)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('grd_bst.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Networks\n",
    "\n",
    "As a final model we use a simple fully connected (FC) ANN to predict the labels. We first build the model (it should not be too complicated, thus we can actually use the `Sequential` interface in `tf.keras`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow       import keras\n",
    "from tensorflow.keras import layers, \\\n",
    "                             initializers, \\\n",
    "                             optimizers, \\\n",
    "                             regularizers, \\\n",
    "                             losses, \\\n",
    "                             metrics\n",
    "from sklearn.metrics  import mean_squared_error, \\\n",
    "                             mean_absolute_error, \\\n",
    "                             r2_score\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# build shortcuts to include layers\n",
    "inputer  = lambda n: layers.Dense(n,\n",
    "                         input_shape=X_train.shape[1:],\n",
    "                         activation='relu',\n",
    "                         kernel_initializer=initializers.glorot_uniform(RAND),\n",
    "                         bias_initializer=tf.zeros_initializer(),\n",
    "                         #activity_regularizer=regularizers.l2(0.20)\n",
    "                        )\n",
    "denser   = lambda n: layers.Dense(n,\n",
    "                         activation='relu',\n",
    "                         kernel_initializer=initializers.glorot_uniform(RAND),\n",
    "                         bias_initializer=tf.zeros_initializer(),\n",
    "                         #activity_regularizer=regularizers.l2(0.05)\n",
    "                        )\n",
    "dropper  = lambda x: layers.Dropout(rate=x, seed=RAND)\n",
    "outputer = lambda n: layers.Dense(n,\n",
    "                         name='output',\n",
    "                         kernel_initializer=initializers.glorot_uniform(RAND),\n",
    "                         bias_initializer=tf.zeros_initializer(),\n",
    "                         #activity_regularizer=regularizers.l2(0.05)\n",
    "                        )\n",
    "\n",
    "# build the model\n",
    "ann_mod = keras.Sequential(name='sft-trunc')\n",
    "\n",
    "ann_mod.add(inputer(30))\n",
    "ann_mod.add(layers.BatchNormalization())\n",
    "ann_mod.add(dropper(0.01))\n",
    "\n",
    "ann_mod.add(outputer(1))\n",
    "\n",
    "# compile the model\n",
    "ann_mod.compile(optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "                loss=losses.MeanSquaredError(),\n",
    "                metrics=[metrics.MeanSquaredError()]\n",
    "               )\n",
    "\n",
    "ann_mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fit the model on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "log.info('Fitting the artificial neural network.')\n",
    "\n",
    "# define callbacks\n",
    "callbacks = [callbacks.ModelCheckpoint(modpath('ann_mod.h5'),\n",
    "                                       monitor='val_loss',\n",
    "                                       verbose=1,\n",
    "                                       save_best_only=True\n",
    "                                      )\n",
    "            ]\n",
    "\n",
    "# fit the model\n",
    "ann_mod_hst = ann_mod.fit(X_train,\n",
    "                          y_train,\n",
    "                          batch_size=X_train.shape[0],\n",
    "                          epochs=5000,\n",
    "                          verbose=1,\n",
    "                          callbacks=callbacks,\n",
    "                          validation_data=(X_val, y_val)\n",
    "                         )\n",
    "\n",
    "# load the best model\n",
    "ann_mod = keras.models.load_model(modpath('ann_mod.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then look at the history of the model and plot the loss and the metric functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_mod_history = pd.DataFrame(ann_mod_hst.history)\n",
    "\n",
    "fig, ax = subplots(1,2)\n",
    "\n",
    "sns.lineplot(data=ann_mod_history[['loss', 'val_loss']],\n",
    "             ax=ax[0]\n",
    "            )\n",
    "ax[0].set(title='Loss Function',\n",
    "          xlabel='epoch',\n",
    "          yscale='log'\n",
    "         )\n",
    "\n",
    "sns.lineplot(data=ann_mod_history[['mean_squared_error',\n",
    "                                   'val_mean_squared_error'\n",
    "                                  ]\n",
    "                                 ],\n",
    "             ax=ax[1]\n",
    "            )\n",
    "ax[1].set(title='MSE',\n",
    "          xlabel='epoch',\n",
    "          yscale='log'\n",
    "         )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('ann_hst.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also graph the same plots applying a _running average_ over the epochs in order to better distinguish the evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_avg(values: pd.Series, window: float) -> np.ndarray:\n",
    "    '''\n",
    "    Compute the (simple) running average of a series of data.\n",
    "    \n",
    "    Required arguments:\n",
    "        values: list of values,\n",
    "        window: temporal window for the computation.\n",
    "    '''\n",
    "    values = values.values.reshape(-1,)\n",
    "    avg    = np.zeros((np.shape(values)[0] - window + 1,))\n",
    "    \n",
    "    for i in range(avg.shape[0]):\n",
    "        avg[i] = np.sum(values[i:i + window]) / window\n",
    "    \n",
    "    return avg\n",
    "\n",
    "# select a \"time\" window\n",
    "win = int(ann_mod_history.shape[0] / 10)\n",
    "run_avg(ann_mod_history['loss'], win)\n",
    "avg = pd.DataFrame({f: run_avg(ann_mod_history[f], win).reshape(-1,)\n",
    "                    for f in ann_mod_history\n",
    "                   }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can plot a more understandable evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dataframe\n",
    "fig, ax = subplots(1,2)\n",
    "\n",
    "sns.lineplot(data=avg[['loss', 'val_loss']],\n",
    "             ax=ax[0]\n",
    "            )\n",
    "ax[0].set(title='Loss Function',\n",
    "          xlabel='step ({:d} epochs windows)'.format(win),\n",
    "          yscale='log'\n",
    "         )\n",
    "\n",
    "sns.lineplot(data=avg[['mean_squared_error', 'val_mean_squared_error']],\n",
    "             ax=ax[1]\n",
    "            )\n",
    "ax[1].set(title='MSE',\n",
    "          xlabel='step ({:d} epochs windows)'.format(win),\n",
    "          yscale='log'\n",
    "         )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('ann_hst_avg.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the simple architecture to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "ann_mod_dot = keras.utils.model_to_dot(ann_mod, show_shapes=True, dpi=300)\n",
    "ann_mod_dot.write_pdf(imgpath('ann_mod_arch.pdf'))\n",
    "\n",
    "Image(ann_mod_dot.create_png(), width=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally compute the predictions and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predictions\n",
    "ann_mod_train_pred = ann_mod.predict(X_train).reshape(-1,)\n",
    "ann_mod_val_pred   = ann_mod.predict(X_val).reshape(-1,)\n",
    "ann_mod_test_pred  = ann_mod.predict(X_test).reshape(-1,)\n",
    "\n",
    "# store metrics\n",
    "ann_mod_metrics = {'tr_mse':    mean_squared_error(y_train,\n",
    "                                                   ann_mod_train_pred\n",
    "                                                  ),\n",
    "                   'tr_mae':    mean_absolute_error(y_train,\n",
    "                                                    ann_mod_train_pred\n",
    "                                                   ),\n",
    "                   'tr_rsq':    r2_score(y_train,\n",
    "                                         ann_mod_train_pred\n",
    "                                        ),\n",
    "                   'v_mse':     mean_squared_error(y_val,\n",
    "                                                   ann_mod_val_pred\n",
    "                                                  ),\n",
    "                   'v_mae':     mean_absolute_error(y_val,\n",
    "                                                    ann_mod_val_pred\n",
    "                                                   ),\n",
    "                   'v_rsq':     r2_score(y_val,\n",
    "                                         ann_mod_val_pred\n",
    "                                        ),\n",
    "                   'te_mse':    mean_squared_error(y_test,\n",
    "                                                   ann_mod_test_pred\n",
    "                                                  ),\n",
    "                   'te_mae':    mean_absolute_error(y_test,\n",
    "                                                    ann_mod_test_pred\n",
    "                                                   ),\n",
    "                   'te_rsq':    r2_score(y_test,\n",
    "                                         ann_mod_test_pred\n",
    "                                        ),\n",
    "                  }\n",
    "ann_mod_metrics = pd.DataFrame(ann_mod_metrics, index=['ann_mod'])\n",
    "\n",
    "print(\\\n",
    "'ARTIFICIAL NEURAL NETWORKS:\\n\\n',\n",
    "'  Training set:\\n\\n',\n",
    "'    MSE: {:.3f}\\n'.format(ann_mod_metrics['tr_mse'].squeeze()),\n",
    "'    MAE: {:.3f}\\n'.format(ann_mod_metrics['tr_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(ann_mod_metrics['tr_rsq'].squeeze()),\n",
    "'  Validation set:\\n\\n',\n",
    "'    MSE: {:.3f}\\n'.format(ann_mod_metrics['v_mse'].squeeze()),\n",
    "'    MAE: {:.3f}\\n'.format(ann_mod_metrics['v_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(ann_mod_metrics['v_rsq'].squeeze()),\n",
    "'  Test set:\\n\\n',\n",
    "'    MSE: {:.3f}\\n'.format(ann_mod_metrics['te_mse'].squeeze()),\n",
    "'    MAE: {:.3f}\\n'.format(ann_mod_metrics['te_mae'].squeeze()),\n",
    "'    R2:  {:.3f}\\n\\n'.format(ann_mod_metrics['te_rsq'].squeeze())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the predictions of the ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training predictions\n",
    "ml_train_predictions['nn']     = ann_mod_train_pred\n",
    "ml_train_predictions['nn_res'] = y_train - ann_mod_train_pred\n",
    "\n",
    "# store validation predictions\n",
    "ml_val_predictions['nn']     = ann_mod_val_pred\n",
    "ml_val_predictions['nn_res'] = y_val - ann_mod_val_pred\n",
    "\n",
    "# store test predictions\n",
    "ml_test_predictions['nn']     = ann_mod_test_pred\n",
    "ml_test_predictions['nn_res'] = y_test - ann_mod_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally plot the histogram of errors, the predictions and the residual distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(2,3)\n",
    "\n",
    "# plot the predictions\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_val_predictions,\n",
    "             x=ml_val_predictions.index,\n",
    "             y='nn',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[0,0]\n",
    "            )\n",
    "ax[0,0].set(title='Predictions and Real Values (Validation)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='exp',\n",
    "             label='real value',\n",
    "             alpha=0.9,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "sns.lineplot(data=ml_test_predictions,\n",
    "             x=ml_test_predictions.index,\n",
    "             y='nn',\n",
    "             label='predictions',\n",
    "             markers=True,\n",
    "             marker='o',\n",
    "             alpha=0.7,\n",
    "             ax=ax[1,0]\n",
    "            )\n",
    "ax[1,0].set(title='Predictions and Real Values (Test)',\n",
    "            xlabel='ID of the prediction',\n",
    "            ylabel=''\n",
    "           )\n",
    "\n",
    "# plot the histogram of residuals\n",
    "sns.distplot(ml_val_predictions['nn_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[0,1]\n",
    "            )\n",
    "ax[0,1].set(title='Histogram of Residuals (Validation)')\n",
    "\n",
    "sns.distplot(ml_test_predictions['nn_res'],\n",
    "             bins=10,\n",
    "             kde=False,\n",
    "             axlabel='residual',\n",
    "             ax=ax[1,1]\n",
    "            )\n",
    "ax[1,1].set(title='Histogram of Residuals (Test)')\n",
    "\n",
    "# plot the distribution of the residuals\n",
    "sns.scatterplot(data=ml_val_predictions,\n",
    "                x='nn',\n",
    "                y='nn_res',\n",
    "                ax=ax[0,2]\n",
    "               )\n",
    "ax[0,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[0,2].set(title='Distribution of the Residuals (Validation)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "\n",
    "sns.scatterplot(data=ml_test_predictions,\n",
    "                x='nn',\n",
    "                y='nn_res',\n",
    "                ax=ax[1,2]\n",
    "               )\n",
    "ax[1,2].axhline(0,\n",
    "                color='black',\n",
    "                ls='--'\n",
    "               )\n",
    "ax[1,2].set(title='Distribution of the Residuals (Test)',\n",
    "            xlabel='prediction',\n",
    "            ylabel='residual'\n",
    "           )\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('ann_mod.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Output\n",
    "\n",
    "Before proceeding further, we save the output of the analysis into CSV and JSON files to have access to them at later times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from the predictions for training, validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns\n",
    "rename_columns = {'lr':     'lin_reg',\n",
    "                  'lr_res': 'lin_reg_residuals',\n",
    "                  'en':     'els_net',\n",
    "                  'en_res': 'els_net_residuals',\n",
    "                  'ls':     'lasso',\n",
    "                  'ls_res': 'lasso_residuals',\n",
    "                  'rd':     'ridge',\n",
    "                  'rd_res': 'ridge_residuals',\n",
    "                  'lv':     'lin_svr',\n",
    "                  'lv_res': 'lin_svr_residuals',\n",
    "                  'sv':     'svr_rbf',\n",
    "                  'sv_res': 'svr_rbf_residuals',\n",
    "                  'rf':     'rnd_for',\n",
    "                  'rf_res': 'rnd_for_residuals',\n",
    "                  'bt':     'grd_bst',\n",
    "                  'bt_res': 'grd_bst_residuals',\n",
    "                  'nn':     'ann_mod',\n",
    "                  'nn_res': 'ann_mod_residuals'\n",
    "                 }\n",
    "\n",
    "# save predictions to file\n",
    "ml_train_predictions.rename(columns=rename_columns).\\\n",
    "                        sort_values(ordering_columns, ignore_index=True).\\\n",
    "                        to_csv(datpath('sft-train-pred.csv'), index=False)\n",
    "ml_val_predictions.rename(columns=rename_columns).\\\n",
    "                        sort_values(ordering_columns, ignore_index=True).\\\n",
    "                        to_csv(datpath('sft-val-pred.csv'), index=False)\n",
    "ml_test_predictions.rename(columns=rename_columns).\\\n",
    "                        sort_values(ordering_columns, ignore_index=True).\\\n",
    "                        to_csv(datpath('sft-test-pred.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the trained models to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "# save models to file (ANN already saved)\n",
    "joblib.dump(lin_reg, modpath('lin_reg.pkl'))\n",
    "joblib.dump(els_net, modpath('els_net.pkl'))\n",
    "joblib.dump(lss_reg, modpath('lasso.pkl'))\n",
    "joblib.dump(rdg_reg, modpath('ridge.pkl'))\n",
    "joblib.dump(lin_svr, modpath('lin_svr.pkl'))\n",
    "joblib.dump(svr_rbf, modpath('svr_rbf.pkl'))\n",
    "joblib.dump(rnd_for, modpath('rnd_for.pkl'))\n",
    "joblib.dump(grd_bst, modpath('grd_bst.pkl'))\n",
    "\n",
    "# save the hyperparameters to JSON\n",
    "with open(outpath('els_net_hyp.json'), 'w') as f:\n",
    "    json.dump(els_net_hyper, f)\n",
    "with open(outpath('lasso_hyp.json'), 'w') as f:\n",
    "    json.dump(lss_reg_hyper, f)\n",
    "with open(outpath('ridge_hyp.json'), 'w') as f:\n",
    "    json.dump(rdg_reg_hyper, f)\n",
    "with open(outpath('lin_svr_hyp.json'), 'w') as f:\n",
    "    json.dump(lin_svr_hyper, f)\n",
    "with open(outpath('svr_rbf_hyp.json'), 'w') as f:\n",
    "    json.dump(svr_rbf_hyper, f)\n",
    "with open(outpath('rnd_for_hyp.json'), 'w') as f:\n",
    "    json.dump({key: float(value) for key, value in rnd_for_hyper.items()}, f)\n",
    "with open(outpath('grd_bst_hyp.json'), 'w') as f:\n",
    "    json.dump({key: float(value) for key, value in grd_bst_hyper.items()}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally save the metrics of the algorithms in a separate CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the metrics in a dataframe\n",
    "ml_metrics = pd.concat([lin_reg_metrics,\n",
    "                        els_net_metrics,\n",
    "                        lss_reg_metrics,\n",
    "                        rdg_reg_metrics,\n",
    "                        lin_svr_metrics,\n",
    "                        svr_rbf_metrics,\n",
    "                        rnd_for_metrics,\n",
    "                        grd_bst_metrics,\n",
    "                        ann_mod_metrics\n",
    "                       ]\n",
    "                      )\n",
    "\n",
    "# list the algorithms in the index\n",
    "algorithms   = ['LR', 'EN', 'Lasso', 'Ridge',\n",
    "                'l-SVR', 'r-SVR',\n",
    "                'RF', 'GBDT',\n",
    "                'ANN'\n",
    "               ]\n",
    "index_mapper = dict(zip(ml_metrics.index, algorithms))\n",
    "\n",
    "# list the metrics in the columns\n",
    "metrics = ['training MSE',\n",
    "           'training MSE 95% CI (lower bound)',\n",
    "           'training MSE 95% CI (upper bound)',\n",
    "           'training MAE',\n",
    "           'training R2',\n",
    "           'validation MSE',\n",
    "           'validation MSE 95% CI (lower bound)',\n",
    "           'validation MSE 95% CI (upper bound)',\n",
    "           'validation MAE',\n",
    "           'validation R2',\n",
    "           'test MSE',\n",
    "           'test MSE 95% CI (lower bound)',\n",
    "           'test MSE 95% CI (upper bound)',\n",
    "           'test MAE',\n",
    "           'test R2'\n",
    "          ]\n",
    "column_mapper = dict(zip(ml_metrics.columns, metrics))\n",
    "\n",
    "# insert new index and new columns\n",
    "ml_metrics = ml_metrics.rename(columns=column_mapper, index=index_mapper)\n",
    "ml_metrics.to_csv(outpath('metrics.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _concatteriori_ Analysis of the Decision Trees\n",
    "\n",
    "As a last step in the analysis, we take a few moments to discuss the importance of the training features inside the **decision trees** and study the _Shapley_ values of the variables in order to better understand the underlying choices of the algorithms. We apply the analysis on the test which has not been used for training since we are interested in \"real world\" scenarios (i.e. untouched sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# import the \"explainers\" and save the values\n",
    "rnd_for_exp = shap.TreeExplainer(rnd_for)\n",
    "grd_bst_exp = shap.TreeExplainer(grd_bst)\n",
    "\n",
    "# extract the Shapley values\n",
    "rnd_for_shap     = rnd_for_exp.shap_values(X_test)\n",
    "grd_bst_shap     = grd_bst_exp.shap_values(X_test)\n",
    "rnd_for_int_shap = rnd_for_exp.shap_interaction_values(X_test)\n",
    "grd_bst_int_shap = grd_bst_exp.shap_interaction_values(X_test)\n",
    "\n",
    "# compute the feature importance\n",
    "rnd_for_imp = rnd_for.feature_importances_.reshape(-1,)\n",
    "grd_bst_imp = grd_bst.feature_importances_.reshape(-1,)\n",
    "\n",
    "# finally store the names of the features\n",
    "features = list(X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore show the Shapley values in plots. We show the importance of the features as computed from the decision trees (i.e. the LightGBM trees), the mean Shapley values for each feature and the interaction Shapley values for a **randomly chosen** sample in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from the **RF**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(1,3)\n",
    "\n",
    "# plot the importance of the features\n",
    "sns.barplot(x=rnd_for_imp,\n",
    "            y=features,\n",
    "            palette=sns.color_palette('muted', len(features)),\n",
    "            ax=ax[0]\n",
    "           )\n",
    "ax[0].set(title='Importance of the Features (RF)')\n",
    "\n",
    "# plot the mean Shapley values\n",
    "sns.barplot(x=rnd_for_shap.mean(axis=0),\n",
    "            y=features,\n",
    "            palette=sns.color_palette('muted', len(features)),\n",
    "            ax=ax[1]\n",
    "           )\n",
    "ax[1].set(title='Average Shapley Values (RF, test set)')\n",
    "\n",
    "# choose one random matrix and plot the heatmap\n",
    "random = np.random.randint(rnd_for_int_shap.shape[0])\n",
    "matrix = rnd_for_int_shap[random]\n",
    "sns.heatmap(matrix,\n",
    "            vmin=matrix.min(),\n",
    "            vmax=matrix.max(),\n",
    "            xticklabels=features,\n",
    "            yticklabels=features,\n",
    "            cmap='RdBu_r',\n",
    "            ax=ax[2]\n",
    "           )\n",
    "ax[2].set(title='Shapley Interaction Values'\n",
    "                '(RF, test set --> ID {:d})'.format(random))\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('rnd_for_shapley.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A peculiar feature of the random forests seems to be a relatively high importance of lower truncation levels, while most of the features do not play a central role. The interaction between the features seems to be rather strong (there are a couple of features who strongly play one against the other) and may be the cause of the difficulty in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then consider the **GBDT**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(1,3)\n",
    "\n",
    "# plot the importance of the features\n",
    "sns.barplot(x=grd_bst_imp,\n",
    "            y=features,\n",
    "            palette=sns.color_palette('muted', len(features)),\n",
    "            ax=ax[0]\n",
    "           )\n",
    "ax[0].set(title='Importance of the Features (GBDT)')\n",
    "\n",
    "# plot the mean Shapley values\n",
    "sns.barplot(x=grd_bst_shap.mean(axis=0),\n",
    "            y=features,\n",
    "            palette=sns.color_palette('muted', len(features)),\n",
    "            ax=ax[1]\n",
    "           )\n",
    "ax[1].set(title='Average Shapley Values (GBDT, test set)')\n",
    "\n",
    "# choose one random matrix and plot the heatmap\n",
    "random = np.random.randint(grd_bst_int_shap.shape[0])\n",
    "matrix = grd_bst_int_shap[random]\n",
    "sns.heatmap(matrix,\n",
    "            vmin=matrix.min(),\n",
    "            vmax=matrix.max(),\n",
    "            xticklabels=features,\n",
    "            yticklabels=features,\n",
    "            cmap='RdBu_r',\n",
    "            ax=ax[2]\n",
    "           )\n",
    "ax[2].set(title='Shapley Interaction Values '\n",
    "                '(GBDT, test set --> ID = {:d})'.format(random))\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(imgpath('grd_bst_shapley.pdf'), dpi=150, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from RF, GBDT attach greater importance to more features (with a predominance of the `weight`, as one might have expected from the pre-analysis). Interaction between features are milder, but the mean Shapley values are more heterogeneous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "As we saw in the pre-analysis and the EDA, the dataset can be quite tricky and difficult to use for training ML algorithms. In particular the EDA underlined some patterns inside the definition of the variables which lead to issues for the learning estimators. We were able to determine that for a certain range of the weight of the observables (namely `weight` $< 1.5$) there is structure to the data and the extrapolation of the labels can already be quite good using unsupervised learning. However the determination of the coefficients of the linear algorithms (strictly without regularisation) seem to be quite precise. We also saw that SVM algorithms fail most of the times in adapting to the datasets while decision trees and neural networks can respond better."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Riccardo Finotello"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "title": "Machine Learning for Level Truncation in Open String field Theory",
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
