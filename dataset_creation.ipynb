{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for String Field Theory\n",
    "\n",
    "In the context of String Field Theory (SFT), we analyse data from different categories of models and study the truncation levels for various observables. The target is to use machine learning (ML) methods to predict the value of the observables without truncating at finite level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "In this notebook we simply take the different files containing the data of the models and we create a single tidy dataset ready for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir  = './data_orig'\n",
    "data_reg  = re.compile(r'.*json$')\n",
    "data_list = [file for file in os.listdir(data_dir) if data_reg.match(file)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in the list include:\n",
    "\n",
    "- **minimal models**, both real and imaginary parts including *weight*, *type*, levels 2 through 24 and the extrapolated label (*exp*),\n",
    "- **lumps** solutions, with the initial point (*init*), *weight*, *type*, levels 2 through 18 and the extrapolated label (*exp*),\n",
    "- **WZW model**, both real and imaginary parts including the level *k*, *weight*, *type*, $\\mathrm{SU}(2)$ quantum numbers *j* and *m* (such that the weight $h = \\frac{j ( j + 1 )}{k + 2}$) and levels 2 through 14 and the extrapolated label (*exp*),\n",
    "- **double lumps** solutions, with the initial point (*init*), *weight*, *type*, levels 2 through 18 and the extrapolated label (*exp*)\n",
    "\n",
    "In general the *init* variable can be discarded as it should not enter the computation of the extrapolated label. Variables which are not present in a specific dataset could be safely replaced with zeros, but for the sake of generality we build a general dataset using `NaN` values (they can be quickly replaced when importing the dataset, for instance, in `pandas`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset in the list we therefore build a tidy dataset before putting everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./data_upload', exist_ok=True)\n",
    "\n",
    "model_dict = {'lumps': 0,\n",
    "              'double-lumps': 1,\n",
    "              'minimal_models': 2,\n",
    "              'wzw': 3\n",
    "             }\n",
    "\n",
    "with open('./data_upload/model_dict.json', 'w') as f:\n",
    "    json.dump(model_dict, f)\n",
    "    \n",
    "model_dict_inv = {value: key for key, value in model_dict.items()}\n",
    "\n",
    "with open('./data_upload/model_dict_reverse.json', 'w') as f:\n",
    "    json.dump(model_dict_inv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_data(file, model, data_dir='./data', rename_levels=True, set_dtype=True, save_path=None, **kwargs):\n",
    "    '''\n",
    "    Create a tidy dataset from a JSON file.\n",
    "    \n",
    "    Needed arguments:\n",
    "        file: the name of the JSON file,\n",
    "        model: a string identifying the physical model.\n",
    "        \n",
    "    Optional arguments:\n",
    "        data_dir:      the root dir containing the file,\n",
    "        rename_levels: rename the columns containing the truncation levels by adding 'level_' as a prefix,\n",
    "        set_dtype:     modify dtypes to be stored,\n",
    "        save_path:     path to a file to save the tidy dataset,\n",
    "        **kwargs:      additional arguments to pass to pd.to_json.\n",
    "    \n",
    "    Returns:\n",
    "        a tidy Pandas dataset if not saved, NoneType otherwise.\n",
    "    '''\n",
    "    \n",
    "    # get the file and drop the init variable\n",
    "    df = pd.read_json(os.path.join(data_dir, file))\n",
    "    \n",
    "    if 'init' in df.columns:\n",
    "        df = df.drop(columns='init')\n",
    "        \n",
    "    if rename_levels:\n",
    "        df = df.rename(columns=lambda c: re.sub(r'^([0-9])$', r'level_0\\1', c))\n",
    "        df = df.rename(columns=lambda c: re.sub(r'^([1-9][0-9])$', r'level_\\1', c))\n",
    "    \n",
    "    # check if real or imaginary parts\n",
    "    is_im   = bool(re.match(r'.*_im[.]json$', file))\n",
    "    \n",
    "    if is_im:\n",
    "        # if imaginary part\n",
    "        df = df.rename(columns=lambda c: re.sub(r'^(.*)$', r'\\1_im', c))    \n",
    "    else:\n",
    "        # if real part (or not specified)\n",
    "        df = df.rename(columns=lambda c: re.sub(r'^(.*)$', r'\\1_re', c))\n",
    "        \n",
    "    # get the name of the columns\n",
    "    columns = list(df.columns)\n",
    "        \n",
    "    # go over the rows and expand them\n",
    "    df_stack = []\n",
    "    for n in range(df.shape[0]):\n",
    "\n",
    "        # get the row (without NaN values)\n",
    "        row = df.iloc[n].dropna()\n",
    "\n",
    "        # check if at least one of the entries is a list\n",
    "        is_list = np.all(row.apply(lambda entry: isinstance(entry, list)))\n",
    "\n",
    "        # if one of the entries if a list, get \n",
    "        if is_list:\n",
    "            row = dict(row)\n",
    "            row = pd.DataFrame(row)\n",
    "\n",
    "            # add column to distinguish the solutions\n",
    "            row['solution'] = n\n",
    "\n",
    "            # append to the list to be concatenated\n",
    "            df_stack.append(row)\n",
    "    \n",
    "    # concatenate if needed\n",
    "    if len(df_stack) > 0:\n",
    "        df = pd.concat(df_stack, axis=0, ignore_index=True)\n",
    "        \n",
    "    # finally add a column with the model specification\n",
    "    df['model'] = model_dict[model]\n",
    "    \n",
    "    # reorder the columns\n",
    "    cols = []\n",
    "    \n",
    "    if 'solution' in df.columns:\n",
    "        cols = ['solution']\n",
    "        \n",
    "    cols = cols + [col for col in columns if not bool(re.match(r'^exp.*', col))]\n",
    "    \n",
    "    if 'model' in df.columns:\n",
    "        cols = cols + ['model']\n",
    "        \n",
    "    if 'exp_im' in columns:\n",
    "        cols = cols + ['exp_im']\n",
    "        \n",
    "    if 'exp_re' in columns:\n",
    "        cols = cols + ['exp_re']\n",
    "        \n",
    "    df = df[cols]\n",
    "        \n",
    "    # reset dtypes for storage\n",
    "    if set_dtype:\n",
    "        # solution is a short int (unsigned)\n",
    "        if 'solution' in df.columns:\n",
    "            df.loc[:, 'solution'] = pd.Series(df['solution'], dtype=np.uint8)\n",
    "            \n",
    "        # the level k is also a short unsigned int if present\n",
    "        if 'k' in df.columns:\n",
    "            df.loc[:, 'k'] = pd.Series(df['k'], dtype=np.uint8)\n",
    "        \n",
    "        # models are categories\n",
    "        df.loc[:, 'model']    = pd.Categorical(pd.Series(df['model'], dtype=np.uint8))\n",
    "        \n",
    "        # every other column is a float\n",
    "        for c in columns:\n",
    "            df.loc[:, c] = pd.Series(df[c], dtype=np.float32)\n",
    "            \n",
    "        # the type column is a category as well\n",
    "        if 'type_re' in df.columns:\n",
    "            df.loc[:, 'type_re'] = pd.Categorical(pd.Series(df['type_re'], dtype=np.uint8))\n",
    "        if 'type_im' in df.columns:\n",
    "            df.loc[:, 'type_im'] = pd.Categorical(pd.Series(df['type_im'], dtype=np.uint8))\n",
    "        \n",
    "    # return the tidy dataset\n",
    "    if save_path is not None:\n",
    "        df.to_json(save_path, **kwargs)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally create the separate datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_tidy = './data_tidy'\n",
    "os.makedirs(data_dir_tidy, exist_ok=True)\n",
    "\n",
    "df_dict = {re.sub(r'[.]json', '', data): tidy_data(file=data,\n",
    "                                                   model=re.sub(r'_re[.]json|_im[.]json|[.]json', '', data),\n",
    "                                                   data_dir=data_dir,\n",
    "                                                   save_path=os.path.join(data_dir_tidy, re.sub(r'[.]json', '', data) + '_tidy.json')\n",
    "                                                  ) for data in data_list\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *lumps* dataset we also remove the first `solution` (i.e. solution $= 0$), as its values may spoil the analysis given the straight forward correlation with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the first entry\n",
    "df_dict['lumps'] = df_dict['lumps'].loc[df_dict['lumps']['solution'] != 0]\n",
    "\n",
    "# rescale the solution number\n",
    "df_dict['lumps']['solution'] = df_dict['lumps'].loc[:, 'solution'].apply(lambda x: x - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Unification\n",
    "\n",
    "From the previous datasets, we need to first merge the data which has both real and imaginary parts, since they are not separate datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [pd.merge(df_dict['minimal_models_re'], df_dict['minimal_models_im'], left_index=True, right_index=True),\n",
    "           pd.merge(df_dict['wzw_re'], df_dict['wzw_im'], left_index=True, right_index=True)\n",
    "          ]\n",
    "\n",
    "# drop redundant columns\n",
    "df_list = [df.drop(columns=['solution_y', 'model_y']).rename(columns={'solution_x': 'solution', 'model_x': 'model'}) for df in df_list]\n",
    "\n",
    "# add the other datasets\n",
    "df_list.append(df_dict['lumps'])\n",
    "df_list.append(df_dict['double-lumps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally merge the newly created datasets in order to have a single file containing the whole information on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "# change temporarily the dtypes of \"type\" to float\n",
    "df.loc[:, 'type_re'] = df['type_re'].astype(np.float32)\n",
    "df.loc[:, 'type_im'] = df['type_im'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this dataset we need to remove identically vanishing columns such as `type_im`, `k_im`, etc. We also need to rename the surviving paired columns to avoid showing the suffix *_re* when it is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanishing_columns = []\n",
    "for column in df.columns:\n",
    "    if column != 'model':\n",
    "        if df[column].mean(skipna=True) == 0 and df[column].std(skipna=True) == 0:\n",
    "            vanishing_columns.append(column)\n",
    "\n",
    "# drop the columns\n",
    "df = df.drop(columns=vanishing_columns)\n",
    "\n",
    "# create a dictionary to rename the columns\n",
    "rename_columns = {re.sub(r'_im$', r'_re', c): re.sub(r'_im$', '', c) for c in vanishing_columns}\n",
    "df = df.rename(columns=rename_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally reorder the columns and modify the dtypes accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['weight', 'type', 'k', 'solution', 'j', 'm',\n",
    "         'level_02_re', 'level_02_im',\n",
    "         'level_03_re', 'level_03_im',\n",
    "         'level_04_re', 'level_04_im',\n",
    "         'level_05_re', 'level_05_im',\n",
    "         'level_06_re', 'level_06_im',\n",
    "         'level_07_re', 'level_07_im',\n",
    "         'level_08_re', 'level_08_im',\n",
    "         'level_09_re', 'level_09_im',\n",
    "         'level_10_re', 'level_10_im',\n",
    "         'level_11_re', 'level_11_im',\n",
    "         'level_12_re', 'level_12_im',\n",
    "         'level_13_re', 'level_13_im',\n",
    "         'level_14_re', 'level_14_im',\n",
    "         'level_15',\n",
    "         'level_16',\n",
    "         'level_17',\n",
    "         'level_18',\n",
    "         'level_19',\n",
    "         'level_20',\n",
    "         'level_21',\n",
    "         'level_22',\n",
    "         'level_23',\n",
    "         'level_24',\n",
    "         'model',\n",
    "         'exp_re', 'exp_im'\n",
    "        ]\n",
    "       ]\n",
    "\n",
    "# fill the NaN exp_im values to avoid mistaking an incomplete case from a purely real solution\n",
    "df.loc[df['exp_im'].isna(), 'exp_im'] = 0.0\n",
    "\n",
    "# fill the NaN values in the imaginary parts of the levels when the corresponding real parts are not NaN as well\n",
    "for n in range(2, 15):\n",
    "    level = 'level_' + f'{n:02}'\n",
    "    df.loc[~df[level + '_re'].isna() & df[level + '_im'].isna(), level + '_im'] = 0.0\n",
    "\n",
    "# modify the dtypes\n",
    "df.loc[:, 'solution'] = pd.Series(df['solution'], dtype='Int8')\n",
    "df.loc[:, 'type']     = pd.Categorical(pd.Series(df['type'], dtype=np.uint8))\n",
    "df.loc[:, 'k']        = pd.Series(df['k'], dtype='Int8')\n",
    "df.loc[:, 'model']    = pd.Categorical(pd.Series(df['model'], dtype=np.uint8))\n",
    "\n",
    "for c in df.columns:\n",
    "    if bool(re.match(r'^weight$|^j$|^m$|^level.*|^exp.*', c)):\n",
    "        df.loc[:, c] = pd.Series(df[c], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally save the dataset to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3300 entries, 0 to 3299\n",
      "Data columns (total 45 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   weight       3300 non-null   float32 \n",
      " 1   type         3300 non-null   category\n",
      " 2   k            1680 non-null   Int8    \n",
      " 3   solution     3280 non-null   Int8    \n",
      " 4   j            1680 non-null   float32 \n",
      " 5   m            1680 non-null   float32 \n",
      " 6   level_02_re  3300 non-null   float32 \n",
      " 7   level_02_im  3300 non-null   float32 \n",
      " 8   level_03_re  3300 non-null   float32 \n",
      " 9   level_03_im  3300 non-null   float32 \n",
      " 10  level_04_re  3300 non-null   float32 \n",
      " 11  level_04_im  3300 non-null   float32 \n",
      " 12  level_05_re  3300 non-null   float32 \n",
      " 13  level_05_im  3300 non-null   float32 \n",
      " 14  level_06_re  3300 non-null   float32 \n",
      " 15  level_06_im  3300 non-null   float32 \n",
      " 16  level_07_re  3300 non-null   float32 \n",
      " 17  level_07_im  3300 non-null   float32 \n",
      " 18  level_08_re  3300 non-null   float32 \n",
      " 19  level_08_im  3300 non-null   float32 \n",
      " 20  level_09_re  3300 non-null   float32 \n",
      " 21  level_09_im  3300 non-null   float32 \n",
      " 22  level_10_re  3300 non-null   float32 \n",
      " 23  level_10_im  3300 non-null   float32 \n",
      " 24  level_11_re  2046 non-null   float32 \n",
      " 25  level_11_im  2046 non-null   float32 \n",
      " 26  level_12_re  1646 non-null   float32 \n",
      " 27  level_12_im  1646 non-null   float32 \n",
      " 28  level_13_re  1631 non-null   float32 \n",
      " 29  level_13_im  1631 non-null   float32 \n",
      " 30  level_14_re  1631 non-null   float32 \n",
      " 31  level_14_im  1631 non-null   float32 \n",
      " 32  level_15     1620 non-null   float32 \n",
      " 33  level_16     1620 non-null   float32 \n",
      " 34  level_17     903 non-null    float32 \n",
      " 35  level_18     903 non-null    float32 \n",
      " 36  level_19     63 non-null     float32 \n",
      " 37  level_20     63 non-null     float32 \n",
      " 38  level_21     21 non-null     float32 \n",
      " 39  level_22     21 non-null     float32 \n",
      " 40  level_23     8 non-null      float32 \n",
      " 41  level_24     8 non-null      float32 \n",
      " 42  model        3300 non-null   category\n",
      " 43  exp_re       3300 non-null   float32 \n",
      " 44  exp_im       3300 non-null   float32 \n",
      "dtypes: Int8(2), category(2), float32(41)\n",
      "memory usage: 548.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.to_json('./data_upload/sft_data.json.gz', orient='index')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step we write a README file containing the explanation of what previously produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "README = '''\n",
    "description:\n",
    "\n",
    "  the dataset contains the values of several variables related to observables in String Field Theory for different models, and at different finite and infinite levels of truncations\n",
    "\n",
    "authors:\n",
    "\n",
    "  M. Kudrna (Charles U., Prague) - original data\n",
    "  R. Finotello (Torino U.) - dataset\n",
    "  \n",
    "dataset:\n",
    "\n",
    "  columns description:\n",
    "  \n",
    "    weight:     conformal weight of the observable (float),\n",
    "    type:       oscillation periodicity (int, categorical: either 2 or 4),\n",
    "    k:          level of the WZW model (int, <NA> if not WZW model),\n",
    "    solution:   identifier of the radius of the solution in the same physical model (int, <NA> for double lumps),\n",
    "    j:          quantum number of the SU(2) representation (float, NaN if not WZW model),\n",
    "    m:          quantum number of the SU(2) representation (float, NaN if not WZW model),\n",
    "    level_*_re: real part of the finite truncation levels (float, Nan if not computed),\n",
    "    level_*_im: imaginary part of the finite truncation levels (float, Nan if not computed),\n",
    "    level_*:    real part of the higher finite truncation levels (float, Nan if not computed),\n",
    "    model:      category of the physical model (int, categorical from 0 to 3, see dictionary below),\n",
    "    exp_re:     real part of the extrapolated truncation at infinity (float),\n",
    "    exp_re:     imaginary part of the extrapolated truncation at infinity (float)\n",
    "    \n",
    "  description:\n",
    "      \n",
    "    content: string field theory observables at different levels of truncation\n",
    "    \n",
    "    notes:\n",
    "    \n",
    "      - JSON dictionaries to translate categorical models to name of the physical model are provided (see model_dict.json and model_dict_reverse.json in this directory)\n",
    "      - NaN or <NA> have been left where the values have not been computed for some reason to be distinguishable from a genuine zero (most of the times they can be safely replaced with zeros using Pandas, or similar tools)\n",
    "    \n",
    "  rows: 3300\n",
    "  \n",
    "  columns: 45\n",
    "  \n",
    "  size: ~300KB (gzipped), ~2.8MB (deflated)\n",
    "  \n",
    "file:\n",
    "\n",
    "  mime: application/gzip\n",
    "  name: sft_data.json.gz\n",
    "  description: gzipped JSON file\n",
    "'''\n",
    "\n",
    "with open('./data_upload/sft_data.json.gz.txt', 'w') as f:\n",
    "    f.write(README)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-sft-trunc",
   "language": "python",
   "name": "ml-sft-trunc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
