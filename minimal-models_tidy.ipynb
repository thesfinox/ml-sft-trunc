{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Truncation in String Field Theory\n",
    "\n",
    "H. Erbin, R. Finotello, M. Kudrna\n",
    "\n",
    "## Minimal Models\n",
    "\n",
    "We consider minimal models in String Field Theory and their truncation levels.\n",
    "We use machine learning techniques to extrapolate the value of the truncations $L$ at $\\infty$ given the results using fits of polynomials in $\\frac{1}{L}$.\n",
    "\n",
    "## Dataset Tidying\n",
    "\n",
    "In what follows we tidy the dataset and prepare a dataset containing the physical observables as rows, and weight, type and truncation levels as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Dataset\n",
    "\n",
    "We first import the dataset using the `pandas` library.\n",
    "The original dataset is a Mathematica file which has been convert to JSON in the format `[{column -> value} ... {column -> value}]` thus the correct *orientation* of the dataset is `records`.\n",
    "There are two different datasets: the first is the real part of the data, the second represents the imaginary part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = pd.read_json('./data/data_re.json', orient='records')\n",
    "df_im = pd.read_json('./data/data_im.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Native datasets are composed by solutions at different radii and, for each one of them, by the values of weight, type and truncation levels of different observables.\n",
    "There are several incomplete cases as it is impossible to compute the same number of truncation levels for all the observables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 63 incomplete cases in each dataset.\n"
     ]
    }
   ],
   "source": [
    "# check consistency between the two datasets\n",
    "assert df_re.isna().astype(np.int).sum().max() == df_re.isna().astype(np.int).sum().max()\n",
    "\n",
    "n_incomplete = df_re.isna().astype(np.int).sum().max()\n",
    "print(f'There are {n_incomplete:d} incomplete cases in each dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datasets are made of 65 rows and 26 columns.\n",
      "Equivalently, they are 65 solutions and 26 features and labels.\n"
     ]
    }
   ],
   "source": [
    "# check consistency between the two datasets\n",
    "assert df_re.shape == df_im.shape\n",
    "\n",
    "df_nrows, df_ncols = df_re.shape\n",
    "print(f'The datasets are made of {df_nrows:d} rows and {df_ncols:d} columns.')\n",
    "print(f'Equivalently, they are {df_nrows:d} solutions and {df_ncols:d} features and labels.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each feature has a different physical interpretation.\n",
    "The column `exp` represents the extrapolated labels which will be used as output of the machine learning algorithms.\n",
    "In fact the `type` feature is categorical and can be rescaled to $\\{ 0,\\, 1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['exp', 'weight', 'type', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
       "       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',\n",
       "       '23', '24'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_re.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['exp', 'weight', 'type', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
       "       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',\n",
       "       '23', '24'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_im.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `type` column takes only values $\\{ 2,\\, 4 \\}$ which can be rescaled by dividing by $2$ and subtracting $1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re['type'] = df_re['type'].apply(lambda x: [(i // 2) - 1 for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally rename the columns to be distinguishable by adding the prefix *lev_* to the truncation levels, and the suffixes *_re* and *_im* to set the real and imaginary parts apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = df_re.rename(columns=lambda x: re.sub('$', '_re', re.sub('^([0-9]*)$', r'lev_\\1', x)))\n",
    "df_im = df_im.rename(columns=lambda x: re.sub('$', '_im', re.sub('^([0-9]*)$', r'lev_\\1', x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incomplete Cases\n",
    "\n",
    "We then need to handle the incomplete cases.\n",
    "We fill every incomplete (`NaN`) case with a list of zeros as long as the number of observables in the solution.\n",
    "We then pad the entire result to be as long as the highest number of observables per solution in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_re = df_re.applymap(lambda x: len(x) if isinstance(x, list) else 0).max(axis=1)\n",
    "length_im = df_im.applymap(lambda x: len(x) if isinstance(x, list) else 0).max(axis=1)\n",
    "\n",
    "# check if the number of observables is consistent\n",
    "assert (length_re == length_im).all()\n",
    "length = length_re # or length im since it is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the incomplete cases with a list of zeros\n",
    "df_re = df_re.applymap(lambda x: x if isinstance(x, list) else [0])\n",
    "df_im = df_im.applymap(lambda x: x if isinstance(x, list) else [0])\n",
    "\n",
    "# pad the lists to be the maximal length\n",
    "df_re = df_re.applymap(lambda x: np.pad(x, (0,length.max() - len(x))))\n",
    "df_im = df_im.applymap(lambda x: np.pad(x, (0,length.max() - len(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking the Dataset\n",
    "\n",
    "We then stack the dataset to obtain a dataset containing only one physical observable per line.\n",
    "We first add a column to dicriminate the number of solutions and observables for ordering purposes (i.e. we assign a number in $[0,\\, 64]$ for the solution and $[0,\\, 16]$ for each observable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution   = []\n",
    "observable = []\n",
    "for n in df_re.index:\n",
    "    solution.append([n] * length.max())\n",
    "    observable.append([i for i in range(length.max())])\n",
    "\n",
    "# add to dataset\n",
    "df_re['solution']   = solution\n",
    "df_re['observable'] = observable\n",
    "df_im['solution']   = solution\n",
    "df_im['observable'] = observable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stack each solution on top of each other by creating a list of `pd.DataFrame` for each solution and then stack all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_series(df, n):\n",
    "    '''\n",
    "    Expand a row of a dataset containing lists to a standalone dataframes.\n",
    "    \n",
    "    Needed arguments:\n",
    "        df: the dataframe (pd.DataFrame),\n",
    "        n:  the id number of the row (int).\n",
    "    '''\n",
    "    \n",
    "    # select the row\n",
    "    row = df.iloc[n]\n",
    "    \n",
    "    # expand to a series\n",
    "    row = row.apply(pd.Series)\n",
    "    \n",
    "    # transpose the dataframe\n",
    "    row = row.transpose()\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re_list = [expand_series(df_re, n) for n in range(df_nrows)]\n",
    "df_im_list = [expand_series(df_im, n) for n in range(df_nrows)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then stack the dataframes using `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re_tidy = pd.concat(df_re_list)\n",
    "df_im_tidy = pd.concat(df_im_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Output\n",
    "\n",
    "We finally clean the output by setting the *dtypes*, reordering the columns, joining the datasets, and removing the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dtypes\n",
    "df_re_tidy = df_re_tidy.astype({'type_re': np.int, 'solution': np.int, 'observable': np.int})\n",
    "df_im_tidy = df_im_tidy.astype({'type_im': np.int, 'solution': np.int, 'observable': np.int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the datasets\n",
    "df_tidy = df_re_tidy.merge(df_im_tidy, how='inner', on=['solution', 'observable'])\n",
    "\n",
    "# assert that no incomplete cases are present\n",
    "assert df_tidy.isna().sum().sum() == 0\n",
    "\n",
    "# check if shape still matches\n",
    "assert df_tidy.shape[0] == df_re_tidy.shape[0]\n",
    "assert df_tidy.shape[0] == df_im_tidy.shape[0]\n",
    "assert df_tidy.shape[1] == df_re_tidy.shape[1] + df_im_tidy.shape[1] - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select vanishing columns\n",
    "df_agg_cols = df_tidy.agg([np.sum, np.mean, np.std])\n",
    "drop_cols   = [key for key in df_agg_cols.columns if (df_agg_cols[key] == 0).all()]\n",
    "df_tidy     = df_tidy.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove vanishing rows\n",
    "df_agg_rows = df_tidy.drop(columns=['solution', 'observable']).agg([np.sum, np.mean, np.std], axis=1)\n",
    "drop_rows   = [index for index in df_agg_rows.index if (df_agg_rows.iloc[index] == 0).all()]\n",
    "df_tidy     = df_tidy.drop(index=drop_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape of the tidy dataset: 802 samples, 34 features.\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates (do not use solution and observable)\n",
    "col_list = [col for col in df_tidy.columns if col != 'solution' and col != 'observable']\n",
    "df_tidy  = df_tidy.drop_duplicates(subset=col_list, ignore_index=True)\n",
    "\n",
    "df_tidy_nrows, df_tidy_ncols = df_tidy.shape\n",
    "\n",
    "print(f'New shape of the tidy dataset: {df_tidy_nrows:d} samples, {df_tidy_ncols:d} features.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the new columns\n",
    "df_tidy = df_tidy.rename(columns={'exp_re': 'exp', 'weight_re': 'weight', 'type_re': 'type'})\n",
    "df_tidy = df_tidy.rename(columns=lambda x: re.sub('_([0-9])_', r'_0\\1_', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the columns\n",
    "sorted_cols = ['solution', 'observable', 'weight', 'type'] + sorted(df_tidy.filter(regex='lev_')) + ['exp']\n",
    "df_tidy     = df_tidy[sorted_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Dataset\n",
    "\n",
    "We finally have the tidy dataset and we can save it to file (we choose JSON format for versatility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tidy.to_json('./data/data_tidy.json.gz', orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
